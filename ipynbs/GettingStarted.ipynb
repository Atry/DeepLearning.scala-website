{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is a great tool that helps us efficiently summarize inherent patterns from tons of input data. I'd like to introduce DeepLearning.scala by letting the framework learn the common difference from Arithmetic progression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "\n",
    "**Input**:\n",
    " Arithmetic progression(AP) as:\n",
    "``` val input: INDArray = Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray``` \n",
    "\n",
    "\n",
    "**Output**: \n",
    " Common Difference of the certain AP as: \n",
    "```val expectedOutput: INDArray = Array(Array(1), Array(3), Array(2)).toNDArray```\n",
    "\n",
    "So here we want DeepLearning.scala to learn the common difference from the AP, i.e. ```{1} from {0, 1, 2} ``` \n",
    "in which `2-1 = 1-0 = 1 `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install DeepLearning.scala\n",
    "\n",
    "DeepLearning.scala is hosted on Maven Central repository.\n",
    "If you use [sbt](http://www.scala-sbt.org), please add the following settings in your `build.sbt`:\n",
    "\n",
    "```\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiableany\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiablenothing\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiableseq\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiabledouble\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiablefloat\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiablehlist\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiablecoproduct\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiableindarray\" % \"latest.release\"\n",
    "\n",
    "addCompilerPlugin(\"com.thoughtworks.implicit-dependent-type\" %% \"implicit-dependent-type\" % \"latest.release\")\n",
    "\n",
    "addCompilerPlugin(\"org.scalamacros\" % \"paradise\" % \"2.1.0\" cross CrossVersion.full)\n",
    "\n",
    "fork := true\n",
    "```\n",
    "\n",
    "Note that this example does not run on Scala 2.12 because [nd4j](http://nd4j.org/) does not support Scala 2.12. Make sure there is not a setting like `scalaVersion := \"2.12.1\"` in your `build.sbt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For [jupyter-scala](https://github.com/alexarchambault/jupyter-scala) or [Ammonite-REPL](http://www.lihaoyi.com/Ammonite/#Ammonite-REPL), you can use magic imports, which will download DeepLearning.scala and its dependencies for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                 \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableHList._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableDouble._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableAny._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Lift._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathFunctions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathOps\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4s.Implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mshapeless._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $plugin.$ivy.`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:1.0.0`\n",
    "\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableany:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablenothing:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableseq:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiabledouble:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablefloat:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablehlist:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableindarray:1.0.0-RC5`\n",
    "\n",
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.0`\n",
    "\n",
    "import com.thoughtworks.deeplearning.DifferentiableHList._\n",
    "import com.thoughtworks.deeplearning.DifferentiableDouble._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray._\n",
    "import com.thoughtworks.deeplearning.DifferentiableAny._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
    "import com.thoughtworks.deeplearning.Lift._\n",
    "import com.thoughtworks.deeplearning.Poly.MathFunctions._\n",
    "import com.thoughtworks.deeplearning.Poly.MathOps\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4s.Implicits._\n",
    "import shapeless._\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Scaladex](https://index.scala-lang.org/thoughtworksinc/deeplearning.scala) for settings of other build tools!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Design your own neural network\n",
    "\n",
    "DeepLearning.scala is also a language that we can use to create complex neural networks.\n",
    "\n",
    "In the following sections, you will learn:\n",
    " * how to define types for a neural network\n",
    " * how to use a neural network as a predictor\n",
    " * how to create a neural network\n",
    " * how to train a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like a `scala.Function`, a neural network has its own input types and output types.\n",
    "\n",
    "For example, the type of the neural network that accepts an N-dimensional array and returns another N-dimensional array is `(INDArray <=> INDArray)##T`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "val myNeuralNetwork: (INDArray <=> INDArray)##T = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `(A <=> B)##T`, A is the input type, and B is the output type. For the example above, both the input type and the output type are `INDArray`.\n",
    "\n",
    "\n",
    "`##T` is a syntactic sugar to create implicit dependent types. See [implicit-dependent-type](https://github.com/ThoughtWorksInc/implicit-dependent-type) for more information about `##`.\n",
    "\n",
    "The above code throws an `NotImplementedError` because we have not been implemtnted the neural network.\n",
    "In later sections of this article, you will replace `???` to a valid neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Use a neural network as a predictor\n",
    "\n",
    "Like a normal `scala.Function`, if you pass the input data to the neural network, it will return some results.\n",
    "You can use the `predict` method to invoke a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "val input: INDArray = Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray\n",
    "val predictionResult: INDArray = myNeuralNetwork.predict(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code throws an `NotImplementedError` because `myNeuralNetwork` has not been implemented.\n",
    "We will fix the problem by creating a valid neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Create a neural network\n",
    "\n",
    "Same as the definition of a normal Scala function, the definition of neural network consists of a type definition for its parameter, a type definition for its return value, and a body that contains mathematical formulas, function-calls, and control flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1  Weight Intialization \n",
    "\n",
    "A neural network is trainable.\n",
    "It means that some variables in the neural network can be changed automatically according to some goals. Those variables are called `weight`.\n",
    "You can create weight variables via `toWeight` method, given its initial value.\n",
    "\n",
    "In order to create a weight, you must create an `Optimizer`, which contains the rule that manages how the weight changes. See [Scaladoc](https://javadoc.io/page/com.thoughtworks.deeplearning/unidoc_2.11/latest/com/thoughtworks/deeplearning/DifferentiableINDArray$$Optimizers$.html) for a list of built-in optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36moptimizer\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit def optimizer: Optimizer = new LearningRate {\n",
    "  def currentLearningRate() = 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcreateMyNeuralNetwork\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createMyNeuralNetwork(implicit input: From[INDArray]##T): To[INDArray]##T = {\n",
    "  val initialValueOfWeight = Nd4j.randn(3, 1)\n",
    "  val weight: To[INDArray]##T = initialValueOfWeight.toWeight\n",
    "  input dot weight\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2 `From` and `To` placeholders\n",
    "\n",
    "When you create a neural network, you have not actually evaluated it yet.\n",
    "In fact, you only build its structure.\n",
    "Variables in the neural network are placeholders,\n",
    "which will be replaced with actual values in the future training or prediction process.\n",
    "\n",
    "`From` is the placeholder type for input parameter,\n",
    "and `To` is the placeholder type for return values and other local variables.\n",
    "\n",
    "`From` must be `implicit` so that it is automatically generated when you create the neural network. Otherwise, you have to manually pass the `From` placeholder to `createMyNeuralNetwork`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mmyNeuralNetwork\u001b[39m: (\u001b[32mTo\u001b[39m[\u001b[32mINDArray\u001b[39m]{type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray;type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray})#\u001b[32mT\u001b[39m = Dot(Identity(),Weight([0.76, -1.34, -1.45]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myNeuralNetwork = createMyNeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train your Neuro Network\n",
    "\n",
    "You have learned that weight will be automatically changed due to some goals.\n",
    "\n",
    "In DeepLearning.scala, when we train a neural network, our goal should always be minimizing the absolute of the return value.\n",
    "\n",
    "For example, if someone repeatedly call `myNeuralNetwork.train(Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray)`,\n",
    "the neural network would try to minimize `input dot weight`.\n",
    "Soon `weight` would become an array of zeros in order to make `input dot weight` zeros,\n",
    "and `myNeuralNetwork.predict(Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray)` would return `Array(Array(0), Array(0), Array(0)).toNDArray`.\n",
    "\n",
    "What if you expect `myNeuralNetwork.predict(Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray)` to return `Array(Array(1), Array(3), Array(2)).toNDArray`?\n",
    "\n",
    "You can create another neural network that evaluates how far between the result of `myNeuralNetwork` and your expectation. The new neural network is usually called **loss function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlossFunction\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lossFunction(implicit pair: From[INDArray :: INDArray :: HNil]##T): To[Double]##T = {\n",
    "  val input = pair.head\n",
    "  val expectedOutput = pair.tail.head\n",
    "  abs(myNeuralNetwork.compose(input) - expectedOutput).sum\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the `lossFunction` get trained continuously, its return value will be close to zero, and the result of  `myNeuralNetwork` must be close to the expected result at the same time.\n",
    "\n",
    "Note the `lossFunction` accepts a placehold of `INDArray :: INDArray :: HNil` as its parameter, which is  a [shapeless](https://github.com/milessabin/shapeless)'s `HList` type.\n",
    "The `HList` consists of two N-dimensional arrays.\n",
    "The first array is the input data used to train the neural network, and the second array is the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36minput\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 1.00, 2.00],\n",
       " [3.00, 6.00, 9.00],\n",
       " [13.00, 15.00, 17.00]]\n",
       "\u001b[36mexpectedOutput\u001b[39m: \u001b[32mINDArray\u001b[39m = [1.00, 3.00, 2.00]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input: INDArray = Array(Array(0, 1, 2), Array(3, 6, 9), Array(13, 15, 17)).toNDArray\n",
    "val expectedOutput: INDArray = Array(Array(1), Array(3), Array(2)).toNDArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-1466655355\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0],\"y\":[63.65337371826172,62.12937927246094,60.60538101196289,59.08137893676758,57.5573844909668,56.03338623046875,54.50938415527344,52.985389709472656,51.461387634277344,49.9373893737793,48.41339111328125,46.8893928527832,45.365394592285156,43.84139633178711,42.31739807128906,40.79339599609375,39.2693977355957,37.745399475097656,36.221397399902344,34.69739532470703,33.17339324951172,31.649394989013672,30.12539291381836,28.60139274597168,27.077392578125,25.553390502929688,24.029390335083008,22.505390167236328,20.981388092041016,19.457387924194336,17.933387756347656,16.409385681152344,14.885384559631348,13.361383438110352,11.837384223937988,10.313384056091309,8.789382934570312,8.912578582763672,8.712579727172852,8.512578964233398,8.777383804321289,8.816577911376953,8.61658000946045,8.41657829284668,8.76538372039795,8.720579147338867,8.520580291748047,8.320578575134277,8.753385543823242,8.624579429626465,8.424579620361328,8.237385749816895,8.728580474853516,8.528579711914062,8.32857894897461,8.225387573242188,8.632579803466797,8.432579040527344,8.232580184936523,8.213388442993164,8.536581993103027,8.336580276489258,8.136580467224121,8.201388359069824,8.440581321716309,8.240579605102539,8.040580749511719,8.189390182495117,8.34458065032959,8.14457893371582,7.944580554962158,8.177389144897461,8.248580932617188,8.048580169677734,7.8485798835754395,8.165390014648438,8.152580261230469,7.952579975128174,7.752580165863037,8.153390884399414,8.05657958984375,7.856579780578613,7.656580924987793,8.141392707824707,7.960579872131348,7.760580062866211,7.625392436981201,8.064580917358398,7.8645806312561035,7.664580345153809,7.613393306732178,7.968580722808838,7.768580913543701,7.568580627441406,7.6013946533203125,7.872580051422119,7.672581195831299,7.472580909729004,7.589395523071289,7.776581764221191]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss on time\"};\n",
       "\n",
       "  Plotly.plot('plot-1466655355', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mlossSeq\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[32m63.65337371826172\u001b[39m,\n",
       "  \u001b[32m62.12937927246094\u001b[39m,\n",
       "  \u001b[32m60.60538101196289\u001b[39m,\n",
       "  \u001b[32m59.08137893676758\u001b[39m,\n",
       "  \u001b[32m57.5573844909668\u001b[39m,\n",
       "  \u001b[32m56.03338623046875\u001b[39m,\n",
       "  \u001b[32m54.50938415527344\u001b[39m,\n",
       "  \u001b[32m52.985389709472656\u001b[39m,\n",
       "  \u001b[32m51.461387634277344\u001b[39m,\n",
       "  \u001b[32m49.9373893737793\u001b[39m,\n",
       "  \u001b[32m48.41339111328125\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mplot\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mScatter\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mScatter\u001b[39m(\n",
       "    \u001b[33mSome\u001b[39m(\n",
       "      \u001b[33mDoubles\u001b[39m(\n",
       "        \u001b[33mVector\u001b[39m(\n",
       "          \u001b[32m0.0\u001b[39m,\n",
       "          \u001b[32m1.0\u001b[39m,\n",
       "          \u001b[32m2.0\u001b[39m,\n",
       "          \u001b[32m3.0\u001b[39m,\n",
       "          \u001b[32m4.0\u001b[39m,\n",
       "          \u001b[32m5.0\u001b[39m,\n",
       "          \u001b[32m6.0\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mres6_4\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-1466655355\"\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lossSeq = for (_ <- 0 until 30) yield\n",
    "  lossFunction.train(input :: expectedOutput :: HNil)\n",
    "plotly.JupyterScala.init()\n",
    "val plot = Seq(\n",
    "  Scatter(\n",
    "   0 until 30 by 1,\n",
    "   lossSeq\n",
    "  )\n",
    ")\n",
    "plot.plot(\n",
    "  title = \"loss on time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7.576580047607422\n"
     ]
    }
   ],
   "source": [
    "// The loss should close to zero\n",
    "println(s\"loss: ${ lossFunction.predict(input :: expectedOutput :: HNil) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: [-1.03, -2.04, 2.50]\n"
     ]
    }
   ],
   "source": [
    "// The prediction result should close to Array(Array(1), Array(0)).toNDArray\n",
    "println(s\"result: ${ myNeuralNetwork.predict(input) }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this article, you have learned:\n",
    "* to create neural networks dealing with complex data structures like `Double`, `INDArray` and `HList` like ordinary programming language\n",
    "* to compose a neural network into a larger neural network\n",
    "* to train a neural network\n",
    "* to use a neural network as a predictor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
