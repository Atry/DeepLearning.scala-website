{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two layer net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景\n",
    "\n",
    "为了提高预测准确率，我们需要使用多层神经网络，因为一般来说网络的层数越多其表达能力越强，因为其参数更多，能表达出的状态信息更多，所以表达能力越强。多层神经网络可以应对更加复杂的问题，这一节我们先从一个小例子开始：构建一个两层神经网络，这一节我们构建的简单两层神经网络可以达到51%的准确率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 引入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                 \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.{FileInputStream, InputStream}\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableHList._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableDouble._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableAny._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray.Layers.Weight\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.{\n",
       "  DifferentiableHList,\n",
       "  DifferentiableINDArray,\n",
       "  Layer,\n",
       "  Symbolic\n",
       "}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Layer.Batch\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathFunctions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathMethods./\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathOps\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.cpu.nativecpu.NDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.ops.transforms.Transforms\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4s.Implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mshapeless._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.immutable.IndexedSeq\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                   ,ReadCIFAR10ToNDArray._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$    ,Utils._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $plugin.$ivy.`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:2.0.0`\n",
    "\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableany:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablenothing:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableseq:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiabledouble:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablefloat:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablehlist:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableindarray:1.0.0-RC7`\n",
    "import $ivy.`org.rauschig:jarchivelib:0.5.0`\n",
    "\n",
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.0`\n",
    "\n",
    "import java.io.{FileInputStream, InputStream}\n",
    "\n",
    "\n",
    "import com.thoughtworks.deeplearning\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import com.thoughtworks.deeplearning.DifferentiableHList._\n",
    "import com.thoughtworks.deeplearning.DifferentiableDouble._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray._\n",
    "import com.thoughtworks.deeplearning.DifferentiableAny._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray.Layers.Weight\n",
    "import com.thoughtworks.deeplearning.{\n",
    "  DifferentiableHList,\n",
    "  DifferentiableINDArray,\n",
    "  Layer,\n",
    "  Symbolic\n",
    "}\n",
    "import com.thoughtworks.deeplearning.Layer.Batch\n",
    "import com.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
    "import com.thoughtworks.deeplearning.Symbolic._\n",
    "import com.thoughtworks.deeplearning.Poly.MathFunctions._\n",
    "import com.thoughtworks.deeplearning.Poly.MathMethods./\n",
    "import com.thoughtworks.deeplearning.Poly.MathOps\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.cpu.nativecpu.NDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
    "import org.nd4j.linalg.ops.transforms.Transforms\n",
    "import org.nd4s.Implicits._\n",
    "import shapeless._\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._\n",
    "\n",
    "import scala.collection.immutable.IndexedSeq\n",
    "import scala.util.Random\n",
    "\n",
    "pprintConfig() = pprintConfig().copy(height = 2)//减少输出的行数，避免页面输出太长\n",
    "\n",
    "import $file.ReadCIFAR10ToNDArray,ReadCIFAR10ToNDArray._\n",
    "import $file.Utils,Utils._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 读取和处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似[前一节](https://thoughtworksinc.github.io/DeepLearning.scala/demo/MiniBatchGradientDescent.html)从CIFAR10 database中读取和处理测试数据的图片和标签信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mNumberOfClasses\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36mtestNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.0\u001b[33m...\u001b[39m\n",
       "\u001b[36mvectorizedTestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CIFAR10中的图片共有10个分类(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
    "val NumberOfClasses: Int = 10\n",
    "\n",
    "//加载测试数据，我们读取100条作为测试数据\n",
    "val testNDArray =\n",
    "  ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/test_batch.bin\", 100)\n",
    "\n",
    "val testData = testNDArray.head\n",
    "\n",
    "val testExpectResult = testNDArray.tail.head\n",
    "\n",
    "val vectorizedTestExpectResult = Utils.makeVectorized(testExpectResult, NumberOfClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 参数调优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟前一节不同，这节我们假如一些参数调优的手段，设置学习率和使用[L2Regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)),L2Regularization可以用来避免过拟合。我们还使用了每个迭代learningRate都下降为原来的0.9995倍的办法来解决训练时间增长时因为learningRate相对太大导致loss下降太慢或者不下降的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36moptimizerFactory\u001b[39m: \u001b[32mAnyRef\u001b[39m with \u001b[32mOptimizerFactory\u001b[39m = $sess.cmd2Wrapper$Helper$$anon$2@28287c5c"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit val optimizerFactory = new DifferentiableINDArray.OptimizerFactory {\n",
    "  override def ndArrayOptimizer(weight: Weight): Optimizer = {\n",
    "    new LearningRate with L2Regularization {\n",
    "\n",
    "      var learningRate = 0.001\n",
    "\n",
    "      override protected def currentLearningRate(): Double = {\n",
    "        learningRate *= 0.9995\n",
    "        learningRate\n",
    "      }\n",
    "\n",
    "      override protected def l2Regularization: Double = 0.03\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 编写第一层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是全连接和[relu](http://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-network)组成的神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfullyConnectedThenRelu\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fullyConnectedThenRelu(inputSize: Int, outputSize: Int)(\n",
    "    implicit row: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val w = (Nd4j.randn(inputSize, outputSize) / math.sqrt(outputSize / 2.0)).toWeight * 0.1\n",
    "  val b = Nd4j.zeros(outputSize).toWeight\n",
    "  max((row dot w) + b, 0.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 编写第二层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟上节相同，我们使用softmax作为分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msoftmax\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(implicit scores: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val expScores = exp(scores)\n",
    "  expScores / expScores.sum(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编写两层神经网络的第二层神经网络，这时一层全连接一层softmax的神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfullyConnectedThenSoftmax\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fullyConnectedThenSoftmax(inputSize: Int, outputSize: Int)(\n",
    "    implicit row: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val w = (Nd4j.randn(inputSize, outputSize) / math.sqrt(outputSize)).toWeight //* 0.1\n",
    "  val b = Nd4j.zeros(outputSize).toWeight\n",
    "  softmax.compose((row dot w) + b)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 组合两层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了实现两层神经网络我们使用`compose`将上面的两层神经网络组合起来，组成一个两层神经网络。`a.compose(b)`可以将`b`的输出作为`a`输入从而将两层神经网络组合起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mhiddenLayer\u001b[39m\n",
       "\u001b[36mpredictor\u001b[39m: (\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m[\u001b[32mINDArray\u001b[39m]{type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray;type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray})#\u001b[32m@\u001b[39m = Compose(Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum(Exp(Identity()),WrappedArray(1)))),PlusINDArray(Dot(Identity(),Weight([[-0.20, -0.33, -0.19, -0.\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hiddenLayer(implicit input: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val layer0 = fullyConnectedThenRelu(3072, 500).compose(input)\n",
    "  fullyConnectedThenSoftmax(500, 10).compose(layer0)\n",
    "}\n",
    "\n",
    "val predictor = hiddenLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 编写LossFunction并组合输入层和[隐含层](http://stats.stackexchange.com/questions/63152/what-does-the-hidden-layer-in-a-neural-network-compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcrossEntropy\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mnetwork\u001b[39m\n",
       "\u001b[36mtrainer\u001b[39m: (\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m[\u001b[32mDouble\u001b[39m]{type OutputData = Double;type OutputDelta = Double;type InputData = shapeless.::[org.nd4j.linalg.api.ndarray.INDArray,shapeless.::[org.nd4j.linalg.api.ndarray.INDArray,shapeless.HNil]];type InputDelta = shapeless.:+:[org.nd4j.linalg.api.ndarray.INDArray,shapeless.:+:[org.nd4j.linalg.api.ndarray.INDArray,shapeless.CNil]]})#\u001b[32m@\u001b[39m = Compose(Negative(ReduceMean(PlusINDArray(MultiplyINDArray(Head(Tail(Identity())),Log(PlusDouble(MultiplyDouble(Head(Identity()),Literal(0.9)),Literal(0.1)))),Mu\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def crossEntropy(\n",
    "    implicit pair: (INDArray :: INDArray :: HNil) @Symbolic): Double @Symbolic = {\n",
    "  val score = pair.head\n",
    "  val label = pair.tail.head\n",
    "  -(label * log(score * 0.9 + 0.1) + (1.0 - label) * log(1.0 - score * 0.9)).mean\n",
    "}\n",
    "\n",
    "def network(\n",
    "   implicit pair: (INDArray :: INDArray :: HNil) @Symbolic): Double @Symbolic = {\n",
    "  val input = pair.head\n",
    "  val label = pair.tail.head\n",
    "  val score: INDArray @Symbolic = predictor.compose(input)\n",
    "  val hnilLayer: HNil @Symbolic = HNil\n",
    "  crossEntropy.compose(score :: label :: hnilLayer)\n",
    "}\n",
    "\n",
    "val trainer = network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 训练神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与上一节相同，训练神经网络并观察每次训练loss的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1 loss is :0.21449708938598633\n",
      "at epoch 2 loss is :0.20017881393432618\n",
      "at epoch 3 loss is :0.19701696634292604\n",
      "at epoch 4 loss is :0.19029536247253417\n",
      "at epoch 5 loss is :0.20084950923919678\n",
      "at epoch 6 loss is :0.18678001165390015\n",
      "at epoch 7 loss is :0.18276526927947997\n",
      "at epoch 8 loss is :0.17886807918548583\n",
      "at epoch 9 loss is :0.19118783473968506\n",
      "at epoch 10 loss is :0.17489395141601563\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-514606496\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0],\"y\":[0.25571749210357664,0.2807183742523193,0.2581465244293213,0.2618283271789551,0.2661424160003662,0.271175479888916,0.2793111324310303,0.32669432163238527,0.3068307638168335,0.38881626129150393,0.34895596504211424,0.4101762294769287,0.4161728858947754,0.41355171203613283,0.4075232982635498,0.4179529190063477,0.4127168655395508,0.4010939121246338,0.40024003982543943,0.31752586364746094,0.3946880102157593,0.30333523750305175,0.34202311038970945,0.3891549825668335,0.41332416534423827,0.40308027267456054,0.36750364303588867,0.4006487846374512,0.38511836528778076,0.3016098976135254,0.3285010576248169,0.29036989212036135,0.29114351272583006,0.27823050022125245,0.27843894958496096,0.26888768672943114,0.2628163814544678,0.2648310661315918,0.2591243267059326,0.25082662105560305,0.24930958747863768,0.2512075424194336,0.2522172689437866,0.24967560768127442,0.24511847496032715,0.2479485273361206,0.24849255084991456,0.2501591444015503,0.24351346492767334,0.2449481248855591,0.24323728084564208,0.24344773292541505,0.24011547565460206,0.23475565910339355,0.23802599906921387,0.23966827392578124,0.23843731880187988,0.23325724601745607,0.23374004364013673,0.23642849922180176,0.23510541915893554,0.23627824783325196,0.23683881759643555,0.23389606475830077,0.23493514060974122,0.23365578651428223,0.22949595451354982,0.2321549892425537,0.2292046070098877,0.23610641956329345,0.22966322898864747,0.23217153549194336,0.23514232635498047,0.23023667335510253,0.23060393333435059,0.23353815078735352,0.22759768962860108,0.23214731216430665,0.22653789520263673,0.23019325733184814,0.2253483295440674,0.2214498996734619,0.23057703971862792,0.2215357780456543,0.224152135848999,0.22404680252075196,0.22494969367980958,0.23119945526123048,0.22830283641815186,0.22484498023986815,0.22299730777740479,0.21978497505187988,0.22464203834533691,0.225800085067749,0.22124271392822265,0.22770235538482667,0.21797480583190917,0.22616500854492189,0.21993424892425537,0.2203381061553955,0.23583998680114746,0.21766331195831298,0.21930527687072754,0.2268434524536133,0.22272591590881347,0.2186206102371216,0.22097306251525878,0.21327342987060546,0.21274590492248535,0.2160034656524658,0.2162710428237915,0.22273616790771483,0.22402610778808593,0.21890592575073242,0.21783592700958251,0.21823830604553224,0.22472085952758789,0.21435840129852296,0.2220147132873535,0.21783103942871093,0.22250807285308838,0.2212414026260376,0.220408034324646,0.2324829578399658,0.22929165363311768,0.21463990211486816,0.2209468364715576,0.21431515216827393,0.2187361717224121,0.2254115104675293,0.22513236999511718,0.22380917072296141,0.22386772632598878,0.21423146724700928,0.22871344089508056,0.21615951061248778,0.21384835243225098,0.21813421249389647,0.21229968070983887,0.2110881805419922,0.2175539493560791,0.2092970609664917,0.21230878829956054,0.21979713439941406,0.21904122829437256,0.21567978858947753,0.21213462352752685,0.22478065490722657,0.21334497928619384,0.21892859935760497,0.21631436347961425,0.23162193298339845,0.20908744335174562,0.2264326572418213,0.21389431953430177,0.21358165740966797,0.2131786823272705,0.21344482898712158,0.2181838035583496,0.21449708938598633,0.21727204322814941,0.21306958198547363,0.2137798547744751,0.21244354248046876,0.21013460159301758,0.21282095909118653,0.21487777233123778,0.20810351371765137,0.21967852115631104,0.21843595504760743,0.2213191032409668,0.20244112014770507,0.21032209396362306,0.21258823871612548,0.21717844009399415,0.21665418148040771,0.21226863861083983,0.21268792152404786,0.21454172134399413,0.2187119483947754,0.21499061584472656,0.2128155469894409,0.2239879846572876,0.2041708707809448,0.2124263286590576,0.20437850952148437,0.21758358478546141,0.20942394733428954,0.21527421474456787,0.21198902130126954,0.22194437980651854,0.2127455234527588,0.21398544311523438,0.20508954524993897,0.2126542091369629,0.21961503028869628,0.2125772476196289,0.20639429092407227,0.21239328384399414,0.20765583515167235,0.20402741432189941,0.20775289535522462,0.20637822151184082,0.20583393573760986,0.21397509574890136,0.21187112331390381,0.21029276847839357,0.20565125942230225,0.206145715713501,0.21225376129150392,0.20758304595947266,0.21388888359069824,0.2131364345550537,0.20779964923858643,0.21091244220733643,0.2074005126953125,0.21021366119384766,0.21817076206207275,0.2047571659088135,0.20889592170715332,0.20610437393188477,0.20463948249816893,0.20661039352416993,0.21356449127197266,0.20613837242126465,0.20619773864746094,0.21450190544128417,0.20552363395690917,0.21920452117919922,0.2111687183380127,0.2039796829223633,0.2030266284942627,0.20213942527770995,0.2009286403656006,0.20007658004760742,0.20876724720001222,0.20591578483581544,0.20778419971466064,0.21096768379211425,0.21824889183044432,0.206510591506958,0.21573457717895508,0.2098855495452881,0.21003007888793945,0.21216588020324706,0.21138925552368165,0.20273470878601074,0.21553528308868408,0.20804905891418457,0.21339640617370606,0.21071457862854004,0.21035237312316896,0.19973058700561525,0.21781384944915771,0.20279171466827392,0.19962575435638427,0.2100700855255127,0.21307382583618165,0.20993247032165527,0.20483360290527344,0.20491867065429686,0.19953887462615966,0.2069241523742676,0.21788473129272462,0.20167675018310546,0.20086522102355958,0.21652657985687257,0.21104204654693604,0.2011025667190552,0.20867500305175782,0.2140634536743164,0.20443053245544435,0.21814651489257814,0.20670433044433595,0.21153602600097657,0.20401856899261475,0.21239254474639893,0.2199531078338623,0.2149174451828003,0.203871750831604,0.20828170776367189,0.1989126443862915,0.2065415382385254,0.20563123226165772,0.20499272346496583,0.21300437450408935,0.20503637790679932,0.19647891521453859,0.20296814441680908,0.205031156539917,0.20697374343872071,0.2225498676300049,0.20853180885314943,0.20646142959594727,0.20727217197418213,0.2037261724472046,0.20699784755706788,0.21123785972595216,0.2000959873199463,0.19819331169128418,0.2072962760925293,0.2073892116546631,0.21445953845977783,0.20679726600646972,0.19878804683685303,0.21813840866088868,0.21395583152770997,0.2079798698425293,0.21293694972991944,0.19753491878509521,0.20404927730560302,0.20116569995880126,0.21207613945007325,0.20671324729919432,0.2091156005859375,0.20382423400878907,0.20647735595703126,0.20857689380645753,0.19456608295440675,0.20602796077728272,0.20415241718292237,0.2054037094116211,0.20730843544006347,0.20328128337860107,0.21204168796539308,0.19024137258529664,0.20061535835266114,0.20580296516418456,0.20443315505981446,0.2015003204345703,0.19509170055389405,0.20128414630889893,0.20699639320373536,0.2053677558898926,0.20520155429840087,0.2065579652786255,0.20466928482055663,0.19816796779632567,0.20547127723693848,0.2052905321121216,0.20766921043395997,0.20167880058288573,0.204101037979126,0.19450374841690063,0.1941365361213684,0.20637857913970947,0.20616888999938965,0.19909001588821412,0.19913384914398194,0.19583150148391723,0.2046668529510498,0.20032315254211425,0.2083869218826294,0.20190978050231934,0.20017881393432618,0.20506844520568848,0.2028421401977539,0.20026755332946777,0.20148792266845703,0.2057830810546875,0.19973617792129517,0.19986945390701294,0.20439453125,0.20517516136169434,0.20826034545898436,0.20423710346221924,0.20556893348693847,0.20029492378234864,0.19232532978057862,0.19440557956695556,0.19535284042358397,0.19362277984619142,0.2074741840362549,0.20545377731323242,0.20779719352722167,0.20225157737731933,0.21214826107025148,0.21020293235778809,0.20444881916046143,0.19928650856018065,0.19639365673065184,0.20653486251831055,0.20161685943603516,0.20473685264587402,0.20892677307128907,0.2026207685470581,0.20442600250244142,0.20289111137390137,0.20211493968963623,0.1996915817260742,0.20025889873504638,0.2048107147216797,0.2075809955596924,0.21719350814819335,0.19954123497009277,0.20444116592407227,0.20611672401428222,0.19671744108200073,0.20666017532348632,0.20345253944396974,0.20232064723968507,0.20453643798828125,0.20003180503845214,0.19323354959487915,0.19318501949310302,0.2070817232131958,0.19661974906921387,0.19452431201934814,0.1913759708404541,0.19224846363067627,0.2023228168487549,0.203745698928833,0.19531028270721434,0.20108442306518554,0.211771821975708,0.19903277158737182,0.2010322093963623,0.20353543758392334,0.1988120675086975,0.19783940315246581,0.20497143268585205,0.19194914102554322,0.19194490909576417,0.20894691944122315,0.20549421310424804,0.19403598308563233,0.19753457307815553,0.20108287334442138,0.19295002222061158,0.19760704040527344,0.19834280014038086,0.2048001766204834,0.20393855571746827,0.19382808208465577,0.1967734217643738,0.20534958839416503,0.19867634773254395,0.19761953353881836,0.19249747991561889,0.20409927368164063,0.20190117359161378,0.20383892059326172,0.20310382843017577,0.20938615798950194,0.20032715797424316,0.19814232587814332,0.19913122653961182,0.20105040073394775,0.1993328332901001,0.20265612602233887,0.1898747682571411,0.20732557773590088,0.19842801094055176,0.20321297645568848,0.18366276025772094,0.19419395923614502,0.20161876678466797,0.20221033096313476,0.20439438819885253,0.19750120639801025,0.19905548095703124,0.1949047327041626,0.19744627475738524,0.19570642709732056,0.20313630104064942,0.20635566711425782,0.20238280296325684,0.2092972755432129,0.19656102657318114,0.1938732624053955,0.19738715887069702,0.20345659255981446,0.19977720975875854,0.20269742012023925,0.2029205083847046,0.20925827026367189,0.20563297271728515,0.19551867246627808,0.2051384449005127,0.20587592124938964,0.1978827476501465,0.20875988006591797,0.20231208801269532,0.20559508800506593,0.20183074474334717,0.2099642276763916,0.19493980407714845,0.19810377359390258,0.190838086605072,0.20610153675079346,0.19392020702362062,0.19107000827789306,0.19674720764160156,0.20260281562805177,0.19947547912597657,0.2047776460647583,0.18977195024490356,0.19784855842590332,0.19510526657104493,0.19571449756622314,0.19737353324890136,0.19487416744232178,0.19372882843017578,0.1981360912322998,0.20537142753601073,0.199786376953125,0.19017997980117798,0.20638458728790282,0.20176320075988768,0.20982794761657714,0.19404809474945067,0.19495923519134523,0.1924616813659668,0.19735491275787354,0.20008013248443604,0.19664592742919923,0.20166389942169188,0.19679946899414064,0.19713430404663085,0.20393593311309816,0.20290346145629884,0.1892973303794861,0.20710954666137696,0.19599666595458984,0.19417734146118165,0.19552924633026122,0.19954917430877686,0.20016920566558838,0.18889992237091063,0.19914724826812744,0.1982709765434265,0.1974651575088501,0.2080928325653076,0.1968887209892273,0.19513287544250488,0.19597399234771729,0.19989074468612672,0.1944032669067383,0.1794045925140381,0.20315203666687012,0.19327085018157958,0.1959318161010742,0.1907082200050354,0.20654706954956054,0.20345702171325683,0.1938299059867859,0.20401511192321778,0.1938906192779541,0.19676361083984376,0.19701696634292604,0.19245952367782593,0.19001104831695556,0.1846681594848633,0.18629846572875977,0.20125408172607423,0.2008641242980957,0.19660511016845703,0.19275062084197997,0.19739148616790772,0.19106664657592773,0.19221053123474122,0.192158842086792,0.19392735958099366,0.20043869018554689,0.20296759605407716,0.19954086542129518,0.19829113483428956,0.19713138341903685,0.19535127878189087,0.19188642501831055,0.1889291763305664,0.19190495014190673,0.18659971952438353,0.19433497190475463,0.19410297870635987,0.20293700695037842,0.1999674081802368,0.20440957546234131,0.19546573162078856,0.19576895236968994,0.19957296848297118,0.2027383804321289,0.20148687362670897,0.20105338096618652,0.19694812297821046,0.19454379081726075,0.1961362361907959,0.201841402053833,0.1837336540222168,0.19382750988006592,0.1983873128890991,0.21161227226257323,0.19059665203094484,0.1915668487548828,0.19557305574417114,0.19656262397766114,0.18336243629455568,0.190142023563385,0.18872356414794922,0.19004676342010499,0.20224609375,0.19942266941070558,0.19731459617614747,0.1870410680770874,0.20067901611328126,0.196396541595459,0.19360074996948243,0.19397456645965577,0.1956021785736084,0.19821947813034058,0.1932232141494751,0.1956562876701355,0.1962408661842346,0.19332807064056395,0.19479069709777833,0.199227237701416,0.1872206449508667,0.18821408748626708,0.19200648069381715,0.20006973743438722,0.19201281070709228,0.20679011344909667,0.18814458847045898,0.19242334365844727,0.1893020510673523,0.1955198049545288,0.20673718452453613,0.1930195450782776,0.20187597274780272,0.18896708488464356,0.19727350473403932,0.1891983389854431,0.2003946304321289,0.1878139615058899,0.1864475965499878,0.19420909881591797,0.19180049896240234,0.19839097261428834,0.20010693073272706,0.19072028398513793,0.1915154218673706,0.19316813945770264,0.20103855133056642,0.18896447420120238,0.1971742868423462,0.20930745601654052,0.19043245315551757,0.20053224563598632,0.194793963432312,0.19718921184539795,0.19192522764205933,0.19286080598831176,0.1997461199760437,0.1842399001121521,0.1923372745513916,0.19547183513641359,0.18719754219055176,0.1909758448600769,0.19227988719940187,0.19508740901947022,0.20236423015594482,0.1878576397895813,0.19246139526367187,0.20110561847686767,0.19338942766189576,0.19060952663421632,0.20095019340515136,0.19667816162109375,0.1956615686416626,0.19808056354522705,0.20067188739776612,0.2048816204071045,0.19273898601531983,0.19754347801208497,0.19457740783691407,0.19980370998382568,0.19305503368377686,0.19869362115859984,0.2006291627883911,0.20067081451416016,0.19889737367630006,0.17949727773666382,0.1913440704345703,0.19519131183624266,0.19826135635375977,0.19851951599121093,0.19871165752410888,0.18526346683502198,0.1869646430015564,0.19425640106201172,0.20019853115081787,0.19406604766845703,0.20384042263031005,0.19938285350799562,0.19888991117477417,0.19797180891036986,0.20027999877929686,0.19423446655273438,0.19651877880096436,0.19090158939361573,0.1905277967453003,0.1967395305633545,0.20027732849121094,0.18961658477783203,0.2004310369491577,0.19003739356994628,0.19555301666259767,0.1962714672088623,0.18288545608520507,0.18964297771453859,0.19966357946395874,0.18316782712936402,0.19136816263198853,0.18217172622680664,0.19705817699432374,0.18735114336013795,0.1976862668991089,0.19558658599853515,0.19924519062042237,0.20270247459411622,0.19954519271850585,0.18983614444732666,0.20061960220336914,0.19317508935928346,0.19988083839416504,0.20067639350891114,0.19682811498641967,0.18929781913757324,0.19207241535186767,0.18381671905517577,0.19653419256210328,0.19300990104675292,0.19480702877044678,0.19398293495178223,0.18168375492095948,0.19150789976119995,0.20031211376190186,0.19421792030334473,0.20135946273803712,0.17967947721481323,0.1937732458114624,0.18704067468643187,0.18995471000671388,0.1911646842956543,0.19029536247253417,0.19531736373901368,0.20179979801177977,0.18654446601867675,0.19142775535583495,0.18838768005371093,0.19064643383026122,0.19419522285461427,0.18908635377883912,0.18464528322219848,0.1823430299758911,0.18268920183181764,0.18303518295288085,0.19255666732788085,0.1958217740058899,0.19476964473724365,0.19802956581115722,0.19827656745910643,0.19163987636566163,0.20147604942321778,0.18994184732437133,0.18367561101913452,0.19428606033325196,0.19642359018325806,0.20072007179260254,0.19603874683380126,0.18951550722122193,0.19837052822113038,0.20491552352905273,0.20111217498779296,0.1947739005088806,0.1989282488822937,0.18757519721984864,0.1970726728439331,0.18549752235412598,0.19088211059570312,0.19072723388671875,0.18799521923065185,0.19566168785095214,0.1878948211669922,0.18905316591262816,0.19819644689559937,0.19041218757629394,0.192383074760437,0.18654558658599854,0.1949033260345459,0.20018932819366456,0.19238196611404418,0.20434703826904296,0.19516499042510987,0.18697493076324462,0.1877169132232666,0.18625003099441528,0.18565518856048585,0.19144620895385742,0.18794100284576415,0.1936173677444458,0.19695996046066283,0.19345725774765016,0.18596566915512086,0.19091315269470216,0.1838421940803528,0.190503990650177,0.20929155349731446,0.1984090805053711,0.19464011192321778,0.1945224404335022,0.1960292100906372,0.20287389755249025,0.18336901664733887,0.1906125783920288,0.19001004695892335,0.18723896741867066,0.18698530197143554,0.19276106357574463,0.20077335834503174,0.20480914115905763,0.20200693607330322,0.1934194564819336,0.1935546875,0.20187859535217284,0.19278309345245362,0.18811842203140258,0.18322848081588744,0.18921921253204346,0.18991031646728515,0.19663147926330565,0.18989957571029664,0.1912734866142273,0.19060426950454712,0.1935110569000244,0.19450931549072265,0.19625009298324586,0.18658491373062133,0.1868677020072937,0.1938357710838318,0.19115763902664185,0.18621654510498048,0.1928149700164795,0.18960232734680177,0.18840081691741944,0.20134027004241944,0.1827068328857422,0.20040030479431153,0.18943912982940675,0.19797887802124023,0.2001250982284546,0.18902864456176757,0.18629640340805054,0.195927631855011,0.18876020908355712,0.19472224712371827,0.18970357179641723,0.19488208293914794,0.1937694787979126,0.1869083523750305,0.18771368265151978,0.1917874574661255,0.18669997453689574,0.19476262331008912,0.1912301540374756,0.1834152102470398,0.18616735935211182,0.1869990587234497,0.19150269031524658,0.18752472400665282,0.2026216983795166,0.19169702529907226,0.19702953100204468,0.20023624897003173,0.1843424677848816,0.1905122995376587,0.1912853717803955,0.19703208208084105,0.18842355012893677,0.1911095380783081,0.18459937572479249,0.18816092014312744,0.19617477655410767,0.18873732089996337,0.18416717052459716,0.1795417547225952,0.1949856162071228,0.19655725955963135,0.1966136336326599,0.20146491527557372,0.18842021226882935,0.20215890407562256,0.19504750967025758,0.19620193243026735,0.18690159320831298,0.19914029836654662,0.19557931423187255,0.19396429061889647,0.18903642892837524,0.19545890092849733,0.1967179775238037,0.1949352502822876,0.1944063663482666,0.19207513332366943,0.20108537673950194,0.19351004362106322,0.18981813192367553,0.17742583751678467,0.20297541618347167,0.18291027545928956,0.18423073291778563,0.1760242223739624,0.19923019409179688,0.19369041919708252,0.18643032312393187,0.19912369251251222,0.20853135585784913,0.18849819898605347,0.19565693140029908,0.18702707290649415,0.18009275197982788,0.1849587917327881,0.19506027698516845,0.18892778158187867,0.1926035165786743,0.18726954460144044,0.18535311222076417,0.1902547836303711,0.19507675170898436,0.18832049369812012,0.1954153895378113,0.19089996814727783,0.19921172857284547,0.18781943321228028,0.18502682447433472,0.18994075059890747,0.19030702114105225,0.2005141258239746,0.18983277082443237,0.20084950923919678,0.19951393604278564,0.19311034679412842,0.1921406865119934,0.19377598762512208,0.18949342966079713,0.18077294826507567,0.19747884273529054,0.1954742431640625,0.1861850619316101,0.19157968759536742,0.18686885833740235,0.18285603523254396,0.1906137466430664,0.1835684061050415,0.19817105531692505,0.18815711736679078,0.1968076467514038,0.18468692302703857,0.19495575428009032,0.19537261724472046,0.18324680328369142,0.18430101871490479,0.18837404251098633,0.18572533130645752,0.19722168445587157,0.1909511685371399,0.18919296264648439,0.1941304922103882,0.1846964955329895,0.17986296415328978,0.18842134475708008,0.1988292694091797,0.19199329614639282,0.18017098903656006,0.190805983543396,0.19935171604156493,0.19158636331558226,0.18840254545211793,0.188490092754364,0.17866392135620118,0.1932908058166504,0.18932664394378662,0.19458673000335694,0.19213664531707764,0.19630043506622313,0.19846577644348146,0.19581034183502197,0.18570486307144166,0.19543604850769042,0.18949928283691406,0.20007033348083497,0.1911023736000061,0.1887449026107788,0.19189143180847168,0.18900301456451415,0.19113781452178955,0.19806063175201416,0.1962034821510315,0.18628060817718506,0.20112259387969972,0.18841109275817872,0.1845021963119507,0.18503935337066652,0.17277170419692994,0.18886805772781373,0.18371255397796632,0.18930853605270387,0.187827730178833,0.19272923469543457,0.19630875587463378,0.18969978094100953,0.19362516403198243,0.18380318880081176,0.1965230107307434,0.1895202398300171,0.20304713249206544,0.19450877904891967,0.18253774642944337,0.18800115585327148,0.19087681770324708,0.18415751457214355,0.18888468742370607,0.1961532711982727,0.18692703247070314,0.18088936805725098,0.18696517944335939,0.19394015073776244,0.19756910800933838,0.1898625373840332,0.18979426622390747,0.1854427933692932,0.19233934879302977,0.1975492000579834,0.19071836471557618,0.17896673679351807,0.17492175102233887,0.192217218875885,0.1856555461883545,0.2046229600906372,0.18581585884094237,0.1872035264968872,0.19079264402389526,0.20382583141326904,0.19509537220001222,0.19623231887817383,0.1846634030342102,0.19464333057403566,0.18911752700805665,0.18419759273529052,0.19955973625183104,0.18373111486434937,0.1789307951927185,0.18405444622039796,0.19039301872253417,0.1866359829902649,0.18406398296356202,0.18461961746215821,0.18339065313339234,0.18889917135238649,0.19614572525024415,0.19367316961288453,0.19452853202819825,0.1881711483001709,0.19013423919677735,0.1825771450996399,0.18982484340667724,0.18016550540924073,0.18579316139221191,0.19238451719284058,0.18515944480895996,0.19228945970535277,0.19591280221939086,0.19288198947906493,0.1956325054168701,0.1812165379524231,0.18448500633239745,0.18709521293640136,0.1938093423843384,0.1901418685913086,0.18752245903015136,0.18500466346740724,0.18511191606521607,0.19535605907440184,0.1954448103904724,0.1891523241996765,0.1974287748336792,0.19240050315856932,0.19550920724868776,0.17819560766220094,0.19849653244018556,0.19386844635009765,0.18536070585250855,0.18861255645751954,0.20192499160766603,0.1758105993270874,0.18830951452255248,0.1863763928413391,0.18620517253875732,0.18011796474456787,0.19750417470932008,0.19117040634155275,0.19619250297546387,0.18799247741699218,0.1948544979095459,0.18859786987304689,0.1885809540748596,0.17544184923171996,0.18130627870559693,0.19011441469192505,0.18964011669158937,0.18108692169189453,0.18765027523040773,0.18589171171188354,0.18404912948608398,0.1941760540008545,0.18622145652770997,0.1935910701751709,0.1759623646736145,0.19494891166687012,0.18237488269805907,0.19297471046447753,0.1918753981590271,0.18974955081939698,0.18879714012145996,0.19410324096679688,0.18384552001953125,0.19181394577026367,0.1815265417098999,0.18130139112472535,0.1841283917427063,0.19362878799438477,0.18268048763275146,0.19850692749023438,0.19380248785018922,0.18678001165390015,0.18107502460479735,0.1865945816040039,0.1939430832862854,0.1914510488510132,0.19110716581344606,0.18576000928878783,0.18790794610977174,0.18723465204238893,0.17670236825942992,0.18018825054168702,0.197502064704895,0.18778548240661622,0.18840570449829103,0.18825907707214357,0.186384916305542,0.18301863670349122,0.1922771692276001,0.17778443098068236,0.18910503387451172,0.1906975746154785,0.1902850389480591,0.18407201766967773,0.1826912760734558,0.175424325466156,0.18625190258026122,0.18431928157806396,0.18759804964065552,0.19758117198944092,0.18219997882843017,0.19056529998779298,0.18140637874603271,0.17953052520751953,0.1918362021446228,0.18799130916595458,0.1835542678833008,0.1952712893486023,0.18396058082580566,0.18667306900024414,0.19422943592071534,0.18210747241973876,0.18570926189422607,0.1911268949508667,0.18261914253234862,0.19381940364837646,0.19240113496780395,0.18992149829864502,0.18933500051498414,0.18534928560256958,0.1856621503829956,0.1859550356864929,0.19187253713607788,0.19711835384368898,0.17166414260864257,0.19277154207229613,0.17902655601501466,0.1935058355331421,0.18751245737075806,0.18935235738754272,0.19230931997299194,0.1853639841079712,0.19277093410491944,0.18739669322967528,0.1900636911392212,0.19037327766418458,0.2041837453842163,0.18420332670211792,0.19006750583648682,0.18070836067199708,0.19592659473419188,0.18999919891357422,0.17775301933288573,0.17938673496246338,0.18322923183441162,0.1812167525291443,0.1882702589035034,0.18359193801879883,0.20623176097869872,0.19572557210922242,0.19484453201293944,0.19160406589508056,0.19082969427108765,0.20227971076965331,0.19431648254394532,0.17985200881958008,0.1795563817024231,0.1872174024581909,0.188527512550354,0.19292125701904297,0.1863471746444702,0.19382257461547853,0.19842398166656494,0.19190393686294555,0.1808093547821045,0.1801835536956787,0.1828492045402527,0.18183473348617554,0.17734882831573487,0.18611021041870118,0.19200687408447265,0.19221099615097045,0.1820622682571411,0.17469398975372313,0.18698419332504274,0.18306918144226075,0.18215267658233641,0.174393630027771,0.18493399620056153,0.1796344041824341,0.18068572282791137,0.17416894435882568,0.1816793918609619,0.18988685607910155,0.1895171284675598,0.18413450717926025,0.1898651361465454,0.19906833171844482,0.18278698921203612,0.18650345802307128,0.1802217960357666,0.18256793022155762,0.19281299114227296,0.19673490524291992,0.18447513580322267,0.18760578632354735,0.18360371589660646,0.19779539108276367,0.1968461513519287,0.1906769633293152,0.18763952255249022,0.18414618968963622,0.19248387813568116,0.17047882080078125,0.1931551694869995,0.18641122579574584,0.18393806219100953,0.19358081817626954,0.1736457586288452,0.18629591464996337,0.1864083766937256,0.19491214752197267,0.17488807439804077,0.19911257028579712,0.1880871534347534,0.18418915271759034,0.17515938282012938,0.19070031642913818,0.1953575372695923,0.18196732997894288,0.19574861526489257,0.1783895254135132,0.19290233850479127,0.18693130016326903,0.18476529121398927,0.1845690965652466,0.1816686749458313,0.1854773759841919,0.19157384634017943,0.19301145076751708,0.18163372278213502,0.1863977789878845,0.19077433347702027,0.18500034809112548,0.19222614765167237,0.18538320064544678,0.17851729393005372,0.19397616386413574,0.19760782718658448,0.1790957570075989,0.18257298469543456,0.19036216735839845,0.19230761528015136,0.19144597053527831,0.18242237567901612,0.18842086791992188,0.19097812175750734,0.18646761178970336,0.18161157369613648,0.18203240633010864,0.19633277654647827,0.1864619255065918,0.20228025913238526,0.17501089572906495,0.18916573524475097,0.18061137199401855,0.19511746168136596,0.19121607542037963,0.17820338010787964,0.1864327907562256,0.18342418670654298,0.18941420316696167,0.17522611618041992,0.1848926305770874,0.19917763471603395,0.1813103199005127,0.18276526927947997,0.18048421144485474,0.1944586753845215,0.18708984851837157,0.18926737308502198,0.20282046794891356,0.1907958745956421,0.18967883586883544,0.18037960529327393,0.18748195171356202,0.19887298345565796,0.1732189178466797,0.18366700410842896,0.1996598720550537,0.19171617031097413,0.19838093519210814,0.1835545778274536,0.1975634813308716,0.1865993022918701,0.18486758470535278,0.19203410148620606,0.18026454448699952,0.17412424087524414,0.1900846004486084,0.18055968284606932,0.18764649629592894,0.1934066653251648,0.18351551294326782,0.17823355197906493,0.18660778999328614,0.18199527263641357,0.17284142971038818,0.18869280815124512,0.18231287002563476,0.1828956723213196,0.19108837842941284,0.1842586636543274,0.19072222709655762,0.1861421585083008,0.18850998878479003,0.1802088737487793,0.18720003366470336,0.17575539350509645,0.1855452299118042,0.18903658390045167,0.19082585573196412,0.18096134662628174,0.1892135500907898,0.18933991193771363,0.18521831035614014,0.18474738597869872,0.19392868280410766,0.18143490552902222,0.17988688945770265,0.1918899416923523,0.19372050762176513,0.1899048089981079,0.1818220853805542,0.18142869472503662,0.19756770133972168,0.19232113361358644,0.1848614454269409,0.19169796705245973,0.18451426029205323,0.18869826793670655,0.18746583461761473,0.19085102081298827,0.18159301280975343,0.18795428276062012,0.18114185333251953,0.17951678037643432,0.18656445741653443,0.18001108169555663,0.19091811180114746,0.18535189628601073,0.17509400844573975,0.18000226020812987,0.1973610520362854,0.18760311603546143,0.17815933227539063,0.194234299659729,0.18298323154449464,0.18483736515045165,0.1865748643875122,0.18752923011779785,0.1777721405029297,0.17754149436950684,0.1978614568710327,0.18244142532348634,0.19412866830825806,0.17069087028503419,0.1803927540779114,0.17994898557662964,0.18580200672149658,0.18142205476760864,0.19170644283294677,0.1957672119140625,0.18890405893325807,0.19324924945831298,0.1867729663848877,0.19055750370025634,0.18750735521316528,0.1959429383277893,0.19176723957061767,0.1864429235458374,0.18247915506362916,0.17673861980438232,0.17851464748382567,0.1854541063308716,0.19241626262664796,0.18591612577438354,0.1962345838546753,0.18672279119491578,0.17703771591186523,0.1847494959831238,0.18456430435180665,0.18187984228134155,0.18116334676742554,0.1906025528907776,0.1861402988433838,0.19109859466552734,0.18705604076385499,0.19523845911026,0.18868987560272216,0.17508609294891359,0.184994900226593,0.17417744398117066,0.18814270496368407,0.18096879720687867,0.1812533736228943,0.17978328466415405,0.18350874185562133,0.18608341217041016,0.18549566268920897,0.1948627710342407,0.18352599143981935,0.17907650470733644,0.1904188871383667,0.18781666755676268,0.19589416980743407,0.17690010070800782,0.1825084686279297,0.19246820211410523,0.17953054904937743,0.19223146438598632,0.18876878023147584,0.18693004846572875,0.18312506675720214,0.17863156795501708,0.18543765544891358,0.18715115785598754,0.19179282188415528,0.18606449365615846,0.18120570182800294,0.185432505607605,0.17516138553619384,0.18493711948394775,0.20005042552948,0.1864743709564209,0.17961857318878174,0.18778930902481078,0.18698118925094603,0.18734047412872315,0.1856316566467285,0.18665863275527955,0.1866384744644165,0.18280405998229982,0.1704329252243042,0.19844270944595338,0.18383338451385497,0.18773910999298096,0.1818704605102539,0.20435543060302735,0.17778439521789552,0.17537519931793213,0.19112582206726075,0.18260688781738282,0.17911583185195923,0.18256785869598388,0.18594605922698976,0.17788147926330566,0.1835686206817627,0.1763349175453186,0.16746091842651367,0.191813063621521,0.17738155126571656,0.17922325134277345,0.19190044403076173,0.1798298716545105,0.19369184970855713,0.18186696767807006,0.18177592754364014,0.17584900856018065,0.18246299028396606,0.17564764022827148,0.17886807918548583,0.17933084964752197,0.18133201599121093,0.18059766292572021,0.1941676139831543,0.1884140968322754,0.18037333488464355,0.18414573669433593,0.19284353256225586,0.18056825399398804,0.19017248153686522,0.18268697261810302,0.17856991291046143,0.19130163192749022,0.18015551567077637,0.1776532292366028,0.17271443605422973,0.19101526737213134,0.18910232782363892,0.19308171272277833,0.1823718547821045,0.18162142038345336,0.17395532131195068,0.19266670942306519,0.17482279539108275,0.175661039352417,0.18805267810821533,0.18430163860321044,0.18427213430404663,0.18460707664489745,0.1878480553627014,0.19066333770751953,0.19301776885986327,0.1890626311302185,0.17727868556976317,0.1914791226387024,0.19573887586593627,0.17602038383483887,0.1871429204940796,0.19144833087921143,0.19747735261917115,0.18920431137084961,0.18439071178436278,0.18427562713623047,0.17926080226898194,0.19638789892196656,0.18298016786575316,0.18026976585388182,0.1856955409049988,0.18976725339889527,0.17687039375305175,0.17886725664138795,0.18653037548065185,0.17025703191757202,0.1795825958251953,0.18184928894042968,0.18071773052215576,0.1811908721923828,0.19625608921051024,0.17952365875244142,0.17490501403808595,0.18564729690551757,0.1894397497177124,0.20000600814819336,0.18864227533340455,0.17977278232574462,0.17941107749938964,0.18571267127990723,0.18994762897491455,0.18693464994430542,0.19222798347473144,0.18119211196899415,0.19775317907333373,0.1950355052947998,0.18638131618499756,0.18820533752441407,0.19051024913787842,0.1753687381744385,0.18502084016799927,0.1886531352996826,0.17729123830795288,0.18338028192520142,0.18356091976165773,0.19212048053741454,0.18355326652526854,0.18937270641326903,0.17569737434387206,0.18426414728164672,0.17999274730682374,0.18269991874694824,0.18840553760528564,0.1951878547668457,0.19090088605880737,0.16906116008758545,0.17852609157562255,0.1871195673942566,0.18148186206817626,0.18202719688415528,0.19260187149047853,0.1799781084060669,0.17583701610565186,0.1838576912879944,0.17685256004333497,0.17336902618408204,0.1889545202255249,0.1829939603805542,0.18157665729522704,0.18909684419631959,0.19374537467956543,0.1846845865249634,0.1915675163269043,0.18132082223892212,0.1833546757698059,0.17582497596740723,0.18254774808883667,0.1777919888496399,0.18467196226119995,0.18120521306991577,0.18896340131759642,0.1830343246459961,0.19204838275909425,0.18689827919006347,0.19467790126800538,0.19062119722366333,0.19070754051208497,0.17696375846862794,0.1826578140258789,0.18429511785507202,0.19228602647781373,0.18559564352035524,0.19254834651947023,0.18258371353149414,0.17948753833770753,0.17859065532684326,0.18421006202697754,0.19400674104690552,0.17970030307769774,0.18994286060333251,0.1795664668083191,0.18204542398452758,0.18062450885772705,0.1885787844657898,0.1846086025238037,0.18650780916213988,0.17873861789703369,0.1831045627593994,0.18409945964813232,0.18898625373840333,0.18654041290283202,0.19050700664520265,0.18446195125579834,0.19095909595489502,0.17487542629241942,0.17165894508361818,0.1817201018333435,0.18614230155944825,0.1761911153793335,0.1697524070739746,0.18808605670928955,0.17285510301589965,0.19010636806488038,0.19505951404571534,0.17919368743896485,0.17786784172058107,0.17573238611221315,0.1891421914100647,0.1946078658103943,0.1901771068572998,0.18086845874786378,0.194458270072937,0.1795002341270447,0.1765216112136841,0.1846470355987549,0.19450048208236695,0.18774338960647582,0.1753981113433838,0.18483831882476806,0.18710582256317138,0.1762209415435791,0.18779878616333007,0.18561415672302245,0.17973415851593016,0.17858970165252686,0.18292789459228515,0.18064873218536376,0.1662452220916748,0.18177952766418456,0.1750604271888733,0.17204302549362183,0.17688095569610596,0.1804969549179077,0.17818375825881957,0.18600653409957885,0.1843180775642395,0.18220850229263305,0.19118783473968506,0.19132949113845826,0.18547601699829103,0.17404048442840575,0.18887606859207154,0.1895250916481018,0.19323217868804932,0.17932417392730712,0.17527045011520387,0.1802952766418457,0.1846381187438965,0.17824842929840087,0.1864870309829712,0.17708200216293335,0.18277084827423096,0.18603509664535522,0.18416717052459716,0.17885913848876953,0.18143922090530396,0.1815488338470459,0.19326858520507811,0.18825647830963135,0.1746092677116394,0.19417691230773926,0.1786126136779785,0.18110805749893188,0.17669100761413575,0.17485449314117432,0.1863486647605896,0.1802419900894165,0.18016840219497682,0.18261123895645143,0.19292320013046266,0.18659462928771972,0.1827670454978943,0.18051331043243407,0.17527936697006224,0.18812987804412842,0.18532559871673585,0.1918940782546997,0.18751039505004882,0.18631948232650758,0.19367492198944092,0.17351030111312865,0.19755778312683106,0.18172764778137207,0.17960013151168824,0.1793668031692505,0.18418385982513427,0.1776931405067444,0.18048266172409058,0.18905205726623536,0.18345394134521484,0.18110846281051635,0.180558180809021,0.1698246717453003,0.17443827390670777,0.18456923961639404,0.17907118797302246,0.17971752882003783,0.19040460586547853,0.18370473384857178,0.1703965663909912,0.18101810216903685,0.19224979877471923,0.1903430700302124,0.18971896171569824,0.18315892219543456,0.1832284927368164,0.1928774356842041,0.19556877613067628,0.18582484722137452,0.18368680477142335,0.18561439514160155,0.1683105707168579,0.19449203014373778,0.17520033121109008,0.18219558000564576,0.18428406715393067,0.18444364070892333,0.18314634561538695,0.18161046504974365,0.18396785259246826,0.18342891931533814,0.18442193269729615,0.17963004112243652,0.17905921936035157,0.18910322189331055,0.18741408586502076,0.1893047571182251,0.19498289823532106,0.1797553539276123,0.18386473655700683,0.19751812219619752,0.18399364948272706,0.17442286014556885,0.19014121294021608,0.1807076096534729,0.18150415420532226,0.1794741630554199,0.18499093055725097,0.17048993110656738,0.17457232475280762,0.17577409744262695,0.18446011543273927,0.19306179285049438,0.1915474772453308,0.1873193621635437,0.18625848293304442,0.18386090993881227,0.19606926441192626,0.17756505012512208,0.1742769479751587,0.18567973375320435,0.16444830894470214,0.1817193865776062,0.1808098554611206,0.17403141260147095,0.18936600685119628,0.18158516883850098,0.17132329940795898,0.17913403511047363,0.17114120721817017,0.1820566773414612,0.18388946056365968,0.19464975595474243,0.17269580364227294,0.16754239797592163,0.18984646797180177,0.174955153465271,0.18771666288375854,0.18744659423828125,0.17571349143981935,0.17853448390960694,0.1890946388244629,0.17010302543640138,0.18727705478668213,0.17151925563812256,0.1759960412979126,0.16727957725524903,0.18801312446594237,0.17900214195251465,0.1790120244026184,0.19154257774353028,0.19188599586486815,0.18421998023986816,0.1809651494026184,0.18742798566818236,0.18073362112045288,0.18775302171707153,0.18445767164230348,0.18433256149291993,0.19104955196380616,0.1753277063369751,0.1816232681274414,0.18750008344650268,0.19272515773773194,0.17886260747909546,0.18437997102737427,0.17393879890441893,0.1774782657623291,0.18373286724090576,0.1821552038192749,0.18701013326644897,0.1817958116531372,0.18393616676330565,0.18585307598114015,0.1776890993118286,0.18041733503341675,0.18257344961166383,0.18552402257919312,0.17712082862854003,0.18015445470809938,0.18627486228942872,0.17357703447341918,0.17707998752593995,0.17283232212066652,0.17364716529846191,0.1900869131088257,0.1798018217086792,0.17402915954589843,0.1860015869140625,0.18419387340545654,0.18305453062057495,0.17770603895187378,0.18949670791625978,0.1912506103515625,0.18394627571105956,0.1812901973724365,0.18249919414520263,0.180228853225708,0.18587334156036378,0.17724156379699707,0.1844682812690735,0.17427098751068115,0.17489395141601563,0.1839498519897461,0.18116341829299926,0.1862663984298706,0.17324503660202026,0.18162059783935547,0.18238765001296997,0.18013758659362794,0.18716206550598144,0.18423973321914672,0.18707879781723022,0.1821852445602417,0.18529586791992186,0.17673404216766359,0.189531672000885,0.17934638261795044,0.18681395053863525,0.18726109266281127,0.17998332977294923,0.18009560108184813,0.19511393308639527,0.17044214010238648,0.18206520080566407,0.18292230367660522,0.17400273084640502,0.17842204570770265,0.17871915102005004,0.17170498371124268,0.18438791036605834,0.19075322151184082,0.18977712392807006,0.1824284553527832,0.17244837284088135,0.18708059787750245,0.1785244345664978,0.18473031520843505,0.1833367943763733,0.1819600224494934,0.17632609605789185,0.17872292995452882,0.1808344841003418,0.18895280361175537,0.17890914678573608,0.17825310230255126,0.18139200210571288,0.17883238792419434,0.19178283214569092,0.1714899182319641,0.17438472509384156,0.18199049234390258,0.18824126720428466,0.1752562403678894,0.18758176565170287,0.1776389002799988,0.1770646333694458,0.18185594081878662,0.18780033588409423,0.1823399543762207,0.18070745468139648,0.17505087852478027,0.1760348916053772,0.1873997688293457,0.1889682412147522,0.18378133773803712,0.17712395191192626,0.18691391944885255,0.16878275871276854,0.18645193576812744,0.1942044734954834,0.1723313808441162,0.18683494329452516,0.183981454372406,0.18697683811187743,0.18150074481964112,0.18490045070648192]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss by time\"};\n",
       "\n",
       "  Plotly.plot('plot-514606496', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrandom\u001b[39m: \u001b[32mRandom\u001b[39m = scala.util.Random@454aadb9\n",
       "\u001b[36mMiniBatchSize\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m256\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtrainData\u001b[39m\n",
       "\u001b[36mlossSeq\u001b[39m: \u001b[32mIndexedSeq\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m0.25571749210357664\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mplot\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mScatter\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mScatter\u001b[39m(\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mres8_5\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-514606496\"\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val random = new Random\n",
    "\n",
    "val MiniBatchSize = 256\n",
    "\n",
    "\n",
    "def trainData(randomIndexArray: Array[Int]): Double = {\n",
    "  val trainNDArray :: expectLabel :: shapeless.HNil =\n",
    "    ReadCIFAR10ToNDArray.getSGDTrainNDArray(randomIndexArray)\n",
    "  val input =\n",
    "    trainNDArray.reshape(MiniBatchSize, 3072)\n",
    "\n",
    "  val expectLabelVectorized =\n",
    "    Utils.makeVectorized(expectLabel, NumberOfClasses)\n",
    "  trainer.train(input :: expectLabelVectorized :: HNil)\n",
    "}\n",
    "\n",
    "val lossSeq =\n",
    "  (\n",
    "    for (iteration <- 0 to 50) yield {\n",
    "      val randomIndex = random\n",
    "        .shuffle[Int, IndexedSeq](0 until 10000) //https://issues.scala-lang.org/browse/SI-6948\n",
    "        .toArray\n",
    "      for (times <- 0 until 10000 / MiniBatchSize) yield {\n",
    "        val randomIndexArray =\n",
    "          randomIndex.slice(times * MiniBatchSize,\n",
    "                            (times + 1) * MiniBatchSize)\n",
    "          val loss = trainData(randomIndexArray)\n",
    "          if(times == 3 & iteration % 5 == 4){\n",
    "            println(\"at epoch \" + (iteration / 5 + 1) + \" loss is :\" + loss)\n",
    "          }\n",
    "          loss\n",
    "      }\n",
    "    }\n",
    "  ).flatten\n",
    "\n",
    "val plot = Seq(\n",
    "  Scatter(lossSeq.indices, lossSeq)\n",
    ")\n",
    "\n",
    "plot.plot(\n",
    "  title = \"loss by time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟上一节相同，我们使用测试数据来查看神经网络判断结果并计算准确率。这次准确率应该会上升到51%左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result is 46.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mright\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m46.0\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val right = Utils.getAccuracy(predictor.predict(testData), testExpectResult)\n",
    "println(s\"the result is $right %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "在这节中我们学到了：\n",
    "\n",
    "* 参数调优\n",
    "* L2Regularization\n",
    "* Relu\n",
    "* 构建一个两层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[完整代码](https://github.com/izhangzhihao/deeplearning-tutorial/blob/master/src/main/scala/com/thoughtworks/deeplearning/tutorial/TwoLayerNet.scala)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
