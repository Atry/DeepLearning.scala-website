{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景\n",
    "\n",
    "为了提高预测准确率，我们需要使用多层神经网络，因为一般来说网络的层数越多其表达能力越强，因为其参数更多，能表达出的状态信息更多，所以表达能力越强。多层神经网络可以应对更加复杂的问题。\n",
    "\n",
    "这一节我们会先定义一个简单的两层神经网络，然后使用[CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html)的训练集来训练这个神经网络。最后使用测试集来验证神经网络的准确率，最终预测准确率可以达到51%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 引入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                 \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.{FileInputStream, InputStream}\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableHList._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableDouble._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableAny._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray.Layers.Weight\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.{\n",
       "  DifferentiableHList,\n",
       "  DifferentiableINDArray,\n",
       "  Layer,\n",
       "  Symbolic\n",
       "}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Layer.Tape\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathFunctions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathMethods./\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathOps\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.cpu.nativecpu.NDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.ops.transforms.Transforms\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4s.Implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mshapeless._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.immutable.IndexedSeq\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                   ,ReadCIFAR10ToNDArray._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$    ,Utils._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $plugin.$ivy.`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:2.0.0`\n",
    "\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableany:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablenothing:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableseq:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiabledouble:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablefloat:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablehlist:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableindarray:1.0.0-RC7`\n",
    "import $ivy.`org.rauschig:jarchivelib:0.5.0`\n",
    "\n",
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.0`\n",
    "\n",
    "import java.io.{FileInputStream, InputStream}\n",
    "\n",
    "\n",
    "import com.thoughtworks.deeplearning\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import com.thoughtworks.deeplearning.DifferentiableHList._\n",
    "import com.thoughtworks.deeplearning.DifferentiableDouble._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray._\n",
    "import com.thoughtworks.deeplearning.DifferentiableAny._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray.Layers.Weight\n",
    "import com.thoughtworks.deeplearning.{\n",
    "  DifferentiableHList,\n",
    "  DifferentiableINDArray,\n",
    "  Layer,\n",
    "  Symbolic\n",
    "}\n",
    "import com.thoughtworks.deeplearning.Layer.Tape\n",
    "import com.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
    "import com.thoughtworks.deeplearning.Symbolic._\n",
    "import com.thoughtworks.deeplearning.Poly.MathFunctions._\n",
    "import com.thoughtworks.deeplearning.Poly.MathMethods./\n",
    "import com.thoughtworks.deeplearning.Poly.MathOps\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
    "import org.nd4j.linalg.ops.transforms.Transforms\n",
    "import org.nd4s.Implicits._\n",
    "import shapeless._\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._\n",
    "\n",
    "import scala.collection.immutable.IndexedSeq\n",
    "import scala.util.Random\n",
    "\n",
    "pprintConfig() = pprintConfig().copy(height = 2)\n",
    "\n",
    "import $file.ReadCIFAR10ToNDArray\n",
    "import $file.Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 构建两层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 参数调优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟前一节不同，这节我们加入一些参数调优的手段，设置学习率和使用[L2Regularization](http://neuralnetworksanddeeplearning.com/chap3.html),L2Regularization可以用来避免[过拟合](https://en.wikipedia.org/wiki/Overfitting)。我们还使用了每个迭代`learningRate`都下降为原来的0.9995倍的办法来解决训练时间增长时因为`learningRate`相对太大导致`loss`下降太慢或者不下降的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36moptimizerFactory\u001b[39m: \u001b[32mAnyRef\u001b[39m with \u001b[32mOptimizerFactory\u001b[39m = $sess.cmd1Wrapper$Helper$$anon$2@755f507c"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit val optimizerFactory = new DifferentiableINDArray.OptimizerFactory {\n",
    "  override def ndArrayOptimizer(weight: Weight): Optimizer = {\n",
    "    new LearningRate with L2Regularization {\n",
    "\n",
    "      var learningRate = 0.001\n",
    "\n",
    "      override protected def currentLearningRate(): Double = {\n",
    "        learningRate *= 0.9995\n",
    "        learningRate\n",
    "      }\n",
    "\n",
    "      override protected def l2Regularization: Double = 0.03\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 编写第一层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是全连接和[relu](http://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-network)组成的神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfullyConnectedThenRelu\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fullyConnectedThenRelu(inputSize: Int, outputSize: Int)(\n",
    "    implicit row: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val w = (Nd4j.randn(inputSize, outputSize) / math.sqrt(outputSize / 2.0)).toWeight * 0.1\n",
    "  val b = Nd4j.zeros(outputSize).toWeight\n",
    "  max((row dot w) + b, 0.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 编写第二层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟上节相同，我们使用`softmax`作为分类器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msoftmax\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(implicit scores: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val expScores = exp(scores)\n",
    "  expScores / expScores.sum(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编写两层神经网络的第二层神经网络，这是一层全连接和一层`softmax`组成的的神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfullyConnectedThenSoftmax\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fullyConnectedThenSoftmax(inputSize: Int, outputSize: Int)(\n",
    "    implicit row: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val w = (Nd4j.randn(inputSize, outputSize) / math.sqrt(outputSize)).toWeight //* 0.1\n",
    "  val b = Nd4j.zeros(outputSize).toWeight\n",
    "  softmax.compose((row dot w) + b)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 组合两层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了实现两层神经网络我们使用`compose`将上面的两层神经网络组合起来，组成一个两层神经网络。`a.compose(b)`可以将`b`的输出作为`a`输入从而将两层神经网络组合起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mhiddenLayer\u001b[39m\n",
       "\u001b[36mpredictor\u001b[39m: (\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m[\u001b[32mINDArray\u001b[39m]{type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray;type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray})#\u001b[32m@\u001b[39m = Compose(Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum(Exp(Identity()),WrappedArray(1)))),PlusINDArray(Dot(Identity(),Weight([[-0.58, 0.40, -0.39, 0.22\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val NumberOfPixels: Int = 3072\n",
    "def hiddenLayer(implicit input: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val layer0 = fullyConnectedThenRelu(NumberOfPixels, 500).compose(input)\n",
    "  fullyConnectedThenSoftmax(500, 10).compose(layer0)\n",
    "}\n",
    "\n",
    "val predictor = hiddenLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 编写`network`并组合输入层和[隐含层](http://stats.stackexchange.com/questions/63152/what-does-the-hidden-layer-in-a-neural-network-compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcrossEntropy\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mnetwork\u001b[39m\n",
       "\u001b[36mtrainer\u001b[39m: (\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m[\u001b[32mDouble\u001b[39m]{type OutputData = Double;type OutputDelta = Double;type InputData = shapeless.::[org.nd4j.linalg.api.ndarray.INDArray,shapeless.::[org.nd4j.linalg.api.ndarray.INDArray,shapeless.HNil]];type InputDelta = shapeless.:+:[org.nd4j.linalg.api.ndarray.INDArray,shapeless.:+:[org.nd4j.linalg.api.ndarray.INDArray,shapeless.CNil]]})#\u001b[32m@\u001b[39m = Compose(Negative(ReduceMean(PlusINDArray(MultiplyINDArray(Head(Tail(Identity())),Log(PlusDouble(MultiplyDouble(Head(Identity()),Literal(0.9)),Literal(0.1)))),Mu\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def crossEntropy(\n",
    "    implicit pair: (INDArray :: INDArray :: HNil) @Symbolic): Double @Symbolic = {\n",
    "  val score = pair.head\n",
    "  val label = pair.tail.head\n",
    "  -(label * log(score * 0.9 + 0.1) + (1.0 - label) * log(1.0 - score * 0.9)).mean\n",
    "}\n",
    "\n",
    "def network(\n",
    "   implicit pair: (INDArray :: INDArray :: HNil) @Symbolic): Double @Symbolic = {\n",
    "  val input = pair.head\n",
    "  val label = pair.tail.head\n",
    "  val score: INDArray @Symbolic = predictor.compose(input)\n",
    "  val hnilLayer: HNil @Symbolic = HNil\n",
    "  crossEntropy.compose(score :: label :: hnilLayer)\n",
    "}\n",
    "\n",
    "val trainer = network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 训练神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与上一节相同，训练神经网络并观察每次训练`loss`的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1 loss is :0.22178800106048585\n",
      "at epoch 2 loss is :0.20296504497528076\n",
      "at epoch 3 loss is :0.19683592319488524\n",
      "at epoch 4 loss is :0.18512377738952637\n",
      "at epoch 5 loss is :0.19063875675201417\n",
      "at epoch 6 loss is :0.19085261821746827\n",
      "at epoch 7 loss is :0.18278274536132813\n",
      "at epoch 8 loss is :0.1842024564743042\n",
      "at epoch 9 loss is :0.18770427703857423\n",
      "at epoch 10 loss is :0.18082261085510254\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-1087245905\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0],\"y\":[0.2716786861419678,0.2616023063659668,0.2565687417984009,0.2654921770095825,0.25346462726593016,0.28282694816589354,0.26446962356567383,0.31708388328552245,0.28445897102355955,0.342868709564209,0.40210781097412107,0.3704195499420166,0.41074542999267577,0.4073797225952148,0.4102479457855225,0.41781177520751955,0.422137451171875,0.4155092239379883,0.39554831981658933,0.39075841903686526,0.35440864562988283,0.4097589492797852,0.408931303024292,0.3981560468673706,0.40746474266052246,0.36974682807922366,0.3786647081375122,0.298990535736084,0.2896981954574585,0.2823279857635498,0.2850463390350342,0.27144551277160645,0.2700038909912109,0.26634554862976073,0.26280148029327394,0.2624965190887451,0.2562645196914673,0.25397093296051027,0.24770946502685548,0.25833544731140134,0.2490297794342041,0.25166902542114256,0.25121397972106935,0.245157790184021,0.24727604389190674,0.2462770462036133,0.2486046552658081,0.24752197265625,0.2476195812225342,0.24116175174713134,0.24264259338378907,0.24305014610290526,0.24592833518981932,0.2400975227355957,0.24423091411590575,0.2462794303894043,0.23632726669311524,0.24077510833740234,0.23529481887817383,0.23888072967529297,0.24227259159088135,0.23784193992614747,0.23969755172729493,0.2416470766067505,0.24278030395507813,0.23746495246887206,0.24149613380432128,0.2352424144744873,0.22808938026428222,0.2508204936981201,0.24044997692108155,0.23775110244750977,0.2373861312866211,0.236112642288208,0.2399817705154419,0.2361588478088379,0.23705630302429198,0.2322537899017334,0.23228657245635986,0.22861874103546143,0.23518879413604737,0.23586742877960204,0.23332045078277588,0.22564759254455566,0.23565988540649413,0.22807881832122803,0.22839550971984862,0.22755722999572753,0.23030030727386475,0.222892427444458,0.23135697841644287,0.22652289867401124,0.2280907154083252,0.22978339195251465,0.22242293357849122,0.22141492366790771,0.22996435165405274,0.22509322166442872,0.2307673692703247,0.22502217292785645,0.23277578353881836,0.23096516132354736,0.23479149341583253,0.22727513313293457,0.22711868286132814,0.22514333724975585,0.22593984603881836,0.22319259643554687,0.21841702461242676,0.22626757621765137,0.22039659023284913,0.21404016017913818,0.22453842163085938,0.2262209177017212,0.2166142702102661,0.22532038688659667,0.22047085762023927,0.2195650815963745,0.22293970584869385,0.21740131378173827,0.21860108375549317,0.21838231086730958,0.22454729080200195,0.22527081966400148,0.22429726123809815,0.21481223106384278,0.22035250663757325,0.2138547420501709,0.22317051887512207,0.21900062561035155,0.2241905689239502,0.22851872444152832,0.22297232151031493,0.21407406330108641,0.22376446723937987,0.2145068883895874,0.21547365188598633,0.21340436935424806,0.21595845222473145,0.21828513145446776,0.2217660665512085,0.22364583015441894,0.22241387367248536,0.21550185680389405,0.21812136173248292,0.2075063705444336,0.2207075834274292,0.21996562480926513,0.21328599452972413,0.2155221462249756,0.21544220447540283,0.22190413475036622,0.20711419582366944,0.21674044132232667,0.2145303249359131,0.21884331703186036,0.20880699157714844,0.22599904537200927,0.222039794921875,0.22178800106048585,0.2248387575149536,0.20596075057983398,0.21756315231323242,0.20954313278198242,0.2177424430847168,0.20834596157073976,0.2193596839904785,0.2157155990600586,0.20665035247802735,0.21652483940124512,0.21279969215393066,0.2180802583694458,0.20335960388183594,0.2141435146331787,0.21105940341949464,0.20591251850128173,0.21328988075256347,0.21225218772888182,0.2083575963973999,0.21474339962005615,0.2062445640563965,0.2184173583984375,0.20893440246582032,0.21029930114746093,0.20921406745910645,0.21276319026947021,0.2020188331604004,0.21528136730194092,0.2123469591140747,0.21814019680023194,0.22051868438720704,0.20872313976287843,0.2112126588821411,0.2192659854888916,0.21356680393218994,0.21547188758850097,0.20838470458984376,0.20483672618865967,0.21346688270568848,0.21311416625976562,0.20945281982421876,0.21647329330444337,0.2142638921737671,0.2094273567199707,0.21482787132263184,0.20999839305877685,0.20328295230865479,0.20587763786315919,0.19942587614059448,0.21407406330108641,0.20866191387176514,0.21463894844055176,0.20765280723571777,0.20005722045898439,0.21002402305603027,0.20675160884857177,0.2086402416229248,0.20485234260559082,0.2106860399246216,0.22142443656921387,0.21144227981567382,0.2069321870803833,0.20543992519378662,0.21397242546081544,0.20303559303283691,0.2042146682739258,0.20369548797607423,0.21735124588012694,0.2036890745162964,0.21853771209716796,0.21298859119415284,0.2037881851196289,0.20469238758087158,0.2173175811767578,0.2051161766052246,0.20750751495361328,0.21100826263427735,0.20738492012023926,0.20691823959350586,0.20363144874572753,0.2094794273376465,0.2115706443786621,0.2192075729370117,0.21327056884765624,0.209220814704895,0.21006410121917723,0.20214762687683105,0.2112752914428711,0.21132373809814453,0.20242087841033934,0.21742920875549315,0.19788612127304078,0.208512544631958,0.207055926322937,0.21553893089294435,0.20194611549377442,0.21560988426208497,0.21293294429779053,0.21269469261169432,0.20515453815460205,0.20357518196105956,0.20086255073547363,0.1996533155441284,0.2005389928817749,0.19473650455474853,0.21135926246643066,0.1988115668296814,0.20742807388305665,0.21139295101165773,0.21154866218566895,0.21564555168151855,0.20573701858520507,0.20392241477966308,0.20712952613830565,0.2108762741088867,0.20389204025268554,0.2037862777709961,0.2071850538253784,0.20036990642547609,0.20574193000793456,0.21034595966339112,0.2000124931335449,0.20353994369506836,0.2114251136779785,0.2091853141784668,0.20205063819885255,0.20635628700256348,0.2053656816482544,0.2031332015991211,0.19826047420501708,0.2137296676635742,0.20869121551513672,0.20876812934875488,0.20638701915740967,0.2031454086303711,0.19721977710723876,0.20807533264160155,0.21760210990905762,0.20502688884735107,0.2029780626296997,0.19946943521499633,0.20544886589050293,0.20718417167663575,0.209572696685791,0.20719995498657226,0.21114654541015626,0.19926025867462158,0.20879437923431396,0.21253376007080077,0.21156210899353028,0.20717756748199462,0.20357069969177247,0.21247444152832032,0.19843952655792235,0.2075495719909668,0.2117889404296875,0.20240190029144287,0.19684724807739257,0.20435633659362792,0.20161476135253906,0.20917105674743652,0.19751520156860353,0.2065974235534668,0.19557801485061646,0.2087301254272461,0.20301954746246337,0.20612239837646484,0.21190438270568848,0.19480794668197632,0.19972848892211914,0.20328099727630616,0.20720794200897216,0.19880698919296264,0.19736753702163695,0.21239900588989258,0.1950239658355713,0.2013538360595703,0.20483415126800536,0.20353994369506836,0.19762637615203857,0.20102248191833497,0.20199718475341796,0.21089763641357423,0.21467742919921876,0.20748963356018066,0.19742363691329956,0.20196118354797363,0.2032787322998047,0.1952112317085266,0.2014765739440918,0.2003340244293213,0.21000988483428956,0.20437335968017578,0.19696201086044313,0.20296504497528076,0.2001558303833008,0.1908036947250366,0.1945570945739746,0.20736536979675294,0.20312852859497071,0.20300779342651368,0.2029435634613037,0.1925528883934021,0.2046536922454834,0.19302494525909425,0.2025923252105713,0.21398344039916992,0.20084142684936523,0.21029884815216066,0.21128082275390625,0.20575351715087892,0.19290237426757811,0.1932906150817871,0.19478459358215333,0.20884435176849364,0.21363115310668945,0.20732808113098145,0.21011500358581542,0.20596089363098144,0.20417194366455077,0.19719221591949462,0.2048619270324707,0.20633878707885742,0.19853563308715821,0.19936078786849976,0.19665547609329223,0.19608047008514404,0.2008761167526245,0.19397810697555543,0.1997562050819397,0.20829191207885742,0.1959272265434265,0.193962562084198,0.20936737060546876,0.20243115425109864,0.19753049612045287,0.21416001319885253,0.19494245052337647,0.19580495357513428,0.19996702671051025,0.1916330099105835,0.20515069961547852,0.20316708087921143,0.20815689563751222,0.20269513130187988,0.2017672061920166,0.19818627834320068,0.21465756893157958,0.19334182739257813,0.202980375289917,0.2004244804382324,0.19602439403533936,0.20345282554626465,0.207936692237854,0.20378689765930175,0.19864351749420167,0.20064311027526854,0.1955059289932251,0.19762003421783447,0.20198259353637696,0.2079925298690796,0.20716142654418945,0.20541696548461913,0.20145740509033203,0.19712010622024537,0.20573024749755858,0.2050769090652466,0.18961130380630492,0.19894633293151856,0.20375144481658936,0.19375476837158204,0.20358765125274658,0.19913495779037477,0.20654935836791993,0.20226821899414063,0.2035374164581299,0.19559791088104247,0.1877908945083618,0.20168495178222656,0.19857112169265748,0.20499377250671386,0.18699591159820556,0.20203242301940919,0.19546834230422974,0.2055349588394165,0.19850022792816163,0.20605933666229248,0.21217069625854493,0.20414719581604004,0.20734870433807373,0.20245828628540039,0.19770853519439696,0.1961605191230774,0.1987051248550415,0.19782932996749877,0.19538626670837403,0.18924763202667236,0.19620200395584106,0.2051788330078125,0.2032402753829956,0.20196185111999512,0.20270321369171143,0.194918429851532,0.2043677806854248,0.1883954644203186,0.19695448875427246,0.194564151763916,0.1973889112472534,0.1839050054550171,0.20146458148956298,0.19468457698822023,0.20524320602416993,0.21629343032836915,0.20129642486572266,0.2035212278366089,0.19678364992141723,0.20569007396697997,0.19391897916793824,0.2021308422088623,0.18917243480682372,0.20003576278686525,0.189460289478302,0.20203714370727538,0.2055375576019287,0.19899632930755615,0.2024311065673828,0.1950075387954712,0.19923117160797119,0.19053456783294678,0.19858126640319823,0.209037184715271,0.20211822986602784,0.196742045879364,0.19527056217193603,0.21255977153778077,0.19625937938690186,0.19717726707458497,0.19555083513259888,0.2013068675994873,0.19075368642807006,0.20108964443206787,0.19786467552185058,0.19799482822418213,0.19145984649658204,0.201263952255249,0.19150543212890625,0.18817334175109862,0.18995248079299926,0.2024308919906616,0.19084627628326417,0.19268381595611572,0.19283580780029297,0.18418705463409424,0.2010751485824585,0.20830292701721193,0.20125722885131836,0.20544967651367188,0.1975540280342102,0.19560803174972535,0.19274227619171141,0.19181654453277588,0.2007072925567627,0.19285742044448853,0.18854013681411744,0.20307638645172119,0.19519646167755128,0.19020719528198243,0.20558631420135498,0.195646071434021,0.19564781188964844,0.19571075439453126,0.2067127227783203,0.19656176567077638,0.19784451723098756,0.19119125604629517,0.19229614734649658,0.1918931484222412,0.1883752703666687,0.207212495803833,0.19023025035858154,0.1954085111618042,0.19297301769256592,0.19899059534072877,0.19503285884857177,0.19954792261123658,0.19638280868530272,0.19068042039871216,0.20228104591369628,0.19263705015182495,0.19683592319488524,0.20061707496643066,0.20073022842407226,0.19206607341766357,0.187030029296875,0.19544901847839355,0.20288562774658203,0.19908816814422609,0.20504555702209473,0.19225234985351564,0.19280534982681274,0.1888291358947754,0.19056260585784912,0.19567201137542725,0.20602431297302246,0.20508794784545897,0.19774320125579833,0.1947385311126709,0.20069944858551025,0.19354147911071778,0.19764947891235352,0.2057328224182129,0.19563199281692506,0.20673892498016358,0.19987063407897948,0.19600402116775512,0.18221452236175537,0.19298985004425048,0.19393359422683715,0.19353234767913818,0.19734300374984742,0.19375190734863282,0.1960301160812378,0.20527079105377197,0.19517121315002442,0.2135791778564453,0.19648160934448242,0.18358607292175294,0.18560537099838256,0.20133590698242188,0.20407700538635254,0.18704657554626464,0.20514392852783203,0.19614168405532836,0.18917132616043092,0.19453904628753663,0.19054533243179322,0.1989408016204834,0.1924160122871399,0.18444466590881348,0.19642648696899415,0.19385242462158203,0.1911420226097107,0.19484058618545533,0.2012723922729492,0.1966855764389038,0.18758891820907592,0.19853124618530274,0.1973721981048584,0.2015977144241333,0.20567431449890136,0.19584283828735352,0.20114483833312988,0.19537796974182128,0.18804012537002562,0.1974175453186035,0.19197045564651488,0.19683068990707397,0.20384814739227294,0.19337787628173828,0.1934633731842041,0.19502819776535035,0.19745299816131592,0.1974316120147705,0.1848718523979187,0.20214433670043946,0.20046796798706054,0.19780131578445434,0.19971715211868285,0.19606668949127198,0.21381673812866211,0.18524229526519775,0.20075612068176268,0.19942543506622315,0.1894500732421875,0.19947500228881837,0.18673124313354492,0.1975801706314087,0.18024964332580568,0.19787800312042236,0.1979688286781311,0.1978445291519165,0.19685757160186768,0.18455435037612916,0.1900714874267578,0.19928200244903566,0.19213690757751464,0.19637128114700317,0.18424060344696044,0.20912642478942872,0.20074760913848877,0.20032744407653807,0.18723458051681519,0.20808265209197999,0.19468951225280762,0.19381392002105713,0.1875481128692627,0.19953304529190063,0.20502684116363526,0.19791265726089477,0.19311048984527587,0.19078423976898193,0.18923213481903076,0.19510526657104493,0.1958003282546997,0.2004786968231201,0.19271934032440186,0.20053844451904296,0.19227094650268556,0.19051940441131593,0.1917688250541687,0.19604979753494262,0.1987274169921875,0.18987321853637695,0.18779935836791992,0.1958160161972046,0.20593101978302003,0.19039528369903563,0.18348278999328613,0.1984678864479065,0.19026510715484618,0.1938116192817688,0.20346477031707763,0.19125065803527833,0.19014959335327147,0.19627174139022827,0.19794553518295288,0.189355731010437,0.1914829730987549,0.20351736545562743,0.1987937331199646,0.20253825187683105,0.20116007328033447,0.19540574550628662,0.1869194984436035,0.19072310924530028,0.18684148788452148,0.19068011045455932,0.19955224990844728,0.20051827430725097,0.19014593362808227,0.19426522254943848,0.19593205451965331,0.19369723796844482,0.18919103145599364,0.18714255094528198,0.18457334041595458,0.1940520167350769,0.1937514066696167,0.19770662784576415,0.18106684684753419,0.1998207688331604,0.19445737600326538,0.19043962955474852,0.18673774003982543,0.1949845552444458,0.19553155899047853,0.19778010845184327,0.18855395317077636,0.20066585540771484,0.20374541282653807,0.1903160333633423,0.18473212718963622,0.1918867349624634,0.19984676837921142,0.1997271180152893,0.2004566192626953,0.20296032428741456,0.19341826438903809,0.1911189317703247,0.18631536960601808,0.19067237377166749,0.18366851806640624,0.19460685253143312,0.18979370594024658,0.1887371063232422,0.1963394522666931,0.19805115461349487,0.19429779052734375,0.19228999614715575,0.18786284923553467,0.1988887906074524,0.19188005924224855,0.20953624248504638,0.18418400287628173,0.18512377738952637,0.1956575632095337,0.19846198558807374,0.19296960830688475,0.2052537202835083,0.19519034624099732,0.19125624895095825,0.18793861865997313,0.19110457897186278,0.1873205542564392,0.19332964420318605,0.19954084157943724,0.19103028774261474,0.19716006517410278,0.18243221044540406,0.20647218227386474,0.19021402597427367,0.1887526273727417,0.1914098858833313,0.20102176666259766,0.18726931810379027,0.19432497024536133,0.20014443397521972,0.20486025810241698,0.1965736746788025,0.18396166563034058,0.19817256927490234,0.1913904666900635,0.1820246934890747,0.18488622903823854,0.18926901817321778,0.17806423902511598,0.1977088451385498,0.19119046926498412,0.1973900318145752,0.18948142528533934,0.19709343910217286,0.1846842885017395,0.20264506340026855,0.18292349576950073,0.19681878089904786,0.19666266441345215,0.1853335976600647,0.1848174214363098,0.18608194589614868,0.19704573154449462,0.19562146663665772,0.1947232961654663,0.20474047660827638,0.1816157579421997,0.19221017360687256,0.18705971240997316,0.19982582330703735,0.20059545040130616,0.18995308876037598,0.19426788091659547,0.20031921863555907,0.1832858920097351,0.1968955636024475,0.1960055112838745,0.18898730278015136,0.19643629789352418,0.20342960357666015,0.19953901767730714,0.1849285125732422,0.19345129728317262,0.1947885513305664,0.1889098882675171,0.185190749168396,0.19052276611328126,0.1806679368019104,0.18855119943618776,0.19612839221954345,0.19397138357162474,0.18112308979034425,0.19628777503967285,0.18895978927612306,0.201993727684021,0.1900019645690918,0.19351119995117189,0.19013880491256713,0.1936988949775696,0.1938871145248413,0.18997323513031006,0.1906963109970093,0.19311070442199707,0.1860688328742981,0.19347857236862182,0.1943300485610962,0.18652646541595458,0.19990648031234742,0.18254783153533935,0.18948466777801515,0.19168546199798583,0.1950591802597046,0.18773555755615234,0.19016014337539672,0.19428682327270508,0.1893457889556885,0.1879405379295349,0.18536050319671632,0.18395020961761474,0.1902659296989441,0.1824898600578308,0.19535515308380128,0.19759397506713866,0.1915708065032959,0.1839306592941284,0.19720207452774047,0.20156450271606446,0.1934088110923767,0.18437738418579103,0.1824666976928711,0.19423907995224,0.19151883125305175,0.1902613639831543,0.18748220205307006,0.19065194129943847,0.190993332862854,0.19322482347488404,0.19668824672698976,0.19161763191223144,0.19167876243591309,0.1867603540420532,0.18412139415740966,0.18981695175170898,0.1909586787223816,0.19552266597747803,0.19558382034301758,0.19500908851623536,0.19431161880493164,0.1851162552833557,0.18351001739501954,0.2009824275970459,0.18908580541610717,0.1966591000556946,0.1953190803527832,0.18027570247650146,0.1905134677886963,0.19624931812286378,0.2049254894256592,0.18664506673812867,0.18311879634857178,0.19924309253692626,0.18815418481826782,0.18586883544921876,0.19060957431793213,0.1838872790336609,0.19411336183547973,0.18336150646209717,0.18355703353881836,0.20198345184326172,0.18508785963058472,0.18703991174697876,0.18181676864624025,0.18573524951934814,0.17880204916000367,0.1862412452697754,0.18846763372421266,0.18444311618804932,0.2009202718734741,0.1783600330352783,0.19527208805084229,0.1971404194831848,0.1862809658050537,0.1998566508293152,0.19232813119888306,0.19600486755371094,0.18811724185943604,0.1938876748085022,0.19120209217071532,0.18846888542175294,0.1975534200668335,0.19153867959976195,0.20363144874572753,0.1879258394241333,0.18609756231307983,0.18540241718292236,0.17696164846420287,0.1822983741760254,0.1883362889289856,0.18581709861755372,0.19296567440032958,0.18527199029922486,0.18952327966690063,0.19293326139450073,0.19385892152786255,0.1931595802307129,0.20037496089935303,0.19820942878723144,0.18657395839691163,0.19703011512756347,0.18702952861785888,0.18370211124420166,0.1931980848312378,0.19063875675201417,0.18784971237182618,0.1921883463859558,0.19155826568603515,0.19721064567565919,0.18796651363372802,0.18880844116210938,0.18736734390258789,0.18743598461151123,0.18888413906097412,0.19559015035629274,0.1809756875038147,0.19277591705322267,0.19327527284622192,0.19477685689926147,0.19535746574401855,0.18845274448394775,0.18617582321166992,0.20295338630676268,0.19745321273803712,0.19589947462081908,0.19294962882995606,0.17369855642318727,0.17741992473602294,0.18866536617279053,0.1949143409729004,0.19104537963867188,0.18288850784301758,0.1823767066001892,0.19540525674819947,0.18814035654067993,0.18961774110794066,0.1932459592819214,0.19366605281829835,0.18381593227386475,0.1870576858520508,0.19628938436508178,0.191576087474823,0.18326084613800048,0.18920459747314453,0.17949070930480956,0.19728890657424927,0.19395021200180054,0.19282026290893556,0.19913778305053711,0.19244093894958497,0.18376775979995727,0.189603853225708,0.19582186937332152,0.17260270118713378,0.18850020170211793,0.17820903062820434,0.19350855350494384,0.18644753694534302,0.1899259328842163,0.19364652633666993,0.19726185798645018,0.1938169002532959,0.19390301704406737,0.19284114837646485,0.18900521993637084,0.18133630752563476,0.18558648824691773,0.19026610851287842,0.2016213893890381,0.18994925022125245,0.19148902893066405,0.2027596950531006,0.17517904043197632,0.1801443576812744,0.1825861930847168,0.18897922039031984,0.18691264390945433,0.18884263038635254,0.19122501611709594,0.18909322023391723,0.19296789169311523,0.19093314409255982,0.17662762403488158,0.18982574939727784,0.1917828917503357,0.20363049507141112,0.19147299528121947,0.18171069622039795,0.18758778572082518,0.20051870346069336,0.18545944690704347,0.18812296390533448,0.1821497678756714,0.1934552788734436,0.18457167148590087,0.1866913080215454,0.18134534358978271,0.18084477186203002,0.20531303882598878,0.18527296781539918,0.18543992042541504,0.18713603019714356,0.17889564037322997,0.19524073600769043,0.17873084545135498,0.19782191514968872,0.18843502998352052,0.18705641031265258,0.17912487983703612,0.18567709922790526,0.18395882844924927,0.18597569465637206,0.1903006076812744,0.19068340063095093,0.1892642378807068,0.20047271251678467,0.18719959259033203,0.19066513776779176,0.18638899326324462,0.19378063678741456,0.18671121597290039,0.1898745059967041,0.1893019676208496,0.19309905767440796,0.194904363155365,0.18718382120132446,0.18793220520019532,0.17975151538848877,0.1875084638595581,0.1884690999984741,0.19541229009628297,0.19633853435516357,0.19149353504180908,0.18819017410278321,0.19499292373657226,0.18748073577880858,0.1906403422355652,0.19834754467010499,0.18285117149353028,0.19501606225967408,0.183160400390625,0.18917129039764405,0.18987590074539185,0.18935012817382812,0.19623042345046998,0.18960063457489013,0.18529034852981568,0.18360433578491211,0.18192325830459594,0.19442211389541625,0.19709274768829346,0.18952698707580568,0.18543825149536133,0.19068864583969117,0.18880705833435057,0.19083611965179442,0.17798211574554443,0.19845776557922362,0.19113646745681762,0.1899725914001465,0.18760340213775634,0.1770010471343994,0.18413547277450562,0.1824882984161377,0.1900997757911682,0.18742506504058837,0.17299386262893676,0.18467020988464355,0.19087613821029664,0.19391337633132935,0.18961737155914307,0.18156423568725585,0.19347875118255614,0.19490342140197753,0.19546475410461425,0.1907687544822693,0.18407754898071288,0.1930375576019287,0.18332951068878173,0.1851294755935669,0.18964970111846924,0.19092321395874023,0.18888932466506958,0.19398181438446044,0.18584692478179932,0.187253737449646,0.1938083291053772,0.1875150203704834,0.18978742361068726,0.18226473331451415,0.18140887022018432,0.18896427154541015,0.18022265434265136,0.17993550300598143,0.19631253480911254,0.19032766819000244,0.19054416418075562,0.18889834880828857,0.19027507305145264,0.19085261821746827,0.20936567783355714,0.18590805530548096,0.1895785927772522,0.1875150203704834,0.18488994836807252,0.1945994973182678,0.1903902292251587,0.19805521965026857,0.1710026502609253,0.1804685115814209,0.17945485115051268,0.2021099805831909,0.19211651086807252,0.18398609161376953,0.19706087112426757,0.19974324703216553,0.18751739263534545,0.18387818336486816,0.17782528400421144,0.19026122093200684,0.19061758518218994,0.19335614442825316,0.19591870307922363,0.18770281076431275,0.18841016292572021,0.17920070886611938,0.20304350852966307,0.18254947662353516,0.188649320602417,0.18180015087127685,0.18587310314178468,0.19122833013534546,0.19187101125717163,0.17399653196334838,0.1879436731338501,0.18647029399871826,0.18333295583724976,0.1955191373825073,0.18335909843444825,0.20450124740600586,0.18239704370498658,0.17481399774551393,0.19189095497131348,0.18943549394607545,0.18385076522827148,0.18564867973327637,0.1952862024307251,0.19229098558425903,0.17897288799285888,0.1889479160308838,0.18637559413909913,0.180979323387146,0.1959715962409973,0.18876080513000487,0.18612949848175048,0.18202857971191405,0.18836408853530884,0.1882091999053955,0.18721823692321776,0.19137928485870362,0.19429938793182372,0.18929448127746581,0.19200800657272338,0.18782441616058348,0.18375445604324342,0.1961594820022583,0.18858110904693604,0.19201010465621948,0.19239944219589233,0.19427721500396727,0.1858399033546448,0.1940675973892212,0.19110140800476075,0.1836725354194641,0.18374984264373778,0.1844504952430725,0.18999327421188356,0.18679569959640502,0.181675922870636,0.19413313865661622,0.1858983039855957,0.18243597745895385,0.18169677257537842,0.19255285263061522,0.1784231185913086,0.19182226657867432,0.19168541431427003,0.19044370651245118,0.1852252244949341,0.18905084133148192,0.18952043056488038,0.18287631273269653,0.17744100093841553,0.18448731899261475,0.19261194467544557,0.1892428755760193,0.20024213790893555,0.18554325103759767,0.1934603452682495,0.1952492117881775,0.19196971654891967,0.18568845987319946,0.19395605325698853,0.18381544351577758,0.17306864261627197,0.17843602895736693,0.18762381076812745,0.18685356378555298,0.19127368927001953,0.1822805643081665,0.1822413206100464,0.18456380367279052,0.18423035144805908,0.19660812616348267,0.18078529834747314,0.18256287574768065,0.189141845703125,0.1840615153312683,0.18298975229263306,0.19342705011367797,0.18033862113952637,0.18490562438964844,0.18632317781448365,0.19252759218215942,0.18276761770248412,0.1911347508430481,0.19553818702697753,0.18996607065200805,0.1856403350830078,0.18845558166503906,0.18310540914535522,0.191232430934906,0.17577171325683594,0.18871500492095947,0.1945757269859314,0.17933192253112792,0.18313934803009033,0.1744696855545044,0.17709658145904542,0.19067986011505128,0.17980314493179322,0.1859417200088501,0.20036468505859376,0.19059454202651976,0.18147921562194824,0.18996065855026245,0.19153591394424438,0.1845794916152954,0.17607462406158447,0.18852379322052001,0.18466558456420898,0.18321001529693604,0.19441293478012084,0.17549347877502441,0.17983536720275878,0.19221158027648927,0.1908156991004944,0.1897528886795044,0.19057499170303344,0.1820059299468994,0.1847909212112427,0.17685766220092775,0.18720780611038207,0.17776272296905518,0.19062538146972657,0.2051483392715454,0.19079493284225463,0.18122923374176025,0.19297887086868287,0.18747373819351196,0.1807490587234497,0.19452396631240845,0.20521166324615478,0.18203303813934327,0.18320919275283815,0.20112743377685546,0.1804645299911499,0.1834731101989746,0.1849440336227417,0.18515462875366212,0.1848474621772766,0.1894287109375,0.19225330352783204,0.17671843767166137,0.18193166255950927,0.1828213334083557,0.18885316848754882,0.18931932449340821,0.18551337718963623,0.18141753673553468,0.18041889667510985,0.17830395698547363,0.18537018299102784,0.18196314573287964,0.18278274536132813,0.18413127660751344,0.176794695854187,0.1842097520828247,0.18678677082061768,0.18435567617416382,0.18187812566757203,0.19805700778961183,0.17112783193588257,0.18898801803588866,0.1758101463317871,0.19362915754318238,0.18595726490020753,0.1899864673614502,0.18578004837036133,0.18215651512145997,0.18735471963882447,0.1912211537361145,0.19559571743011475,0.19041229486465455,0.18771634101867676,0.18914166688919068,0.1727988004684448,0.18495447635650636,0.189011287689209,0.18303227424621582,0.17654353380203247,0.18902772665023804,0.19412310123443605,0.1829704999923706,0.17661664485931397,0.190247106552124,0.18636943101882936,0.18251549005508422,0.18334904909133912,0.18478264808654785,0.18351095914840698,0.1849764347076416,0.17909250259399415,0.18317027091979982,0.18533048629760743,0.1964329719543457,0.19328842163085938,0.18707884550094606,0.17656537294387817,0.17697079181671144,0.19024357795715333,0.18040742874145507,0.18415679931640624,0.18773878812789918,0.19266633987426757,0.1730966091156006,0.18163156509399414,0.1887983798980713,0.19086847305297852,0.18779797554016114,0.17429707050323487,0.18846065998077394,0.18746109008789064,0.1845536708831787,0.1834881067276001,0.18365683555603027,0.19054923057556153,0.17943108081817627,0.18115632534027098,0.18650345802307128,0.1764984369277954,0.1796701192855835,0.18009276390075685,0.18060364723205566,0.17950769662857055,0.20114946365356445,0.18777329921722413,0.1918484687805176,0.1753587007522583,0.187952721118927,0.1805471658706665,0.18261559009552003,0.18436062335968018,0.1896573543548584,0.19046090841293334,0.18753662109375,0.18664828538894654,0.1945636034011841,0.17433650493621827,0.20190441608428955,0.1862042188644409,0.18269401788711548,0.18335720300674438,0.18403902053833007,0.18289809226989745,0.1633238434791565,0.18588767051696778,0.18992186784744264,0.18415879011154174,0.18723578453063966,0.18059282302856444,0.18482716083526612,0.17693023681640624,0.1841031789779663,0.17729485034942627,0.18687198162078858,0.19304184913635253,0.19212546348571777,0.19215813875198365,0.17495341300964357,0.17965087890625,0.1891395092010498,0.1883086919784546,0.18517643213272095,0.18909647464752197,0.1824096441268921,0.1833379864692688,0.18864818811416625,0.18445379734039308,0.17739899158477784,0.18856236934661866,0.1778108596801758,0.18673359155654906,0.1873778820037842,0.18016133308410645,0.1914064884185791,0.19092986583709717,0.19143941402435302,0.1827192187309265,0.18794353008270265,0.19464406967163086,0.1918445944786072,0.19344905614852906,0.17820940017700196,0.1955573081970215,0.18055295944213867,0.1891542673110962,0.18010275363922118,0.19221298694610595,0.18978424072265626,0.19483492374420167,0.17774643898010253,0.19268811941146852,0.18545422554016114,0.17573096752166747,0.18720600605010987,0.181569242477417,0.18530571460723877,0.18356776237487793,0.1940396785736084,0.18384579420089722,0.1760568618774414,0.17989652156829833,0.18659332990646363,0.18526241779327393,0.16552348136901857,0.18306272029876708,0.17043927907943726,0.18127280473709106,0.17833497524261474,0.1809333086013794,0.1925201654434204,0.18848807811737062,0.1885012149810791,0.19140148162841797,0.18439583778381347,0.18196637630462648,0.18534382581710815,0.18827030658721924,0.1870267391204834,0.17854191064834596,0.19023528099060058,0.1845068573951721,0.1803761601448059,0.17999579906463622,0.18890864849090577,0.19170591831207276,0.1891191005706787,0.17968287467956542,0.18696000576019287,0.17926170825958251,0.19521429538726806,0.17980369329452514,0.17129098176956176,0.17741137742996216,0.1849661111831665,0.17128249406814575,0.18219635486602784,0.18281785249710084,0.18583920001983642,0.1929747939109802,0.18460050821304322,0.184310245513916,0.18171899318695067,0.1852940082550049,0.1845705270767212,0.18269239664077758,0.18345071077346803,0.18197293281555177,0.1842024564743042,0.1874111294746399,0.1706406593322754,0.1924433946609497,0.18559943437576293,0.18967961072921752,0.19114657640457153,0.18030660152435302,0.1878040075302124,0.18181817531585692,0.1785257339477539,0.18551698923110962,0.1916435480117798,0.19902334213256836,0.1892242431640625,0.18329074382781982,0.1746265172958374,0.19216568470001222,0.17824386358261107,0.1892254114151001,0.18820306062698364,0.1869364857673645,0.18967227935791015,0.1760002613067627,0.17648658752441407,0.18687435388565063,0.17829563617706298,0.17976901531219483,0.18996875286102294,0.18746384382247924,0.17751381397247315,0.19710748195648192,0.1817075490951538,0.18740220069885255,0.1832801103591919,0.17931008338928223,0.17493927478790283,0.18434064388275145,0.1868032693862915,0.1771479606628418,0.18450303077697755,0.1814180374145508,0.18942395448684693,0.19093137979507446,0.18995655775070192,0.18240511417388916,0.18186644315719605,0.19371283054351807,0.1965127944946289,0.1772036075592041,0.18481501340866088,0.18891124725341796,0.18354238271713258,0.19061121940612794,0.188970148563385,0.18514467477798463,0.16880311965942382,0.18350032567977906,0.19095054864883423,0.18487286567687988,0.18950221538543702,0.17759490013122559,0.18556760549545287,0.17131283283233642,0.17292280197143556,0.1824678897857666,0.17997647523880006,0.1827087163925171,0.18389227390289306,0.17598488330841064,0.18050131797790528,0.1816336393356323,0.19029239416122437,0.1734701633453369,0.17987072467803955,0.17813310623168946,0.18313150405883788,0.1762104034423828,0.17721261978149414,0.16962592601776122,0.1860821008682251,0.18852448463439941,0.19510989189147948,0.1838517665863037,0.17870956659317017,0.1922934651374817,0.1849842071533203,0.1850951910018921,0.17644410133361815,0.17603731155395508,0.1893343448638916,0.18235777616500853,0.18929200172424315,0.19819966554641724,0.17521098852157593,0.181805419921875,0.18061904907226561,0.17936625480651855,0.18550981283187867,0.18991971015930176,0.18858565092086793,0.18177397251129152,0.17399704456329346,0.17174534797668456,0.18958876132965088,0.18686299324035643,0.18921082019805907,0.17940239906311034,0.1795818567276001,0.1886721134185791,0.20329842567443848,0.18445193767547607,0.17318882942199706,0.17771198749542236,0.17225942611694336,0.181069278717041,0.1811951756477356,0.18770053386688232,0.19015668630599974,0.19083008766174317,0.1872847318649292,0.18225653171539308,0.1800801396369934,0.19336402416229248,0.17798380851745604,0.16882777214050293,0.16293761730194092,0.1752002716064453,0.1885021448135376,0.18532122373580934,0.19408040046691893,0.18502880334854127,0.195365047454834,0.19388893842697144,0.1812278151512146,0.18271946907043457,0.18215895891189576,0.18858065605163574,0.1896353006362915,0.18887481689453126,0.17352046966552734,0.1832432508468628,0.19333808422088622,0.19641048908233644,0.19708068370819093,0.17960023880004883,0.1761861801147461,0.18399398326873778,0.17908897399902343,0.17558413743972778,0.18124890327453613,0.19131755828857422,0.17349790334701537,0.1829930067062378,0.18543537855148315,0.1911094903945923,0.18430126905441285,0.1838019847869873,0.18756453990936278,0.18065869808197021,0.18439760208129882,0.18737707138061524,0.18575845956802367,0.17881838083267212,0.1905056953430176,0.17788176536560057,0.19382314682006835,0.17154276371002197,0.18192462921142577,0.19206392765045166,0.1772865891456604,0.1751018524169922,0.18558099269866943,0.18346128463745118,0.18476309776306152,0.18508071899414064,0.1862364411354065,0.18405615091323851,0.17277331352233888,0.18197336196899414,0.1870021104812622,0.18543041944503785,0.1879974126815796,0.18184058666229247,0.1878656268119812,0.17548913955688478,0.17893569469451903,0.18079992532730102,0.18178226947784423,0.18180220127105712,0.18462553024291992,0.1796320915222168,0.18474245071411133,0.19483933448791504,0.18951835632324218,0.18770427703857423,0.1854834198951721,0.18257441520690917,0.1830950379371643,0.17732490301132203,0.1838466763496399,0.1803413987159729,0.18736083507537843,0.17102723121643065,0.18577191829681397,0.18224506378173827,0.18468906879425048,0.17956364154815674,0.1774055242538452,0.18301103115081788,0.18363585472106933,0.18379833698272705,0.1766297698020935,0.1893549680709839,0.18858847618103028,0.18082170486450194,0.1822510004043579,0.1879805564880371,0.18278571367263793,0.19674112796783447,0.17922312021255493,0.1798674941062927,0.18020654916763307,0.18388962745666504,0.17727583646774292,0.18948304653167725,0.1823996901512146,0.18529535531997682,0.17709498405456542,0.17964375019073486,0.17743415832519532,0.18619911670684813,0.17308411598205567,0.17542295455932616,0.18121559619903566,0.1783539056777954,0.18285549879074098,0.17555661201477052,0.18645440340042113,0.18861572742462157,0.1762269616127014,0.18195236921310426,0.1762484312057495,0.19928970336914062,0.18520755767822267,0.18249295949935912,0.18980414867401124,0.18252129554748536,0.18392659425735475,0.18456326723098754,0.17670750617980957,0.17989517450332643,0.18778976202011108,0.192421293258667,0.1823190212249756,0.18648238182067872,0.18624889850616455,0.17322448492050171,0.170784854888916,0.17511744499206544,0.18079700469970703,0.17455101013183594,0.18921818733215331,0.1807806134223938,0.18368543386459352,0.18681068420410157,0.1843278169631958,0.18085289001464844,0.18968427181243896,0.18169739246368408,0.18107231855392455,0.1895737648010254,0.19318641424179078,0.1920685887336731,0.18936771154403687,0.18442729711532593,0.17874064445495605,0.18494800329208375,0.18363490104675292,0.18034005165100098,0.18646994829177857,0.17719556093215943,0.17695354223251342,0.1705009937286377,0.1819488525390625,0.1716388940811157,0.17851662635803223,0.18374216556549072,0.17885725498199462,0.17853083610534667,0.181806743144989,0.18353348970413208,0.19036890268325807,0.17782968282699585,0.1792558550834656,0.18817579746246338,0.18848063945770263,0.17419326305389404,0.1868046522140503,0.19245028495788574,0.1752036690711975,0.18523526191711426,0.1809819221496582,0.17917850017547607,0.17723578214645386,0.18695523738861083,0.18271255493164062,0.17827402353286742,0.17983886003494262,0.1810467004776001,0.181573486328125,0.18519482612609864,0.18497302532196044,0.18050849437713623,0.1856032133102417,0.17554969787597657,0.18233447074890136,0.17976945638656616,0.17604026794433594,0.17779322862625122,0.17916072607040406,0.1833577036857605,0.18629579544067382,0.1823711395263672,0.17958732843399047,0.18203039169311525,0.18788681030273438,0.19327759742736816,0.19338880777359008,0.18972818851470946,0.18379883766174315,0.17947856187820435,0.1918848752975464,0.1904377818107605,0.18207063674926757,0.18295661211013795,0.18538910150527954,0.18901388645172118,0.17638903856277466,0.18106400966644287,0.17708888053894042,0.18369669914245607,0.16893985271453857,0.18458821773529052,0.19001201391220093,0.18087415695190429,0.18025407791137696,0.17973504066467286,0.1792264461517334,0.18094543218612671,0.16945886611938477,0.17931873798370362,0.18353304862976075,0.18918776512145996,0.17827411890029907,0.18430798053741454,0.17391930818557738,0.1880105495452881,0.18669464588165283,0.18383805751800536,0.1858680009841919,0.18539154529571533,0.17562425136566162,0.16547337770462037,0.17852475643157958,0.18055572509765624,0.17984485626220703,0.1838567852973938,0.1834431529045105,0.1739778161048889,0.18505651950836183,0.1734497904777527,0.18458847999572753,0.1791972517967224,0.18415594100952148,0.17709102630615234,0.1786039352416992,0.17642996311187745,0.18176555633544922,0.1907954216003418,0.1854156494140625,0.18413846492767333,0.18352197408676146,0.17584555149078368,0.1863994002342224,0.17664041519165039,0.1813417911529541,0.17819939851760863,0.1857708215713501,0.18083738088607787,0.18082261085510254,0.1825181007385254,0.17775074243545533,0.18395979404449464,0.18193471431732178,0.1804153800010681,0.18850038051605225,0.1856001615524292,0.1865929365158081,0.19123822450637817,0.17230786085128785,0.17556145191192626,0.1777480959892273,0.19877759218215943,0.18299858570098876,0.1676041841506958,0.17909843921661378,0.17476508617401124,0.19323717355728148,0.19875168800354004,0.1832873821258545,0.1921250343322754,0.1850822925567627,0.1788878083229065,0.19301974773406982,0.18500359058380128,0.18190557956695558,0.18367335796356202,0.17628111839294433,0.19084384441375732,0.175559401512146,0.18854763507843017,0.18207687139511108,0.1957646131515503,0.17611273527145385,0.18184263706207277,0.1787840962409973,0.16671853065490722,0.18733432292938232,0.1786919951438904,0.17933199405670167,0.1854986071586609,0.19190332889556885,0.17886279821395873,0.16612181663513184,0.1837447166442871,0.18964110612869262,0.18626730442047118,0.18455309867858888,0.1751423954963684,0.1830858588218689,0.18840041160583496,0.17430477142333983,0.18857049942016602,0.17506178617477416,0.18274326324462892,0.1782182812690735,0.18031680583953857,0.1762162208557129,0.1712283968925476,0.1832317590713501,0.17073974609375,0.1875817894935608,0.17884351015090943,0.17387838363647462,0.18221138715744017,0.17203130722045898,0.17289406061172485,0.188277006149292,0.1748943567276001,0.17956783771514892,0.17823101282119752,0.187196683883667,0.17925900220870972,0.18999407291412354]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss by time\"};\n",
       "\n",
       "  Plotly.plot('plot-1087245905', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrandom\u001b[39m: \u001b[32mRandom\u001b[39m = scala.util.Random@284770f4\n",
       "\u001b[36mMiniBatchSize\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m256\u001b[39m\n",
       "\u001b[36mNumberOfClasses\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtrainData\u001b[39m\n",
       "\u001b[36mlossSeq\u001b[39m: \u001b[32mIndexedSeq\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m0.2716786861419678\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mplot\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mScatter\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mScatter\u001b[39m(\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mres7_6\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-1087245905\"\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val random = new Random\n",
    "\n",
    "val MiniBatchSize = 256\n",
    "\n",
    "//10 label of CIFAR10 images(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
    "val NumberOfClasses: Int = 10\n",
    "\n",
    "def trainData(randomIndexArray: Array[Int]): Double = {\n",
    "  val trainNDArray :: expectLabel :: shapeless.HNil =\n",
    "    ReadCIFAR10ToNDArray.getSGDTrainNDArray(randomIndexArray)\n",
    "  val input =\n",
    "    trainNDArray.reshape(MiniBatchSize, NumberOfPixels)\n",
    "\n",
    "  val expectLabelVectorized =\n",
    "    Utils.makeVectorized(expectLabel, NumberOfClasses)\n",
    "  trainer.train(input :: expectLabelVectorized :: HNil)\n",
    "}\n",
    "\n",
    "val lossSeq =\n",
    "  (\n",
    "    for (iteration <- 0 to 50) yield {\n",
    "      val randomIndex = random\n",
    "        .shuffle[Int, IndexedSeq](0 until 10000) //https://issues.scala-lang.org/browse/SI-6948\n",
    "        .toArray\n",
    "      for (times <- 0 until 10000 / MiniBatchSize) yield {\n",
    "        val randomIndexArray =\n",
    "          randomIndex.slice(times * MiniBatchSize,\n",
    "                            (times + 1) * MiniBatchSize)\n",
    "          val loss = trainData(randomIndexArray)\n",
    "          if(times == 3 & iteration % 5 == 4){\n",
    "            println(\"at epoch \" + (iteration / 5 + 1) + \" loss is :\" + loss)\n",
    "          }\n",
    "          loss\n",
    "      }\n",
    "    }\n",
    "  ).flatten\n",
    "\n",
    "val plot = Seq(\n",
    "  Scatter(lossSeq.indices, lossSeq)\n",
    ")\n",
    "\n",
    "plot.plot(\n",
    "  title = \"loss by time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 读取和处理测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似[前一节](https://thoughtworksinc.github.io/DeepLearning.scala/demo/MiniBatchGradientDescent.html)从CIFAR10 database中读取和处理测试测试集的图片和标签信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtestNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.0\u001b[33m...\u001b[39m\n",
       "\u001b[36mvectorizedTestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testNDArray =\n",
    "  ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/test_batch.bin\", 100)\n",
    "\n",
    "val testData = testNDArray.head\n",
    "\n",
    "val testExpectResult = testNDArray.tail.head\n",
    "\n",
    "val vectorizedTestExpectResult = Utils.makeVectorized(testExpectResult, NumberOfClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 验证神经网络预测准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟上一节相同，我们使用测试数据来验证神经网络的预测结果并计算准确率。这次准确率应该会上升到51%左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result is 51.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mright\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m51.0\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val right = Utils.getAccuracy(predictor.predict(testData), testExpectResult)\n",
    "println(s\"the result is $right %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "在这节中我们学到了：\n",
    "\n",
    "* 参数调优\n",
    "* L2Regularization\n",
    "* Relu\n",
    "* 构建一个两层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[完整代码](https://github.com/izhangzhihao/deeplearning-tutorial/blob/master/src/main/scala/com/thoughtworks/deeplearning/tutorial/TwoLayerNet.scala)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
