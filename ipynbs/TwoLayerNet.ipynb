{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two layer net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景\n",
    "\n",
    "构建一个两层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "\n",
    "1.创建一个SBT项目，并引入相关依赖（参照[Getting Started](https://github.com/ThoughtWorksInc/DeepLearning.scala/wiki/Getting-Started) 或者将下面的依赖引入build.sbt, 注意DeepLearning.scala暂不支持scala2.12.X )\n",
    "```\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiableany\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiablenothing\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiableseq\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiabledouble\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiablefloat\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiablehlist\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiablecoproduct\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiableindarray\" % \"latest.release\"\n",
    "\n",
    "addCompilerPlugin(\"com.thoughtworks.implicit-dependent-type\" %% \"implicit-dependent-type\" % \"latest.release\")\n",
    "\n",
    "addCompilerPlugin(\"org.scalamacros\" % \"paradise\" % \"2.1.0\" cross CrossVersion.full)\n",
    "\n",
    "fork := true\n",
    "```\n",
    "2.[下载CIFAR-10 binary version (suitable for C programs)](https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz)，文件大小162 MB，md5sum：c32a1d4ab5d03f1284b67883e8d87530\n",
    "\n",
    "3.将下载好的文件解压到src/main/resources目录。\n",
    "\n",
    "4.Scala类ReadCIFAR10ToNDArray用于从上面的文件中读取图片及其标签数据并做归一化处理（[更多信息](https://www.cs.toronto.edu/~kriz/cifar.html)）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                 \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.{FileInputStream, InputStream}\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableHList._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableDouble._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableAny._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.{DifferentiableHList, DifferentiableINDArray, Layer}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Layer.Batch\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Lift.Layers.Identity\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Lift._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathFunctions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathMethods./\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathOps\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.cpu.nativecpu.NDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.ops.transforms.Transforms\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4s.Implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mshapeless._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.immutable.IndexedSeq\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mobject\u001b[39m \u001b[36mReadCIFAR10ToNDArray\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $plugin.$ivy.`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:1.0.0`\n",
    "\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableany:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablenothing:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableseq:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiabledouble:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablefloat:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablehlist:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableindarray:1.0.0-RC5`\n",
    "\n",
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.0`\n",
    "\n",
    "import java.io.{FileInputStream, InputStream}\n",
    "\n",
    "\n",
    "import com.thoughtworks.deeplearning\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import com.thoughtworks.deeplearning.DifferentiableHList._\n",
    "import com.thoughtworks.deeplearning.DifferentiableDouble._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray._\n",
    "import com.thoughtworks.deeplearning.DifferentiableAny._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
    "import com.thoughtworks.deeplearning.{DifferentiableHList, DifferentiableINDArray, Layer}\n",
    "import com.thoughtworks.deeplearning.Layer.Batch\n",
    "import com.thoughtworks.deeplearning.Lift.Layers.Identity\n",
    "import com.thoughtworks.deeplearning.Lift._\n",
    "import com.thoughtworks.deeplearning.Poly.MathFunctions._\n",
    "import com.thoughtworks.deeplearning.Poly.MathMethods./\n",
    "import com.thoughtworks.deeplearning.Poly.MathOps\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.cpu.nativecpu.NDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
    "import org.nd4j.linalg.ops.transforms.Transforms\n",
    "import org.nd4s.Implicits._\n",
    "import shapeless._\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._\n",
    "\n",
    "import scala.collection.immutable.IndexedSeq\n",
    "\n",
    "pprintConfig() = pprintConfig().copy(height = 5)//减少输出的行数，避免页面输出太长\n",
    "\n",
    "import $file.ReadCIFAR10ToNDArray,ReadCIFAR10ToNDArray._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "5.[Mini-Batch Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent): 在大规模数据训练时，数据可以达到百万级量级。如果计算整个训练集，来获得仅仅一个参数的更新速度就太慢了。一个常用的方法是计算训练集中的小批量（batches）数据以提升参数更新速度。\n",
    "\n",
    "   \n",
    "6.如果你使用IntelliJ或者eclipse等其它IDE，智能提示可能会失效，代码有部分可能会爆红，这是IDE的问题，代码本身并无问题。\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 构建神经网络\n",
    "\n",
    "1.新建一个Scala类TwoLayerNet\n",
    "\n",
    "2.从CIFAR10 database中读取测试数据的图片和标签信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mNumberOfClasses\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36mtestNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.55, 0.55, 0.56, 0.54, 0.49, 0.45, 0.59, 0.59, 0.62, 0.65, 0.63, 0.62, 0.64, 0.63, 0.64, 0.61, 0.61, 0.62, 0.64, 0.66, 0.67, 0.67, 0.66, 0.62, 0.60, 0.59, 0.57, 0.54, 0.55, 0.55, 0.58, 0.57, 0.57, 0.55, 0.56, 0.53, 0.49, 0.46, 0.59, 0.59, 0.\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CIFAR10中的图片共有10个分类(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
    "val CLASSES: Int = 10\n",
    "\n",
    "//加载测试数据，我们读取100条作为测试数据\n",
    "val testNDArray =\n",
    "  ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/test_batch.bin\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.编写处理标签数据的工具方法，将N行一列的NDArray转换为N行NumberOfClasses列的NDArray，每行对应的正确分类的值为1，其它列的值为0。这样做是为了向cross-entropy loss公式靠拢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmakeVectorized\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**\n",
    "  * 处理标签数据：将N行一列的NDArray转换为N行CLASSES列的NDArray，每行对应的正确分类的值为1，其它列的值为0\n",
    "  *\n",
    "  * @param ndArray 标签数据\n",
    "  * @return N行CLASSES列的NDArray\n",
    "  */\n",
    "def makeVectorized(ndArray: INDArray): INDArray = {\n",
    "  val shape = ndArray.shape()\n",
    "\n",
    "  val p = Nd4j.zeros(shape(0), CLASSES)\n",
    "  for (i <- 0 until shape(0)) {\n",
    "    val double = ndArray.getDouble(i, 0)\n",
    "    val column = double.toInt\n",
    "    p.put(i, column, 1)\n",
    "  }\n",
    "  p\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.分离和处理图像和标签数据，注意：这里和与SoftmaxLinearClassifier中不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtest_data\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.55, 0.55, 0.56, 0.54, 0.49, 0.45, 0.59, 0.59, 0.62, 0.65, 0.63, 0.62, 0.64, 0.63, 0.64, 0.61, 0.61, 0.62, 0.64, 0.66, 0.67, 0.67, 0.66, 0.62, 0.60, 0.59, 0.57, 0.54, 0.55, 0.55, 0.58, 0.57, 0.57, 0.55, 0.56, 0.53, 0.49, 0.46, 0.59, 0.59, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtest_expect_result\u001b[39m: \u001b[32mINDArray\u001b[39m = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.00, 0.00, 9.00, 6.00, 6.00, 5.00, 4.00, 5.00, 9.00, 2.00, 4.00, 1.00, 9.00, 5.00, 4.00, 6.00, 5.00, 6.00, 0.00, 9.00, 3.00, 9.00, 7.00, 6.00, 9.00, 8.00, 0.00, 3.00, 8.00, 8.00, 7.00, 7.00, 4.00, 6.00, 7.00, 3.00, 6.00, 3.00, 6.00, 2.00, 1.0\u001b[33m...\u001b[39m\n",
       "\u001b[36mtest_p\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       " [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test_data = testNDArray.head\n",
    "\n",
    "val test_expect_result = testNDArray.tail.head\n",
    "  \n",
    "val test_p = makeVectorized(test_expect_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.设置学习率，学习率是Weight变化的快慢的直观描述，学习率设置的过小会导致loss下降的很慢，需要更长时间来训练，学习率设置的过大虽然刚开始下降很快但是会导致在接近最低点的时候在附近徘徊loss下降会非常慢。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36moptimizerFactory\u001b[39m: \u001b[32mAnyRef\u001b[39m with \u001b[32mOptimizerFactory\u001b[39m{def ndArrayOptimizer(weight: com.thoughtworks.deeplearning.DifferentiableINDArray.Layers.Weight): com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers.L2Regularization} = $sess.cmd4Wrapper$Helper$$anon$2@3141d093"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit val optimizerFactory = new DifferentiableINDArray.OptimizerFactory {\n",
    "  override def ndArrayOptimizer(\n",
    "      weight: DifferentiableINDArray.Layers.Weight): L2Regularization = {\n",
    "    new DifferentiableINDArray.Optimizers.L2Regularization {\n",
    "      override protected def l2Regularization = 0.03\n",
    "\n",
    "      var learningRate = 0.001\n",
    "\n",
    "      override protected def currentLearningRate(): Double = {\n",
    "        learningRate\n",
    "      }\n",
    "\n",
    "      override def updateNDArray(oldValue: INDArray,\n",
    "                                 delta: INDArray): INDArray = {\n",
    "        learningRate *= 0.9995\n",
    "        super.updateNDArray(oldValue, delta)\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.编写第一层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfullyConnectedThenRelu\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fullyConnectedThenRelu(inputSize: Int, outputSize: Int)(\n",
    "    implicit row: From[INDArray]##T): To[INDArray]##T = {\n",
    "  val w = (Nd4j.randn(inputSize, outputSize) / math.sqrt(outputSize / 2.0)).toWeight * 0.1\n",
    "  val b = Nd4j.zeros(outputSize).toWeight\n",
    "  max((row dot w) + b, 0.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.编写softmax函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msoftmax\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(implicit scores: From[INDArray] ##T): To[INDArray] ##T = {\n",
    "  val expScores = exp(scores)\n",
    "  expScores / expScores.sum(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.编写第二层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfullyConnectedThenSoftmax\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fullyConnectedThenSoftmax(inputSize: Int, outputSize: Int)(\n",
    "    implicit row: From[INDArray]##T): To[INDArray]##T = {\n",
    "  val w = (Nd4j.randn(inputSize, outputSize) / math.sqrt(outputSize)).toWeight //* 0.1\n",
    "  val b = Nd4j.zeros(outputSize).toWeight\n",
    "  softmax.compose((row dot w) + b)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.将上面的两层神经网络组合起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mhiddenLayer\u001b[39m\n",
       "\u001b[36mpredictor\u001b[39m: (\u001b[32mTo\u001b[39m[\u001b[32mINDArray\u001b[39m]{type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray;type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray})#\u001b[32mT\u001b[39m = Compose(Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum(Exp(Identity()),WrappedArray(1)))),PlusINDArray(Dot(Identity(),Weight([[-0.51, -0.22, 0.86, 0.55, -0.05, -0.48, 0.13, 0.13, 0.48, 0.10],\n",
       " [-0.25, 0.00, -0.48, 0.32, 0.05, -0.23, -0.27, 0.43, -0.00, 0.22],\n",
       " [-0.44, -0.19, 0.19, 0.26, 0.73, -0.18, -0.37, -0.11, -0.12, 0.37],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hiddenLayer(implicit input: From[INDArray]##T): To[INDArray]##T = {\n",
    "  val layer0 = fullyConnectedThenRelu(3072, 500).compose(input)\n",
    "  fullyConnectedThenSoftmax(500, 10).compose(layer0)\n",
    "}\n",
    "\n",
    "val predictor = hiddenLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.编写损失函数Loss Function，将此次判断的结果和真实结果进行计算得出cross-entropy loss并返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcrossEntropy\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def crossEntropy(\n",
    "    implicit pair: From[INDArray :: INDArray :: HNil]##T): To[Double]##T = {\n",
    "  val score = pair.head\n",
    "  val label = pair.tail.head\n",
    "  -(label * log(score * 0.9 + 0.1) + (1.0 - label) * log(1.0 - score * 0.9)).sum\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.组合输入层和隐含层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mnetwork\u001b[39m\n",
       "\u001b[36mtrainer\u001b[39m: (\u001b[32mTo\u001b[39m[\u001b[32mDouble\u001b[39m]{type OutputData = Double;type OutputDelta = Double;type InputData = shapeless.::[org.nd4j.linalg.api.ndarray.INDArray,shapeless.::[org.nd4j.linalg.api.ndarray.INDArray,shapeless.HNil]];type InputDelta = shapeless.:+:[org.nd4j.linalg.api.ndarray.INDArray,shapeless.:+:[org.nd4j.linalg.api.ndarray.INDArray,shapeless.CNil]]})#\u001b[32mT\u001b[39m = Compose(Negative(ReduceSum(PlusINDArray(MultiplyINDArray(Head(Tail(Identity())),Log(PlusDouble(MultiplyDouble(Head(Identity()),Literal(0.9)),Literal(0.1)))),MultiplyINDArray(PlusDouble(Negative(Head(Tail(Identity()))),Literal(1.0)),Log(PlusDouble(Negative(MultiplyDouble(Head(Identity()),Literal(0.9))),Literal(1.0))))))),HCons(Compose(Compose(Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum(\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def network(\n",
    "   implicit pair: From[INDArray :: INDArray :: HNil]##T): To[Double]##T = {\n",
    "  val input = pair.head\n",
    "  val label = pair.tail.head\n",
    "  val score: To[INDArray]##T = predictor.compose(input)\n",
    "  val hnilLayer: To[HNil]##T = HNil\n",
    "  crossEntropy.compose(score :: label :: hnilLayer)\n",
    "}\n",
    "\n",
    "val trainer = network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.训练神经网络并观察每次训练loss的变化，loss的变化趋势是降低，但是不是每次都降低(前途是光明的，道路是曲折的)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-1041040961\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0,1989.0,1990.0,1991.0,1992.0,1993.0,1994.0,1995.0,1996.0,1997.0,1998.0,1999.0],\"y\":[684.9188232421875,686.76513671875,670.3001708984375,657.9227294921875,654.7225952148438,657.9554443359375,662.3475341796875,651.6888427734375,622.773193359375,639.247802734375,624.741455078125,627.119873046875,634.3587646484375,607.95556640625,610.867919921875,627.096435546875,632.6058349609375,638.1961059570312,622.1295166015625,634.6151123046875,643.6934204101562,661.4866943359375,736.3333740234375,748.501220703125,775.5603637695312,1089.0494384765625,1027.819580078125,949.523681640625,985.453125,954.4664306640625,759.9876708984375,711.1599731445312,734.125244140625,687.8888549804688,697.4808349609375,675.303955078125,671.6768798828125,657.8465576171875,625.3602294921875,636.4618530273438,625.5488891601562,622.6849975585938,617.0186767578125,624.0480346679688,620.2276611328125,608.0530395507812,598.77978515625,612.38232421875,589.0994262695312,606.357421875,587.7338256835938,618.4432373046875,599.2841796875,590.6209106445312,589.9593505859375,583.26708984375,589.562744140625,586.92822265625,588.6104125976562,598.029296875,564.2275390625,578.7061767578125,594.49755859375,598.005615234375,570.782958984375,573.6897583007812,569.6104736328125,580.5941772460938,574.470458984375,578.36181640625,598.5103759765625,589.3699951171875,592.1181640625,565.2799072265625,582.6554565429688,557.2821655273438,565.67822265625,593.2940063476562,576.3756713867188,587.9459228515625,563.2825927734375,578.0101318359375,595.5107421875,590.1348876953125,557.0499877929688,543.8658447265625,574.5484008789062,586.9879150390625,587.6912841796875,593.957275390625,575.507080078125,578.3074340820312,559.5474853515625,554.183837890625,557.4849853515625,552.28857421875,568.0980834960938,557.478759765625,552.2639770507812,575.862548828125,570.0604858398438,581.5084228515625,567.158203125,545.123046875,542.6301879882812,556.4122314453125,544.8072509765625,564.9581298828125,548.3883056640625,568.8082885742188,538.6053466796875,550.18896484375,570.361328125,568.327880859375,548.1435546875,568.3309326171875,570.5631103515625,554.01171875,564.5730590820312,550.482177734375,555.6045532226562,534.9568481445312,560.2615966796875,555.26416015625,552.8013916015625,547.5005493164062,554.1009521484375,576.9118041992188,543.7740478515625,539.296142578125,549.9562377929688,566.4639892578125,544.2852783203125,550.571044921875,563.5713500976562,553.8826293945312,555.966552734375,558.7008056640625,540.57861328125,545.5531005859375,528.7066650390625,544.0423583984375,561.740234375,547.9053955078125,549.3929443359375,524.3292236328125,547.6671142578125,544.8804321289062,555.7420043945312,540.2576904296875,565.2994384765625,539.6168823242188,565.1021728515625,584.0130615234375,544.6488037109375,545.0592041015625,550.729736328125,550.5745849609375,536.168701171875,526.2619018554688,552.607666015625,577.7783813476562,552.950439453125,541.9925537109375,549.0931396484375,565.8756103515625,538.46533203125,560.45751953125,553.3305053710938,551.2799072265625,536.3579711914062,528.1834716796875,529.5520629882812,548.3197631835938,521.2784423828125,544.3198852539062,540.048095703125,545.37109375,539.51806640625,543.9705810546875,548.57666015625,555.04150390625,537.657470703125,527.68017578125,543.1451416015625,547.6295776367188,523.0849609375,492.8158874511719,538.9705200195312,550.876220703125,551.7197265625,525.428955078125,524.655029296875,539.4668579101562,535.5592041015625,542.1526489257812,543.3487548828125,539.683837890625,537.0120849609375,528.65283203125,531.447265625,530.43896484375,522.506591796875,529.8035888671875,540.19384765625,551.239501953125,535.32080078125,517.759033203125,521.1392211914062,549.7349853515625,535.25634765625,537.556640625,561.64404296875,546.3807983398438,505.00433349609375,515.579345703125,538.5869750976562,535.3280029296875,511.5837707519531,525.3359985351562,522.48779296875,541.888671875,539.9630737304688,519.4343872070312,517.7576293945312,519.2822265625,536.3167114257812,518.17431640625,557.3423461914062,556.93701171875,520.182373046875,542.9147338867188,544.4463500976562,534.3270263671875,538.4998779296875,512.59521484375,548.809814453125,507.3992919921875,522.0343627929688,533.2012939453125,552.7109985351562,528.064697265625,523.8272705078125,524.7476196289062,526.8199462890625,541.017578125,541.3489990234375,537.4949951171875,508.8731689453125,522.9425048828125,530.8267822265625,549.32470703125,541.6364135742188,530.6735229492188,543.7572021484375,521.83154296875,560.129150390625,540.7339477539062,528.944580078125,530.777099609375,515.6962280273438,535.4644775390625,517.5933837890625,503.15338134765625,510.0975341796875,519.8955078125,520.8758544921875,514.6597290039062,530.0634765625,529.1414794921875,524.8528442382812,528.2431640625,513.4451904296875,526.3390502929688,504.1331787109375,529.8580322265625,525.6527709960938,518.8465576171875,522.9423217773438,516.8156127929688,528.5947265625,511.78094482421875,523.7781982421875,529.4110107421875,532.5916748046875,499.819580078125,516.1887817382812,556.1241455078125,549.6067504882812,523.1175537109375,521.43212890625,519.3179321289062,531.6676635742188,525.258056640625,516.6602172851562,539.13720703125,519.6868896484375,532.8663330078125,510.649658203125,533.66259765625,553.996337890625,557.9915771484375,532.2048950195312,521.9708251953125,504.2145690917969,533.54345703125,509.1192932128906,525.26123046875,526.26806640625,518.3594360351562,502.73992919921875,524.319580078125,523.9654541015625,520.8538818359375,554.4998779296875,522.662109375,504.152587890625,548.2886352539062,523.64697265625,518.3245849609375,532.5887451171875,501.6996765136719,522.9239501953125,515.3028564453125,488.0526123046875,556.4518432617188,534.1610717773438,515.016357421875,528.8192138671875,517.3876342773438,535.6176147460938,553.665283203125,501.89068603515625,493.292236328125,519.839599609375,547.5001831054688,528.295654296875,512.5504150390625,526.8917236328125,530.953857421875,509.45037841796875,535.4560546875,543.1829833984375,524.9402465820312,533.5697021484375,503.46014404296875,510.9393310546875,495.9554748535156,507.74005126953125,521.2281494140625,516.7067260742188,495.09710693359375,511.6209411621094,507.507080078125,517.995361328125,523.2999267578125,522.002197265625,518.5601806640625,506.6684265136719,521.1156005859375,511.41546630859375,491.77734375,540.829345703125,501.1754150390625,535.5994873046875,510.6025085449219,503.59857177734375,507.5096740722656,527.9132080078125,510.40863037109375,515.5513305664062,546.52978515625,533.443115234375,543.385986328125,543.9337158203125,525.97412109375,504.14862060546875,538.12060546875,500.38507080078125,501.6023254394531,488.424560546875,524.5517578125,511.49346923828125,516.8253784179688,491.4323425292969,527.3419189453125,512.250732421875,521.314697265625,504.38177490234375,527.6541748046875,501.4368591308594,515.777587890625,500.54620361328125,517.0606689453125,493.322509765625,535.9986572265625,523.4469604492188,503.3697509765625,506.26251220703125,517.6372680664062,542.0616455078125,526.3643798828125,503.7703552246094,513.3911743164062,508.1144714355469,514.555908203125,503.4484558105469,500.19329833984375,512.818603515625,524.7389526367188,525.0321044921875,507.2422180175781,521.211181640625,509.07098388671875,506.24212646484375,513.4014892578125,532.0598754882812,504.4653625488281,513.7545776367188,500.8106384277344,507.7441101074219,517.2628173828125,520.9705200195312,495.490234375,475.89447021484375,508.5135498046875,524.5123291015625,521.0398559570312,501.36773681640625,503.7447814941406,536.711181640625,497.39019775390625,494.5069885253906,492.790283203125,520.88818359375,507.27545166015625,518.93896484375,514.9462890625,508.1118469238281,501.4945373535156,514.661376953125,472.395751953125,501.591552734375,502.1630554199219,516.482177734375,498.2817687988281,536.2166748046875,507.96319580078125,516.830322265625,558.5897827148438,511.130615234375,507.54364013671875,516.55322265625,512.47412109375,537.3187255859375,500.1305236816406,501.7452392578125,531.0751953125,515.2335205078125,494.2495422363281,525.7550048828125,517.9930419921875,511.4232177734375,519.6715087890625,483.54150390625,518.544189453125,493.75604248046875,515.9803466796875,496.2249450683594,516.802978515625,525.444580078125,511.118896484375,494.84918212890625,491.4690856933594,514.0790405273438,491.1051330566406,498.3531188964844,552.7730712890625,522.6922607421875,487.481201171875,501.7008972167969,520.403076171875,510.25079345703125,496.80596923828125,516.7559204101562,527.3750610351562,521.34375,500.91534423828125,503.9461669921875,503.9335021972656,468.9169921875,518.2427978515625,498.8338623046875,529.6317138671875,505.9405517578125,491.4415283203125,504.5137939453125,508.36700439453125,510.2083435058594,483.5269775390625,520.5767822265625,496.5776672363281,518.055908203125,489.7567443847656,502.6258850097656,532.6976318359375,523.4351806640625,488.343505859375,513.4464721679688,516.3994140625,521.2967529296875,511.29595947265625,522.9647827148438,507.3096618652344,513.2811889648438,501.23779296875,500.2607421875,491.7168273925781,484.95306396484375,483.97369384765625,501.23126220703125,496.9482421875,465.278564453125,501.7156982421875,487.9241943359375,477.9143981933594,509.8951416015625,505.15576171875,487.86187744140625,500.66534423828125,503.8975830078125,515.0812377929688,491.72210693359375,499.95263671875,529.2874755859375,517.840576171875,515.5761108398438,507.45733642578125,527.453369140625,501.8233642578125,510.72637939453125,507.03643798828125,504.996337890625,501.45379638671875,510.3738708496094,492.3329162597656,523.7850341796875,491.9206237792969,478.1231689453125,482.0348205566406,486.8847961425781,499.6524353027344,495.5550537109375,520.4747314453125,508.45977783203125,490.5191345214844,499.54949951171875,520.84033203125,505.2522277832031,486.551513671875,498.7022705078125,501.1195983886719,516.6912841796875,489.52325439453125,509.8580017089844,528.5682373046875,507.7100524902344,494.4197692871094,531.2806396484375,515.5018310546875,524.14404296875,490.45196533203125,493.27813720703125,496.5447082519531,496.456298828125,499.58660888671875,532.285888671875,490.37255859375,516.729248046875,494.65484619140625,503.1641845703125,490.64312744140625,491.04864501953125,483.718017578125,492.8077392578125,518.5416259765625,509.5359802246094,513.579833984375,482.86102294921875,485.923583984375,497.02093505859375,482.40869140625,501.7146301269531,509.66131591796875,488.3824768066406,487.7711181640625,511.1954040527344,505.42523193359375,496.3341979980469,493.32476806640625,524.5634765625,491.240478515625,505.0077209472656,502.99871826171875,484.74371337890625,503.962646484375,509.9705810546875,503.93682861328125,507.43951416015625,499.9431457519531,523.9471435546875,491.6296081542969,498.30206298828125,510.9339599609375,513.0824584960938,501.966552734375,507.0011901855469,501.6695861816406,516.5362548828125,512.5086059570312,516.2022705078125,483.8002624511719,495.6095886230469,496.0137023925781,485.398681640625,491.7809143066406,477.076171875,507.8212585449219,504.21636962890625,493.40740966796875,513.4505615234375,514.2232055664062,493.6827392578125,483.54638671875,508.1065368652344,508.5102233886719,522.556884765625,487.86297607421875,494.49493408203125,487.485107421875,486.3157958984375,491.913818359375,524.2669677734375,521.4559326171875,501.0210266113281,531.6704711914062,495.9132385253906,504.76251220703125,541.53125,487.8570861816406,484.2513732910156,485.98614501953125,487.0286865234375,486.20196533203125,500.68817138671875,494.7685546875,520.498291015625,470.44659423828125,502.6842041015625,481.336181640625,510.27593994140625,508.923828125,489.9435119628906,500.8065490722656,483.1772766113281,494.8509216308594,472.55328369140625,501.4216003417969,505.77313232421875,490.8328552246094,499.46746826171875,491.3359375,498.05474853515625,523.9321899414062,485.2558288574219,484.5131530761719,508.67864990234375,506.42718505859375,500.090087890625,484.44586181640625,481.44805908203125,478.77587890625,478.91754150390625,481.6352233886719,509.3169860839844,504.21673583984375,525.4322509765625,481.81475830078125,486.14044189453125,499.77056884765625,493.399658203125,493.31951904296875,476.1441650390625,468.77264404296875,499.728515625,488.58929443359375,487.8864440917969,495.3829345703125,525.1661987304688,487.9833679199219,473.69793701171875,459.6131286621094,498.44659423828125,516.2703247070312,499.85870361328125,477.893310546875,499.8846740722656,502.21026611328125,513.4447021484375,485.3223876953125,512.3477172851562,482.9943542480469,487.7684326171875,504.80731201171875,488.2933044433594,526.4573974609375,510.93548583984375,479.7757568359375,495.2698669433594,485.06695556640625,486.76019287109375,481.5046081542969,489.4117431640625,505.40753173828125,500.3662414550781,500.6958312988281,487.602783203125,513.1209106445312,473.4620666503906,452.9156494140625,485.30718994140625,489.5640869140625,501.2333679199219,502.7190246582031,467.4332275390625,529.045654296875,500.4964599609375,496.0065002441406,500.0875244140625,500.9425354003906,504.85986328125,480.5555419921875,512.217529296875,483.9183654785156,490.3062438964844,480.9064025878906,495.02471923828125,503.22515869140625,480.1752014160156,500.8221740722656,492.0577087402344,502.85296630859375,484.361328125,517.8886108398438,493.76788330078125,510.93194580078125,514.1610107421875,481.88946533203125,507.0796203613281,500.1338806152344,469.7453918457031,508.8153381347656,470.699462890625,496.6689758300781,488.1427001953125,516.0430908203125,498.79522705078125,480.51617431640625,482.68487548828125,497.4996337890625,476.34954833984375,513.99267578125,505.4278259277344,495.7938232421875,500.03692626953125,479.602294921875,488.8609619140625,530.837890625,503.0624694824219,500.7666015625,482.96429443359375,478.7124328613281,501.564208984375,494.52935791015625,499.78656005859375,498.50567626953125,477.07318115234375,496.27685546875,482.14306640625,470.821044921875,475.7510070800781,499.8965148925781,463.1436462402344,518.049072265625,504.47991943359375,493.6446838378906,474.38433837890625,484.9294128417969,471.5157470703125,530.63232421875,477.14532470703125,499.73516845703125,485.6844482421875,484.0020751953125,487.5715026855469,505.5700378417969,483.7970886230469,484.7568054199219,494.6898498535156,482.007568359375,461.7525939941406,470.0953674316406,511.51373291015625,489.4892272949219,476.98712158203125,474.72906494140625,490.59521484375,509.3697509765625,487.9527893066406,497.11297607421875,502.9985656738281,468.1826171875,506.12847900390625,478.37152099609375,466.50079345703125,507.32989501953125,491.80145263671875,492.4780578613281,468.49468994140625,504.1331787109375,491.8599548339844,488.3359680175781,466.18267822265625,495.6480712890625,474.6586608886719,465.9644470214844,478.8665771484375,512.4075317382812,504.4980773925781,497.6484680175781,502.08514404296875,477.2998046875,491.6836853027344,504.219482421875,483.0320739746094,484.91455078125,480.0469970703125,467.5948486328125,480.2454833984375,487.21417236328125,445.45843505859375,465.9281311035156,506.97528076171875,493.02880859375,452.727783203125,465.57928466796875,475.8583984375,494.5622253417969,498.30792236328125,484.90802001953125,461.48876953125,473.32763671875,485.12921142578125,486.9452209472656,488.9109191894531,485.450439453125,505.7976379394531,480.0749206542969,480.4935302734375,491.8056945800781,499.85015869140625,482.34539794921875,461.731689453125,504.89984130859375,477.4319763183594,486.7955322265625,489.5840148925781,483.6463623046875,503.68902587890625,494.2270202636719,484.0477600097656,515.40869140625,494.36895751953125,483.7017822265625,465.2416076660156,491.5181884765625,483.6011962890625,476.5631103515625,511.406982421875,483.7156982421875,479.29217529296875,521.248779296875,483.5014343261719,467.18438720703125,479.95721435546875,486.84716796875,469.54925537109375,529.5382080078125,476.2569580078125,487.8985595703125,476.7899475097656,501.926025390625,516.494140625,484.92669677734375,483.2557373046875,486.02606201171875,497.19061279296875,498.1815185546875,461.18072509765625,490.4507141113281,473.4125061035156,477.7073059082031,480.0561828613281,463.1782531738281,484.78118896484375,508.1025390625,490.2673645019531,497.25628662109375,520.0419921875,477.34124755859375,471.96112060546875,496.8385925292969,487.9192810058594,523.6629638671875,478.24688720703125,470.3657531738281,468.0091857910156,483.3303527832031,516.0115966796875,480.2315979003906,518.02001953125,481.8978271484375,470.08978271484375,475.882080078125,498.68365478515625,484.840087890625,493.1322326660156,493.68829345703125,494.1885681152344,455.6305847167969,494.65673828125,460.1569519042969,463.8338623046875,448.0582580566406,496.3218994140625,477.1855163574219,485.078369140625,485.29132080078125,495.77325439453125,485.77783203125,487.190673828125,481.83197021484375,483.8854064941406,479.99761962890625,470.4070739746094,493.79132080078125,496.819091796875,490.8364562988281,459.751953125,490.42864990234375,512.0303955078125,482.09564208984375,488.51519775390625,485.6200256347656,483.3148193359375,464.49151611328125,470.68768310546875,481.63287353515625,504.8254089355469,496.749267578125,473.2499694824219,452.98529052734375,482.93048095703125,486.8638916015625,441.6421813964844,519.9814453125,495.957275390625,474.7935485839844,482.90179443359375,527.050537109375,473.375,495.24969482421875,462.753173828125,501.468505859375,480.4039306640625,488.44732666015625,501.6078186035156,464.2362365722656,458.3377685546875,489.31103515625,491.39068603515625,485.9889831542969,485.8733825683594,484.7881164550781,486.93646240234375,498.676513671875,497.0512390136719,493.1272888183594,495.39404296875,516.342529296875,496.9610900878906,492.93115234375,511.0570068359375,488.4090881347656,455.7518005371094,467.46600341796875,480.69879150390625,457.6171875,481.0727233886719,493.586181640625,469.19720458984375,507.0330810546875,475.12554931640625,468.3777770996094,471.80389404296875,477.92987060546875,458.38116455078125,476.0326232910156,485.02496337890625,507.3668518066406,459.6818542480469,482.49298095703125,488.7205810546875,471.3808288574219,492.90185546875,487.1964111328125,501.08966064453125,472.7886962890625,506.4297790527344,487.3170166015625,480.757080078125,491.54193115234375,466.01507568359375,504.5700378417969,483.2043762207031,517.9085693359375,474.34130859375,503.14971923828125,495.2762145996094,482.70098876953125,486.84185791015625,480.9018249511719,487.512939453125,493.40057373046875,493.5840759277344,471.3906555175781,493.31988525390625,466.397216796875,494.9862365722656,461.62274169921875,497.94512939453125,502.2654724121094,477.36541748046875,470.7628173828125,448.7669677734375,473.23004150390625,500.24127197265625,483.5944519042969,498.5101318359375,462.7543640136719,485.0987548828125,467.867919921875,488.9573974609375,475.6973876953125,499.6479187011719,464.2203369140625,506.77587890625,472.11297607421875,487.3603515625,454.1932373046875,506.6942138671875,478.8646545410156,484.7929992675781,483.53411865234375,483.08465576171875,500.9640197753906,473.54315185546875,502.722900390625,483.57928466796875,477.8855895996094,473.7244873046875,468.7601318359375,456.08392333984375,489.75146484375,488.2332763671875,478.9010314941406,462.11737060546875,455.3686218261719,500.0164489746094,512.1864624023438,484.50372314453125,499.6251525878906,496.797119140625,487.926513671875,443.36029052734375,477.0048828125,482.0225830078125,449.75787353515625,480.24859619140625,497.03448486328125,464.4910888671875,451.8697509765625,475.88946533203125,502.6958923339844,480.06097412109375,488.9770812988281,474.22576904296875,464.72882080078125,484.6656494140625,455.3608703613281,485.24200439453125,498.2970275878906,484.4135437011719,481.2394714355469,472.738037109375,499.3719482421875,483.4813537597656,490.04290771484375,466.09814453125,491.3135070800781,495.0047302246094,493.0756530761719,483.13787841796875,475.65399169921875,506.9176330566406,483.5567626953125,480.15570068359375,463.1174011230469,484.62158203125,486.403564453125,472.8138427734375,469.45941162109375,485.4505615234375,480.4468994140625,473.6404113769531,497.34039306640625,481.1499328613281,478.50543212890625,484.9405212402344,498.0484619140625,498.2757263183594,448.60382080078125,493.20416259765625,453.8270263671875,463.1556701660156,487.0517578125,476.2388916015625,450.6890869140625,461.3902587890625,470.9132995605469,503.02154541015625,452.15203857421875,481.14764404296875,471.3952331542969,472.405517578125,447.6957092285156,503.579345703125,464.1399230957031,471.94964599609375,486.1084899902344,468.38641357421875,469.4129943847656,481.7575378417969,490.0093078613281,456.22625732421875,466.1949462890625,461.52606201171875,491.4095153808594,475.521240234375,477.3519592285156,481.5322265625,453.5483703613281,487.6841125488281,467.2530212402344,480.5029296875,503.42559814453125,483.8684387207031,499.037841796875,469.073486328125,481.51300048828125,453.0075988769531,455.021240234375,485.8141174316406,489.4675598144531,485.5976257324219,502.92767333984375,463.80670166015625,474.07403564453125,478.2543640136719,467.13531494140625,487.1739807128906,473.36431884765625,482.093505859375,477.2694091796875,460.52142333984375,491.9228820800781,476.9145202636719,496.92974853515625,489.0784912109375,490.72747802734375,483.19158935546875,486.47332763671875,481.5583190917969,504.92388916015625,470.5746765136719,489.71832275390625,462.20391845703125,448.6981506347656,459.704833984375,490.7641906738281,467.4132080078125,438.0792236328125,466.4273986816406,461.0461120605469,454.8781433105469,504.5571594238281,477.9776916503906,482.989990234375,478.36395263671875,478.7079162597656,490.48779296875,449.1010437011719,475.7132568359375,480.75146484375,470.6942138671875,496.5028381347656,503.14068603515625,485.7265625,465.68115234375,450.69781494140625,495.6990051269531,487.5533142089844,498.227783203125,483.74853515625,460.896484375,473.155517578125,497.2480163574219,474.4369812011719,464.78314208984375,470.7431640625,494.1690673828125,438.84515380859375,474.7662658691406,475.03570556640625,487.4732666015625,487.356201171875,478.864013671875,496.451171875,486.710205078125,482.0409240722656,477.9419250488281,496.7345886230469,476.822509765625,478.639404296875,451.1279602050781,439.05694580078125,478.8084411621094,465.95880126953125,461.2508850097656,491.2084045410156,503.9293518066406,470.35162353515625,456.0960693359375,461.5764465332031,478.47723388671875,458.10321044921875,451.2141418457031,478.62799072265625,458.10076904296875,496.1320495605469,471.9403076171875,459.68438720703125,459.5556640625,490.8712158203125,479.9374084472656,489.991943359375,471.1304626464844,485.67779541015625,445.32244873046875,472.22735595703125,476.1876220703125,460.0537109375,488.05682373046875,487.15606689453125,475.88897705078125,464.1322937011719,472.0474853515625,441.580810546875,510.1315612792969,492.70458984375,480.8365478515625,488.7120361328125,501.17041015625,481.6885986328125,473.39617919921875,461.66082763671875,471.1995849609375,500.82159423828125,472.384521484375,478.80267333984375,443.8526306152344,489.5107421875,481.49114990234375,490.3899841308594,453.03533935546875,488.59423828125,473.727294921875,456.1145935058594,472.3089599609375,427.83917236328125,470.9693298339844,479.6338806152344,463.0351257324219,481.21051025390625,458.72564697265625,477.4704895019531,459.1137390136719,489.30047607421875,468.42010498046875,475.2073974609375,467.7554931640625,458.4070129394531,485.04193115234375,473.4638977050781,478.5801086425781,441.35943603515625,500.6226501464844,473.7430725097656,474.50628662109375,468.62493896484375,485.3731689453125,491.8421630859375,495.900390625,494.4476318359375,471.356689453125,468.2030944824219,481.8188781738281,514.3236083984375,504.77545166015625,475.72967529296875,481.814208984375,457.8721923828125,460.1217041015625,456.62335205078125,466.53460693359375,469.1766662597656,461.69378662109375,476.0269470214844,487.3610534667969,496.7072448730469,478.1275939941406,471.01861572265625,498.7804260253906,477.4306335449219,480.329345703125,451.89569091796875,473.07281494140625,465.1178894042969,513.3450927734375,445.46136474609375,503.1366271972656,450.6211242675781,500.14324951171875,450.8203430175781,461.1123046875,477.3391418457031,475.9182434082031,474.789794921875,480.64141845703125,439.37548828125,489.34417724609375,463.3076171875,466.19970703125,471.52716064453125,467.4385986328125,459.4090270996094,456.8121643066406,490.7373046875,492.7485656738281,480.5775451660156,473.7054748535156,473.3047790527344,471.51092529296875,465.37939453125,482.2674560546875,480.298095703125,468.9906921386719,498.10809326171875,473.8741455078125,485.192626953125,465.801025390625,469.90887451171875,434.1478271484375,443.9423828125,459.59161376953125,483.8427734375,490.37530517578125,482.0661315917969,475.8986511230469,479.188720703125,516.828857421875,460.20355224609375,497.97235107421875,468.1632080078125,462.1949462890625,472.6258239746094,458.0687561035156,508.6831359863281,468.6760559082031,463.343505859375,488.7340087890625,483.73626708984375,479.77557373046875,457.29534912109375,447.6644287109375,485.32781982421875,447.4246826171875,452.7189025878906,469.83221435546875,429.56365966796875,467.7249755859375,469.88836669921875,460.68658447265625,480.154052734375,476.6832275390625,457.38037109375,473.02880859375,481.009521484375,452.9266357421875,485.8850402832031,458.43798828125,490.24615478515625,464.6605224609375,470.3353576660156,462.2635498046875,443.2218322753906,477.24188232421875,455.7841796875,457.17864990234375,481.2982482910156,486.6395263671875,467.89227294921875,481.14276123046875,480.7790222167969,485.19976806640625,442.8224182128906,465.0947265625,446.77996826171875,460.633056640625,472.9627685546875,503.1526184082031,459.5055236816406,445.0501403808594,497.9610900878906,441.168212890625,479.44525146484375,466.8239440917969,476.1649169921875,504.0923156738281,492.17193603515625,466.6480407714844,463.5868835449219,481.448486328125,477.2177734375,488.78460693359375,447.15557861328125,454.56549072265625,489.8297119140625,502.164306640625,430.9573059082031,475.9338073730469,477.0545959472656,473.8719177246094,482.57177734375,464.85980224609375,475.02850341796875,460.98822021484375,481.77862548828125,488.72161865234375,465.6241455078125,485.96258544921875,464.6980895996094,479.41668701171875,467.10870361328125,462.23529052734375,460.34521484375,464.6327819824219,474.4718017578125,483.65570068359375,447.8487548828125,457.7153015136719,462.0596008300781,476.2416687011719,452.225830078125,476.66912841796875,467.8355712890625,475.5540771484375,482.580322265625,478.3478698730469,487.52178955078125,488.18841552734375,463.92388916015625,490.25982666015625,432.2122802734375,449.3430480957031,453.70367431640625,483.33306884765625,463.4071350097656,477.1361999511719,436.03448486328125,467.5872802734375,481.2243957519531,446.4600524902344,491.70001220703125,480.3296203613281,455.3250732421875,492.8390197753906,466.79827880859375,459.696044921875,485.62628173828125,483.810302734375,478.8367004394531,449.2423095703125,470.34930419921875,485.42333984375,461.38116455078125,470.5082092285156,467.1551513671875,440.71307373046875,463.3080139160156,478.5603942871094,481.98480224609375,455.344482421875,477.10809326171875,475.625,496.9806823730469,493.9396057128906,482.90380859375,456.1270446777344,472.57855224609375,471.2411193847656,437.1645812988281,490.89508056640625,455.0946044921875,479.4875183105469,473.23419189453125,469.9107360839844,481.4443359375,475.5736389160156,483.607177734375,498.7636413574219,502.1697998046875,468.20452880859375,495.70391845703125,477.16949462890625,464.3919372558594,474.85809326171875,446.3492431640625,477.7908935546875,460.1936340332031,444.01953125,498.7701721191406,473.6759033203125,480.9458312988281,495.3384704589844,506.46575927734375,476.560302734375,471.983642578125,447.850341796875,489.0452880859375,472.7539978027344,467.2315673828125,482.93670654296875,479.2229309082031,467.3607482910156,458.91473388671875,460.8892822265625,485.4012756347656,499.13043212890625,478.5484619140625,466.94891357421875,486.248046875,457.4642028808594,443.4555358886719,440.36566162109375,485.54852294921875,466.04803466796875,454.5379638671875,451.6468811035156,446.84356689453125,443.0003967285156,490.18603515625,470.0590515136719,488.667236328125,464.3987731933594,478.175048828125,495.31085205078125,469.4864807128906,482.88140869140625,457.7272033691406,450.1263427734375,463.3669738769531,467.8139343261719,468.4537353515625,461.2047119140625,427.84857177734375,495.2395935058594,491.2486572265625,474.72412109375,469.7309265136719,463.1910095214844,482.69500732421875,483.53326416015625,495.1129455566406,494.9752502441406,490.39935302734375,479.8858337402344,473.2205810546875,436.0122375488281,488.6383972167969,473.28204345703125,457.7807922363281,502.79522705078125,469.10699462890625,477.1931457519531,439.574951171875,481.84613037109375,468.1176452636719,487.5550537109375,474.7038879394531,468.8372802734375,465.7552185058594,455.61810302734375,462.7547607421875,464.612060546875,479.697265625,476.38153076171875,477.5955505371094,461.41461181640625,444.1571350097656,479.4198303222656,439.1014099121094,462.17938232421875,458.92132568359375,454.05059814453125,462.37774658203125,476.6491394042969,484.73541259765625,469.2818908691406,459.9207763671875,452.55487060546875,459.5550537109375,481.4932861328125,457.72943115234375,461.8331298828125,438.67669677734375,457.8740539550781,439.50665283203125,445.2231140136719,448.94439697265625,465.3787841796875,466.4126281738281,481.9814453125,471.373291015625,465.1319885253906,475.5171813964844,440.0501708984375,457.59564208984375,432.9578552246094,454.9803466796875,446.67791748046875,487.7430114746094,494.9833679199219,495.4541015625,456.3914794921875,446.93017578125,459.5258483886719,469.73004150390625,450.8177185058594,471.0880126953125,472.6543884277344,496.1783447265625,467.46844482421875,471.870849609375,453.91802978515625,467.48687744140625,447.06085205078125,458.7145080566406,464.4809265136719,503.1873474121094,463.3446044921875,471.49090576171875,462.8274841308594,459.5248718261719,480.9490661621094,452.4062805175781,471.52056884765625,452.85345458984375,458.0162658691406,453.34991455078125,460.6526184082031,447.10528564453125,446.930419921875,454.24658203125,470.3685302734375,495.4651794433594,474.366455078125,464.25341796875,467.0055847167969,472.2178955078125,468.7945861816406,457.54742431640625,459.77325439453125,479.52093505859375,459.6524353027344,489.00006103515625,473.8843688964844,450.435546875,460.821044921875,471.0542907714844,446.51239013671875,466.61895751953125,437.8567810058594,468.52362060546875,470.0881652832031,481.9635009765625,481.56976318359375,474.0604553222656,464.8336486816406,457.29461669921875,465.0517578125,463.82122802734375,471.84393310546875,470.7375183105469,451.3964538574219,459.81121826171875,464.0266418457031,454.45166015625,499.7506103515625,468.84344482421875,489.68865966796875,462.49102783203125,468.0183410644531,441.73260498046875,456.5994873046875,445.3753967285156,469.77392578125,479.20751953125,491.9321594238281,449.4713439941406,472.923583984375,467.2025451660156,469.66937255859375,456.4603271484375,470.7476806640625,470.7405700683594,462.1473388671875,485.3270568847656,473.81134033203125,466.9652099609375,473.49658203125,477.76702880859375,468.20147705078125,461.6758728027344,458.4619140625,479.7242431640625,487.8163757324219,470.8436279296875,466.2884826660156,468.87274169921875,438.00390625,467.173828125,448.0634765625,470.4186706542969,462.75689697265625,478.0677795410156,438.66192626953125,468.99700927734375,473.2766418457031,479.3902587890625,477.41033935546875,469.1943359375,462.55810546875,479.9702453613281,441.49224853515625,464.55743408203125,461.1757507324219,457.50970458984375,487.3284606933594,453.00140380859375,484.0804443359375,465.9886779785156,467.8987121582031,477.80126953125,471.31610107421875,463.2410888671875,460.35064697265625,456.8089904785156,464.4192810058594,493.8688659667969,447.33026123046875,438.5260925292969,466.75946044921875,485.2301330566406,465.008056640625,466.61981201171875,432.65185546875,473.45635986328125,441.4845886230469,458.0382080078125,462.746826171875,478.0267333984375,467.38885498046875,488.3579406738281,454.6968688964844,492.47216796875,466.1669921875,432.693603515625,446.3946533203125,462.611572265625,465.428955078125,462.0971374511719,472.5418395996094,467.3296813964844,507.1502685546875,484.37884521484375,456.8797302246094,479.5362548828125,479.26715087890625,461.9556884765625,433.2008361816406,466.1163330078125,456.1417236328125,459.46490478515625,458.34747314453125,476.68658447265625,468.5950012207031,468.82470703125,466.6041259765625,474.15972900390625,456.67474365234375,431.95550537109375,462.43365478515625,481.04266357421875,488.505615234375,484.44427490234375,444.75494384765625,427.6699523925781,441.7611389160156,481.1645812988281,452.60894775390625,458.21124267578125,472.88909912109375,477.97808837890625,461.06494140625,462.1412658691406,486.7030029296875,461.8442687988281,446.39410400390625,484.57452392578125,455.60009765625,472.6526184082031,468.13238525390625,461.10052490234375,484.35577392578125,472.4969482421875,470.34307861328125,444.2859191894531,466.470947265625,451.440673828125,461.35992431640625,462.025390625,444.798583984375,455.6658935546875,428.15643310546875,470.25323486328125,432.09912109375,466.51153564453125,477.1748962402344,451.0733642578125,485.67437744140625,486.01824951171875,491.09051513671875,457.847412109375,481.1297607421875,469.5341491699219,467.55133056640625,482.55615234375,450.04486083984375,451.5804138183594,428.2200927734375,459.4515686035156,453.99169921875,470.15545654296875,475.1114501953125,453.0446472167969,459.59197998046875,461.60504150390625,449.67584228515625,457.381591796875,461.2069091796875,483.3701171875,460.5291748046875,485.126708984375,458.050048828125,432.0867004394531,465.6253662109375,445.10382080078125,452.6202392578125,462.06689453125,447.71246337890625,454.4206848144531,469.5535888671875,469.33660888671875,460.2086181640625,465.8728942871094,451.17437744140625,454.0637512207031,470.3551940917969,473.1906433105469,471.3525390625,462.77825927734375,451.33807373046875,458.7625732421875,431.9705810546875,441.3587341308594,463.1236572265625,481.0423583984375,480.17333984375,446.4000549316406,467.7027587890625,456.0010681152344,490.89892578125,465.54248046875,453.90802001953125,434.61962890625,464.9093933105469,461.60015869140625,473.889892578125,483.6072998046875,460.8103942871094,451.86737060546875,453.97052001953125,484.3408203125,461.5694885253906,466.4494323730469,461.729248046875,480.8638000488281,485.4201965332031,469.52227783203125,454.62335205078125,493.0145263671875,462.218505859375,482.6976623535156,461.71600341796875,519.934326171875,467.306640625,474.69415283203125,470.3608093261719,503.17388916015625,440.95379638671875,485.1739501953125,481.42352294921875,468.99737548828125,459.1064453125,438.36737060546875,455.5809020996094,456.45367431640625,439.25164794921875,459.3951416015625,429.0815124511719,465.3182373046875,461.791259765625,477.40264892578125,459.6996154785156,458.98248291015625,473.83001708984375,467.47332763671875,474.2056579589844,473.61474609375,476.9927062988281,467.8376159667969,490.197509765625,452.23681640625,462.1232604980469,482.5325927734375,478.55218505859375,462.37396240234375,480.76116943359375,460.7880859375,476.79925537109375,461.4794921875,451.1904602050781,456.284912109375,478.0816650390625,486.3280334472656,488.8321533203125,453.6708984375,482.2344665527344,446.232177734375]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss on time\"};\n",
       "\n",
       "  Plotly.plot('plot-1041040961', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mlossSeq\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[32m684.9188232421875\u001b[39m,\n",
       "  \u001b[32m686.76513671875\u001b[39m,\n",
       "  \u001b[32m670.3001708984375\u001b[39m,\n",
       "  \u001b[32m657.9227294921875\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mplot\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mScatter\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mScatter\u001b[39m(\n",
       "    \u001b[33mSome\u001b[39m(\n",
       "      \u001b[33mDoubles\u001b[39m(\n",
       "        \u001b[33mVector\u001b[39m(\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mres11_4\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-1041040961\"\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lossSeq = for (_ <- 0 until 2000) yield {\n",
    "  val trainNDArray = ReadCIFAR10ToNDArray.getSGDTrainNDArray(256)\n",
    "  network.train(\n",
    "    trainNDArray.head :: makeVectorized(trainNDArray.tail.head) :: HNil)\n",
    "}\n",
    "\n",
    "plotly.JupyterScala.init()\n",
    "val plot = Seq(\n",
    "  Scatter(\n",
    "    0 until 2000 by 1,\n",
    "    lossSeq\n",
    "  )\n",
    ")\n",
    "\n",
    "plot.plot(\n",
    "  title = \"loss on time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.使用训练后的神经网络判断测试数据的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: [[0.01, 0.08, 0.17, 0.35, 0.02, 0.10, 0.19, 0.00, 0.07, 0.00],\n",
      " [0.04, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.56, 0.34],\n",
      " [0.23, 0.04, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.64, 0.07],\n",
      " [0.37, 0.03, 0.05, 0.01, 0.01, 0.01, 0.00, 0.00, 0.51, 0.01],\n",
      " [0.00, 0.00, 0.16, 0.04, 0.59, 0.04, 0.13, 0.03, 0.01, 0.00],\n",
      " [0.00, 0.02, 0.05, 0.17, 0.02, 0.09, 0.63, 0.01, 0.00, 0.01],\n",
      " [0.00, 0.04, 0.01, 0.68, 0.00, 0.22, 0.06, 0.00, 0.00, 0.00],\n",
      " [0.00, 0.00, 0.20, 0.04, 0.38, 0.01, 0.34, 0.03, 0.00, 0.00],\n",
      " [0.01, 0.01, 0.30, 0.18, 0.22, 0.20, 0.04, 0.03, 0.02, 0.00],\n",
      " [0.02, 0.47, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.39, 0.11],\n",
      " [0.13, 0.00, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.85, 0.00],\n",
      " [0.00, 0.16, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.04, 0.79],\n",
      " [0.00, 0.24, 0.07, 0.16, 0.07, 0.16, 0.27, 0.02, 0.01, 0.00],\n",
      " [0.04, 0.29, 0.06, 0.20, 0.01, 0.04, 0.26, 0.02, 0.03, 0.03],\n",
      " [0.04, 0.25, 0.08, 0.02, 0.02, 0.00, 0.01, 0.11, 0.09, 0.37],\n",
      " [0.06, 0.00, 0.18, 0.04, 0.16, 0.03, 0.02, 0.02, 0.48, 0.00],\n",
      " [0.00, 0.01, 0.01, 0.19, 0.00, 0.66, 0.04, 0.03, 0.04, 0.00],\n",
      " [0.04, 0.02, 0.13, 0.24, 0.25, 0.09, 0.10, 0.05, 0.01, 0.07],\n",
      " [0.03, 0.02, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.92, 0.03],\n",
      " [0.00, 0.00, 0.02, 0.05, 0.04, 0.04, 0.80, 0.05, 0.00, 0.01],\n",
      " [0.02, 0.03, 0.11, 0.03, 0.64, 0.03, 0.01, 0.03, 0.02, 0.07],\n",
      " [0.13, 0.00, 0.77, 0.02, 0.02, 0.04, 0.00, 0.01, 0.00, 0.00],\n",
      " [0.54, 0.05, 0.02, 0.01, 0.02, 0.00, 0.00, 0.00, 0.32, 0.02],\n",
      " [0.00, 0.27, 0.00, 0.03, 0.01, 0.03, 0.01, 0.09, 0.00, 0.54],\n",
      " [0.00, 0.00, 0.23, 0.00, 0.68, 0.01, 0.01, 0.06, 0.00, 0.00],\n",
      " [0.01, 0.04, 0.27, 0.02, 0.21, 0.01, 0.35, 0.03, 0.05, 0.02],\n",
      " [0.00, 0.02, 0.08, 0.07, 0.37, 0.06, 0.33, 0.05, 0.00, 0.01],\n",
      " [0.13, 0.02, 0.19, 0.03, 0.20, 0.03, 0.02, 0.21, 0.13, 0.06],\n",
      " [0.01, 0.70, 0.02, 0.03, 0.03, 0.01, 0.02, 0.00, 0.00, 0.17],\n",
      " [0.00, 0.01, 0.14, 0.04, 0.13, 0.03, 0.62, 0.02, 0.00, 0.00],\n",
      " [0.00, 0.07, 0.20, 0.15, 0.12, 0.04, 0.36, 0.05, 0.00, 0.01],\n",
      " [0.00, 0.00, 0.30, 0.09, 0.26, 0.25, 0.06, 0.03, 0.00, 0.00],\n",
      " [0.01, 0.01, 0.17, 0.10, 0.22, 0.17, 0.20, 0.03, 0.08, 0.00],\n",
      " [0.00, 0.04, 0.22, 0.17, 0.02, 0.04, 0.46, 0.05, 0.00, 0.01],\n",
      " [0.04, 0.07, 0.00, 0.01, 0.00, 0.00, 0.00, 0.01, 0.12, 0.75],\n",
      " [0.00, 0.23, 0.16, 0.05, 0.13, 0.08, 0.16, 0.13, 0.03, 0.02],\n",
      " [0.00, 0.00, 0.27, 0.12, 0.22, 0.11, 0.23, 0.04, 0.00, 0.01],\n",
      " [0.02, 0.10, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.12, 0.75],\n",
      " [0.02, 0.27, 0.01, 0.10, 0.01, 0.04, 0.04, 0.02, 0.18, 0.30],\n",
      " [0.03, 0.00, 0.05, 0.10, 0.05, 0.36, 0.03, 0.05, 0.33, 0.00],\n",
      " [0.40, 0.02, 0.06, 0.01, 0.09, 0.01, 0.01, 0.16, 0.21, 0.03],\n",
      " [0.00, 0.00, 0.27, 0.05, 0.18, 0.03, 0.44, 0.03, 0.00, 0.00],\n",
      " [0.00, 0.06, 0.02, 0.47, 0.01, 0.03, 0.02, 0.06, 0.01, 0.31],\n",
      " [0.00, 0.03, 0.09, 0.16, 0.30, 0.09, 0.29, 0.04, 0.00, 0.01],\n",
      " [0.29, 0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.02, 0.57, 0.12],\n",
      " [0.08, 0.02, 0.00, 0.01, 0.00, 0.01, 0.00, 0.00, 0.47, 0.40],\n",
      " [0.00, 0.01, 0.02, 0.48, 0.02, 0.29, 0.15, 0.01, 0.00, 0.01],\n",
      " [0.05, 0.01, 0.00, 0.02, 0.00, 0.00, 0.00, 0.00, 0.84, 0.06],\n",
      " [0.00, 0.00, 0.14, 0.01, 0.56, 0.01, 0.13, 0.14, 0.00, 0.00],\n",
      " [0.00, 0.00, 0.23, 0.01, 0.36, 0.01, 0.37, 0.01, 0.00, 0.00],\n",
      " [0.01, 0.04, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.12, 0.83],\n",
      " [0.09, 0.07, 0.13, 0.01, 0.17, 0.01, 0.03, 0.12, 0.30, 0.07],\n",
      " [0.00, 0.04, 0.05, 0.07, 0.05, 0.04, 0.70, 0.02, 0.02, 0.01],\n",
      " [0.00, 0.07, 0.07, 0.34, 0.05, 0.22, 0.13, 0.06, 0.02, 0.04],\n",
      " [0.02, 0.02, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.93, 0.03],\n",
      " [0.11, 0.03, 0.03, 0.01, 0.02, 0.01, 0.00, 0.00, 0.75, 0.04],\n",
      " [0.01, 0.00, 0.11, 0.13, 0.22, 0.24, 0.05, 0.20, 0.03, 0.00],\n",
      " [0.01, 0.13, 0.03, 0.05, 0.02, 0.04, 0.52, 0.02, 0.13, 0.06],\n",
      " [0.03, 0.03, 0.04, 0.06, 0.03, 0.04, 0.05, 0.03, 0.58, 0.10],\n",
      " [0.03, 0.00, 0.31, 0.08, 0.45, 0.06, 0.03, 0.02, 0.02, 0.00],\n",
      " [0.00, 0.00, 0.18, 0.01, 0.60, 0.01, 0.06, 0.15, 0.00, 0.00],\n",
      " [0.00, 0.00, 0.03, 0.53, 0.13, 0.21, 0.08, 0.01, 0.00, 0.00],\n",
      " [0.00, 0.11, 0.09, 0.04, 0.10, 0.01, 0.54, 0.09, 0.02, 0.00],\n",
      " [0.02, 0.18, 0.08, 0.03, 0.12, 0.04, 0.03, 0.01, 0.03, 0.47],\n",
      " [0.00, 0.02, 0.20, 0.21, 0.11, 0.05, 0.36, 0.02, 0.00, 0.01],\n",
      " [0.00, 0.00, 0.39, 0.07, 0.26, 0.03, 0.17, 0.08, 0.00, 0.00],\n",
      " [0.01, 0.67, 0.02, 0.01, 0.01, 0.00, 0.01, 0.01, 0.04, 0.23],\n",
      " [0.26, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.73, 0.00],\n",
      " [0.00, 0.02, 0.04, 0.38, 0.03, 0.33, 0.10, 0.03, 0.02, 0.06],\n",
      " [0.01, 0.12, 0.00, 0.01, 0.00, 0.00, 0.00, 0.04, 0.05, 0.76],\n",
      " [0.03, 0.00, 0.40, 0.07, 0.23, 0.10, 0.10, 0.05, 0.03, 0.00],\n",
      " [0.00, 0.01, 0.06, 0.07, 0.30, 0.08, 0.27, 0.20, 0.00, 0.01],\n",
      " [0.07, 0.04, 0.05, 0.04, 0.03, 0.03, 0.02, 0.00, 0.67, 0.05],\n",
      " [0.18, 0.01, 0.02, 0.01, 0.01, 0.02, 0.00, 0.00, 0.75, 0.01],\n",
      " [0.22, 0.04, 0.01, 0.01, 0.04, 0.01, 0.01, 0.13, 0.23, 0.30],\n",
      " [0.00, 0.00, 0.35, 0.01, 0.50, 0.01, 0.11, 0.02, 0.00, 0.00],\n",
      " [0.03, 0.03, 0.03, 0.05, 0.05, 0.02, 0.05, 0.02, 0.06, 0.67],\n",
      " [0.01, 0.05, 0.01, 0.44, 0.00, 0.31, 0.03, 0.00, 0.13, 0.02],\n",
      " [0.00, 0.05, 0.04, 0.42, 0.01, 0.09, 0.39, 0.00, 0.00, 0.01],\n",
      " [0.06, 0.19, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.69, 0.04],\n",
      " [0.15, 0.01, 0.04, 0.02, 0.01, 0.02, 0.00, 0.01, 0.72, 0.03],\n",
      " [0.00, 0.88, 0.00, 0.05, 0.00, 0.03, 0.02, 0.00, 0.00, 0.01],\n",
      " [0.00, 0.05, 0.50, 0.08, 0.11, 0.05, 0.19, 0.00, 0.00, 0.01],\n",
      " [0.19, 0.01, 0.16, 0.05, 0.02, 0.07, 0.01, 0.01, 0.47, 0.01],\n",
      " [0.29, 0.01, 0.28, 0.11, 0.01, 0.11, 0.00, 0.00, 0.17, 0.01],\n",
      " [0.15, 0.01, 0.02, 0.01, 0.05, 0.00, 0.00, 0.33, 0.12, 0.30],\n",
      " [0.21, 0.01, 0.06, 0.09, 0.03, 0.04, 0.00, 0.02, 0.51, 0.02],\n",
      " [0.23, 0.02, 0.01, 0.01, 0.00, 0.01, 0.00, 0.00, 0.64, 0.07],\n",
      " [0.17, 0.00, 0.02, 0.03, 0.01, 0.02, 0.00, 0.00, 0.73, 0.01],\n",
      " [0.13, 0.07, 0.00, 0.01, 0.01, 0.01, 0.00, 0.01, 0.34, 0.42],\n",
      " [0.18, 0.05, 0.00, 0.00, 0.01, 0.00, 0.00, 0.00, 0.49, 0.26],\n",
      " [0.00, 0.00, 0.21, 0.06, 0.32, 0.07, 0.32, 0.02, 0.00, 0.00],\n",
      " [0.03, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.96, 0.00],\n",
      " [0.01, 0.00, 0.32, 0.05, 0.22, 0.03, 0.27, 0.07, 0.02, 0.01],\n",
      " [0.01, 0.01, 0.27, 0.03, 0.38, 0.03, 0.12, 0.10, 0.02, 0.02],\n",
      " [0.01, 0.00, 0.22, 0.16, 0.13, 0.17, 0.16, 0.05, 0.06, 0.03],\n",
      " [0.00, 0.01, 0.22, 0.05, 0.35, 0.07, 0.28, 0.03, 0.00, 0.00],\n",
      " [0.31, 0.01, 0.10, 0.04, 0.03, 0.04, 0.01, 0.01, 0.45, 0.00],\n",
      " [0.11, 0.00, 0.65, 0.04, 0.02, 0.06, 0.03, 0.08, 0.01, 0.00],\n",
      " [0.03, 0.01, 0.10, 0.20, 0.04, 0.14, 0.02, 0.25, 0.02, 0.19]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mresult\u001b[39m: \u001b[32mTo\u001b[39m.\u001b[32m<refinement>\u001b[39m.this.type.\u001b[32mOutputData\u001b[39m = [[0.01, 0.08, 0.17, 0.35, 0.02, 0.10, 0.19, 0.00, 0.07, 0.00],\n",
       " [0.04, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.56, 0.34],\n",
       " [0.23, 0.04, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.64, 0.07],\n",
       " [0.37, 0.03, 0.05, 0.01, 0.01, 0.01, 0.00, 0.00, 0.51, 0.01],\n",
       " [0.00, 0.00, 0.16, 0.04, 0.59, 0.04, 0.13, 0.03, 0.01, 0.00],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = predictor.predict(test_data)\n",
    "println(s\"result: $result\") //输出判断结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.编写工具方法，从一行INDArray中获得值最大的元素所在的列，目的是获得神经网络判断的结果，方便和原始标签比较以得出正确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfindMaxItemIndex\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**\n",
    "  * 从一行INDArray中获得值最大的元素所在的列\n",
    "  * @param iNDArray\n",
    "  * @return\n",
    "  */\n",
    "def findMaxItemIndex(iNDArray: INDArray): Int = {\n",
    "  val shape = iNDArray.shape()\n",
    "  val col = shape(1)\n",
    "  var maxValue = 0.0\n",
    "  var maxIndex = 0\n",
    "  for (index <- 0 until col) {\n",
    "    val itemValue = iNDArray.getDouble(0, index)\n",
    "    if (itemValue > maxValue) {\n",
    "      maxValue = itemValue\n",
    "      maxIndex = index\n",
    "    }\n",
    "  }\n",
    "  maxIndex\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.判断神经网络对测试数据分类判断的正确率，正确率应该在52%左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result is 47 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mright\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m47\u001b[39m\n",
       "\u001b[36mshape\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m100\u001b[39m, \u001b[32m10\u001b[39m)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var right = 0\n",
    "\n",
    "val shape = result.shape()\n",
    "for (row <- 0 until shape(0)) {\n",
    "  val rowItem = result.getRow(row)\n",
    "  val index = findMaxItemIndex(rowItem)\n",
    "  if (index == test_expect_result.getDouble(row, 0)) {\n",
    "    right += 1\n",
    "  }\n",
    "}\n",
    "println(s\"the result is $right %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "16.[完整代码](https://github.com/izhangzhihao/deeplearning-tutorial/blob/master/src/main/scala/com/thoughtworks/deeplearning/tutorial/TwoLayerNet.scala)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
