{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two layer net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景\n",
    "\n",
    "为了提高预测准确率，我们需要使用多层神经网络，因为一般来说网络的层数越多其表达能力越强，因为其参数更多，能表达出的状态信息更多，所以表达能力越强。多层神经网络可以应对更加复杂的问题，这一节我们先从一个小例子开始：构建一个两层神经网络，这一节我们构建的简单两层神经网络可以达到51%的准确率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                 \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.{FileInputStream, InputStream}\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableHList._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableDouble._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableAny._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray.Layers.Weight\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.{\n",
       "  DifferentiableHList,\n",
       "  DifferentiableINDArray,\n",
       "  Layer,\n",
       "  Symbolic\n",
       "}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Layer.Batch\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathFunctions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathMethods./\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathOps\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.cpu.nativecpu.NDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.ops.transforms.Transforms\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4s.Implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mshapeless._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.immutable.IndexedSeq\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                   ,ReadCIFAR10ToNDArray._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$    ,Utils._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $plugin.$ivy.`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:2.0.0`\n",
    "\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableany:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablenothing:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableseq:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiabledouble:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablefloat:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablehlist:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableindarray:1.0.0-RC7`\n",
    "import $ivy.`org.rauschig:jarchivelib:0.5.0`\n",
    "\n",
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.0`\n",
    "\n",
    "import java.io.{FileInputStream, InputStream}\n",
    "\n",
    "\n",
    "import com.thoughtworks.deeplearning\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import com.thoughtworks.deeplearning.DifferentiableHList._\n",
    "import com.thoughtworks.deeplearning.DifferentiableDouble._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray._\n",
    "import com.thoughtworks.deeplearning.DifferentiableAny._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray.Layers.Weight\n",
    "import com.thoughtworks.deeplearning.{\n",
    "  DifferentiableHList,\n",
    "  DifferentiableINDArray,\n",
    "  Layer,\n",
    "  Symbolic\n",
    "}\n",
    "import com.thoughtworks.deeplearning.Layer.Batch\n",
    "import com.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
    "import com.thoughtworks.deeplearning.Symbolic._\n",
    "import com.thoughtworks.deeplearning.Poly.MathFunctions._\n",
    "import com.thoughtworks.deeplearning.Poly.MathMethods./\n",
    "import com.thoughtworks.deeplearning.Poly.MathOps\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.cpu.nativecpu.NDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
    "import org.nd4j.linalg.ops.transforms.Transforms\n",
    "import org.nd4s.Implicits._\n",
    "import shapeless._\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._\n",
    "\n",
    "import scala.collection.immutable.IndexedSeq\n",
    "import scala.util.Random\n",
    "\n",
    "pprintConfig() = pprintConfig().copy(height = 5)//减少输出的行数，避免页面输出太长\n",
    "\n",
    "import $file.ReadCIFAR10ToNDArray,ReadCIFAR10ToNDArray._\n",
    "import $file.Utils,Utils._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似[前一节](https://thoughtworksinc.github.io/DeepLearning.scala/demo/MiniBatchGradientDescent.html)从CIFAR10 database中读取和处理测试数据的图片和标签信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mNumberOfClasses\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36mtestNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.55, 0.55, 0.56, 0.54, 0.49, 0.45, 0.59, 0.59, 0.62, 0.65, 0.63, 0.62, 0.64, 0.63, 0.64, 0.61, 0.61, 0.62, 0.64, 0.66, 0.67, 0.67, 0.66, 0.62, 0.60, 0.59, 0.57, 0.54, 0.55, 0.55, 0.58, 0.57, 0.57, 0.55, 0.56, 0.53, 0.49, 0.46, 0.59, 0.59, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.55, 0.55, 0.56, 0.54, 0.49, 0.45, 0.59, 0.59, 0.62, 0.65, 0.63, 0.62, 0.64, 0.63, 0.64, 0.61, 0.61, 0.62, 0.64, 0.66, 0.67, 0.67, 0.66, 0.62, 0.60, 0.59, 0.57, 0.54, 0.55, 0.55, 0.58, 0.57, 0.57, 0.55, 0.56, 0.53, 0.49, 0.46, 0.59, 0.59, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.00, 0.00, 9.00, 6.00, 6.00, 5.00, 4.00, 5.00, 9.00, 2.00, 4.00, 1.00, 9.00, 5.00, 4.00, 6.00, 5.00, 6.00, 0.00, 9.00, 3.00, 9.00, 7.00, 6.00, 9.00, 8.00, 0.00, 3.00, 8.00, 8.00, 7.00, 7.00, 4.00, 6.00, 7.00, 3.00, 6.00, 3.00, 6.00, 2.00, 1.0\u001b[33m...\u001b[39m\n",
       "\u001b[36mvectorizedTestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       " [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CIFAR10中的图片共有10个分类(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
    "val NumberOfClasses: Int = 10\n",
    "\n",
    "//加载测试数据，我们读取100条作为测试数据\n",
    "val testNDArray =\n",
    "  ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/test_batch.bin\", 100)\n",
    "\n",
    "val testData = testNDArray.head\n",
    "\n",
    "val testExpectResult = testNDArray.tail.head\n",
    "\n",
    "val vectorizedTestExpectResult = Utils.makeVectorized(testExpectResult, NumberOfClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟前一节不同，这节我们假如一些参数调优的手段，设置学习率和使用[L2Regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)),L2Regularization可以用来避免过拟合。我们还使用了每个迭代learningRate都下降为原来的0.9995倍的办法来解决训练时间增长时因为learningRate相对太大导致loss下降太慢或者不下降的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36moptimizerFactory\u001b[39m: \u001b[32mAnyRef\u001b[39m with \u001b[32mOptimizerFactory\u001b[39m = $sess.cmd2Wrapper$Helper$$anon$2@79671b4a"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit val optimizerFactory = new DifferentiableINDArray.OptimizerFactory {\n",
    "  override def ndArrayOptimizer(weight: Weight): Optimizer = {\n",
    "    new LearningRate with L2Regularization {\n",
    "\n",
    "      var learningRate = 0.001\n",
    "\n",
    "      override protected def currentLearningRate(): Double = {\n",
    "        learningRate *= 0.9995\n",
    "        learningRate\n",
    "      }\n",
    "\n",
    "      override protected def l2Regularization: Double = 0.03\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编写两层神经网络的第一层神经网络，这是一层全连接一层[relu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))的神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfullyConnectedThenRelu\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fullyConnectedThenRelu(inputSize: Int, outputSize: Int)(\n",
    "    implicit row: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val w = (Nd4j.randn(inputSize, outputSize) / math.sqrt(outputSize / 2.0)).toWeight * 0.1\n",
    "  val b = Nd4j.zeros(outputSize).toWeight\n",
    "  max((row dot w) + b, 0.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟上节相同，我们使用softmax作为分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msoftmax\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(implicit scores: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val expScores = exp(scores)\n",
    "  expScores / expScores.sum(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编写两层神经网络的第二层神经网络，这时一层全连接一层softmax的神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfullyConnectedThenSoftmax\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fullyConnectedThenSoftmax(inputSize: Int, outputSize: Int)(\n",
    "    implicit row: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val w = (Nd4j.randn(inputSize, outputSize) / math.sqrt(outputSize)).toWeight //* 0.1\n",
    "  val b = Nd4j.zeros(outputSize).toWeight\n",
    "  softmax.compose((row dot w) + b)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了实现两层神经网络我们将上面的两层神经网络组合起来，组成一个两层神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mhiddenLayer\u001b[39m\n",
       "\u001b[36mpredictor\u001b[39m: (\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m[\u001b[32mINDArray\u001b[39m]{type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray;type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray})#\u001b[32m@\u001b[39m = Compose(Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum(Exp(Identity()),WrappedArray(1)))),PlusINDArray(Dot(Identity(),Weight([[0.48, -0.53, 0.27, -0.03, 0.86, -0.18, -0.08, 0.11, 0.50, 0.30],\n",
       " [-0.34, 0.27, 0.38, 0.19, -0.39, -0.33, 0.24, 1.10, -0.33, 0.02],\n",
       " [0.28, 0.23, -0.47, 0.31, -0.25, -0.16, 0.36, -0.49, -0.33, -0.21],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hiddenLayer(implicit input: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val layer0 = fullyConnectedThenRelu(3072, 500).compose(input)\n",
    "  fullyConnectedThenSoftmax(500, 10).compose(layer0)\n",
    "}\n",
    "\n",
    "val predictor = hiddenLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟上节类似，编写损失函数Loss Function并组合输入层和隐含层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcrossEntropy\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mnetwork\u001b[39m\n",
       "\u001b[36mtrainer\u001b[39m: (\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m[\u001b[32mDouble\u001b[39m]{type OutputData = Double;type OutputDelta = Double;type InputData = shapeless.::[org.nd4j.linalg.api.ndarray.INDArray,shapeless.::[org.nd4j.linalg.api.ndarray.INDArray,shapeless.HNil]];type InputDelta = shapeless.:+:[org.nd4j.linalg.api.ndarray.INDArray,shapeless.:+:[org.nd4j.linalg.api.ndarray.INDArray,shapeless.CNil]]})#\u001b[32m@\u001b[39m = Compose(Negative(ReduceMean(PlusINDArray(MultiplyINDArray(Head(Tail(Identity())),Log(PlusDouble(MultiplyDouble(Head(Identity()),Literal(0.9)),Literal(0.1)))),MultiplyINDArray(PlusDouble(Negative(Head(Tail(Identity()))),Literal(1.0)),Log(PlusDouble(Negative(MultiplyDouble(Head(Identity()),Literal(0.9))),Literal(1.0))))))),HCons(Compose(Compose(Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def crossEntropy(\n",
    "    implicit pair: (INDArray :: INDArray :: HNil) @Symbolic): Double @Symbolic = {\n",
    "  val score = pair.head\n",
    "  val label = pair.tail.head\n",
    "  -(label * log(score * 0.9 + 0.1) + (1.0 - label) * log(1.0 - score * 0.9)).mean\n",
    "}\n",
    "\n",
    "def network(\n",
    "   implicit pair: (INDArray :: INDArray :: HNil) @Symbolic): Double @Symbolic = {\n",
    "  val input = pair.head\n",
    "  val label = pair.tail.head\n",
    "  val score: INDArray @Symbolic = predictor.compose(input)\n",
    "  val hnilLayer: HNil @Symbolic = HNil\n",
    "  crossEntropy.compose(score :: label :: hnilLayer)\n",
    "}\n",
    "\n",
    "val trainer = network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与上一节相同，训练神经网络并观察每次训练loss的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2686176776885986\n",
      "0.24069557189941407\n",
      "0.2286484956741333\n",
      "0.22166686058044432\n",
      "0.2212003707885742\n",
      "0.2047487735748291\n",
      "0.2205038547515869\n",
      "0.2037975788116455\n",
      "0.2008286476135254\n",
      "0.20051753520965576\n",
      "0.1977792501449585\n",
      "0.19189105033874512\n",
      "0.19221527576446534\n",
      "0.19379620552062987\n",
      "0.18764128684997558\n",
      "0.19361141920089722\n",
      "0.20302889347076417\n",
      "0.18837952613830566\n",
      "0.1835470676422119\n",
      "0.19125791788101196\n",
      "0.18389480113983153\n",
      "0.19737205505371094\n",
      "0.19984123706817628\n",
      "0.1866359829902649\n",
      "0.18605028390884398\n",
      "0.18975214958190917\n",
      "0.1769115447998047\n",
      "0.18534992933273314\n",
      "0.18796164989471437\n",
      "0.18478775024414062\n",
      "0.18617348670959472\n",
      "0.18978433609008788\n",
      "0.18722929954528808\n",
      "0.18470964431762696\n",
      "0.17999396324157715\n",
      "0.17767066955566407\n",
      "0.17470579147338866\n",
      "0.18521106243133545\n",
      "0.18271849155426026\n",
      "0.18173846006393432\n",
      "0.17681776285171508\n",
      "0.18871110677719116\n",
      "0.1836523413658142\n",
      "0.18117042779922485\n",
      "0.17274922132492065\n",
      "0.18421320915222167\n",
      "0.1865368366241455\n",
      "0.17430260181427001\n",
      "0.1857973098754883\n",
      "0.17410311698913575\n",
      "0.17901455163955687\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-1370164548\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0],\"y\":[0.2686176776885986,0.2694873332977295,0.27004404067993165,0.2747493743896484,0.2722179651260376,0.2773293972015381,0.2909088134765625,0.2981644868850708,0.39488930702209474,0.36118483543395996,0.3249274969100952,0.41205244064331054,0.38381061553955076,0.39850716590881347,0.33101644515991213,0.40863757133483886,0.37689905166625975,0.2818512678146362,0.25991458892822267,0.2613953113555908,0.2526510238647461,0.257223916053772,0.2533559322357178,0.2535257816314697,0.2525095224380493,0.2510200023651123,0.25837762355804444,0.2568665981292725,0.2551729679107666,0.2509321689605713,0.2427988290786743,0.2493617296218872,0.24201500415802002,0.2516254186630249,0.2479569911956787,0.2452953815460205,0.24409184455871583,0.23802800178527833,0.2395712614059448,0.24069557189941407,0.24119935035705567,0.23671400547027588,0.23826375007629394,0.23468167781829835,0.23820230960845948,0.23724489212036132,0.23923146724700928,0.2375650405883789,0.2306985378265381,0.23479900360107422,0.23473811149597168,0.23682477474212646,0.2382070541381836,0.23569416999816895,0.23722410202026367,0.234200382232666,0.23578925132751466,0.22900636196136476,0.2289111375808716,0.2357019901275635,0.23756914138793944,0.2321178436279297,0.2288806438446045,0.23469076156616211,0.23476214408874513,0.23274292945861816,0.22656793594360353,0.22877194881439208,0.2297813892364502,0.2264636278152466,0.2294917106628418,0.23614258766174318,0.22735674381256105,0.2284471035003662,0.23341166973114014,0.23160963058471679,0.2268157720565796,0.2297295093536377,0.2286484956741333,0.22737412452697753,0.22435948848724366,0.22941551208496094,0.22455377578735353,0.2179473638534546,0.22882251739501952,0.22840263843536376,0.22838711738586426,0.22628872394561766,0.22709572315216064,0.22577896118164062,0.21681566238403321,0.2197704315185547,0.21670608520507811,0.22813687324523926,0.21924197673797607,0.2212442398071289,0.2206261157989502,0.22386770248413085,0.22059803009033202,0.22054839134216309,0.2245091438293457,0.21628932952880858,0.24760966300964354,0.23733348846435548,0.22914695739746094,0.21932647228240967,0.23130426406860352,0.21978497505187988,0.2332207441329956,0.2251976490020752,0.22410235404968262,0.23142549991607667,0.22485480308532715,0.21658790111541748,0.22285468578338624,0.22198810577392578,0.219769287109375,0.22166686058044432,0.23359839916229247,0.22295663356781006,0.22501235008239745,0.22562658786773682,0.22439138889312743,0.2229161739349365,0.2202286958694458,0.22366571426391602,0.21646509170532227,0.2086569547653198,0.22609472274780273,0.22021989822387694,0.21785306930541992,0.21615638732910156,0.22429823875427246,0.21247880458831786,0.216102933883667,0.21771597862243652,0.21747679710388185,0.21831786632537842,0.21841645240783691,0.2132565498352051,0.2221691846847534,0.2177882432937622,0.2251060962677002,0.2110597848892212,0.21886935234069824,0.21029653549194335,0.2125032663345337,0.2137901782989502,0.20997262001037598,0.2177650213241577,0.22881689071655273,0.22176599502563477,0.22410335540771484,0.2269977807998657,0.2245077133178711,0.2238976001739502,0.2212003707885742,0.21654832363128662,0.21616995334625244,0.2242950439453125,0.21889197826385498,0.21297922134399414,0.20863595008850097,0.2093677043914795,0.21445188522338868,0.2186896800994873,0.21485044956207275,0.21287837028503417,0.20334694385528565,0.20747621059417726,0.2113924503326416,0.21260900497436525,0.2142890453338623,0.2143805742263794,0.21866984367370607,0.20461368560791016,0.21247448921203613,0.21540703773498535,0.22168822288513185,0.2212930679321289,0.21165530681610106,0.22092175483703613,0.2067892074584961,0.21402268409729003,0.20575299263000488,0.20641579627990722,0.20677871704101564,0.2059483766555786,0.21330981254577636,0.21273770332336425,0.21714377403259277,0.21541192531585693,0.2031407594680786,0.20331144332885742,0.20209643840789795,0.2047487735748291,0.21420483589172362,0.20601880550384521,0.21300349235534669,0.21744856834411622,0.2103424549102783,0.2092259168624878,0.21350460052490233,0.21325247287750243,0.21792755126953126,0.20945842266082765,0.2157428741455078,0.20575029850006105,0.20607149600982666,0.20646934509277343,0.2062079906463623,0.21066138744354249,0.20951709747314454,0.2081310510635376,0.20749404430389404,0.2211658000946045,0.20450732707977295,0.20001771450042724,0.20589065551757812,0.2186953067779541,0.20851564407348633,0.20986347198486327,0.1986682891845703,0.21048908233642577,0.21340556144714357,0.20712850093841553,0.20721082687377929,0.2103055238723755,0.19814457893371581,0.20792722702026367,0.21076445579528807,0.20410604476928712,0.218361759185791,0.2055903434753418,0.2205038547515869,0.20296213626861573,0.20520486831665039,0.21510028839111328,0.21304492950439452,0.2190110206604004,0.1977214217185974,0.21033964157104493,0.21908175945281982,0.20697410106658937,0.2023787498474121,0.21678128242492675,0.20779967308044434,0.2027158260345459,0.21661806106567383,0.21011013984680177,0.20524847507476807,0.20858376026153563,0.21501688957214354,0.21175217628479004,0.20921871662139893,0.21292343139648437,0.20643534660339355,0.20409226417541504,0.20493745803833008,0.2128415584564209,0.19868698120117187,0.21092963218688965,0.20938396453857422,0.20209252834320068,0.21057770252227784,0.21089725494384765,0.20689816474914552,0.19975147247314454,0.2032064437866211,0.20838937759399415,0.19501472711563111,0.20109786987304687,0.219710636138916,0.2037975788116455,0.2160771369934082,0.23208096027374267,0.2205094575881958,0.21116213798522948,0.22202749252319337,0.2090456962585449,0.19041056632995607,0.20599446296691895,0.20331807136535646,0.2062159299850464,0.21262836456298828,0.21410841941833497,0.20891084671020507,0.20089707374572754,0.20186154842376708,0.2017597436904907,0.19837334156036376,0.2166914463043213,0.21124560832977296,0.2164778709411621,0.19912517070770264,0.20521070957183837,0.2001046657562256,0.21171975135803223,0.20119280815124513,0.2039811611175537,0.21704478263854982,0.2024986743927002,0.21002445220947266,0.2129422664642334,0.2069523811340332,0.1989374876022339,0.20428085327148438,0.2000123977661133,0.20911505222320556,0.20554280281066895,0.19863100051879884,0.20714402198791504,0.2008286476135254,0.20581977367401122,0.20060651302337645,0.20790107250213624,0.20388898849487305,0.20469741821289061,0.20136466026306152,0.2060950756072998,0.20225377082824708,0.19352904558181763,0.20391569137573243,0.19348191022872924,0.19323832988739015,0.20185379981994628,0.21310801506042482,0.19189138412475587,0.19897406101226806,0.20313692092895508,0.1954806089401245,0.19072748422622682,0.2025608539581299,0.2002955675125122,0.20769758224487306,0.21519293785095214,0.20422883033752443,0.2124478340148926,0.19914166927337645,0.20707569122314454,0.20350189208984376,0.2061603546142578,0.2092897891998291,0.19877686500549316,0.2134695053100586,0.20799460411071777,0.19478580951690674,0.20228924751281738,0.19972002506256104,0.20567383766174316,0.19934492111206054,0.20051753520965576,0.20628790855407714,0.20587024688720704,0.2041184902191162,0.20484845638275145,0.20988588333129882,0.21200716495513916,0.21153266429901124,0.20596561431884766,0.19996535778045654,0.20237283706665038,0.20251545906066895,0.20861520767211914,0.19983785152435302,0.2125462055206299,0.20054895877838136,0.20177547931671141,0.19882793426513673,0.19603060483932494,0.20133590698242188,0.19568731784820556,0.21101632118225097,0.20877370834350586,0.20071451663970946,0.2082219123840332,0.19649367332458495,0.20576636791229247,0.2016521453857422,0.20582423210144044,0.19966686964035035,0.20317742824554444,0.19485461711883545,0.20264883041381837,0.18559653759002687,0.19723095893859863,0.21371984481811523,0.19999022483825685,0.20040426254272461,0.20115308761596679,0.1977792501449585,0.20253562927246094,0.20260090827941896,0.2074291467666626,0.19981542825698853,0.20197958946228028,0.1916585922241211,0.20272452831268312,0.20649321079254152,0.19540342092514038,0.18694922924041749,0.20325980186462403,0.20535950660705565,0.19910956621170045,0.2031402587890625,0.18969544172286987,0.1794382691383362,0.19442403316497803,0.19979363679885864,0.19256553649902344,0.20820944309234618,0.19821579456329347,0.2106257915496826,0.20107898712158204,0.19829883575439453,0.2001413345336914,0.1916470408439636,0.20253734588623046,0.20134801864624025,0.20263943672180176,0.1980629563331604,0.19502015113830568,0.19753512144088745,0.2088188648223877,0.2130551815032959,0.19801303148269653,0.20366084575653076,0.19561502933502198,0.20576958656311034,0.19189105033874512,0.20369515419006348,0.2003535270690918,0.19951112270355226,0.2021778106689453,0.20213372707366944,0.1946258544921875,0.21238865852355956,0.20331120491027832,0.19160326719284057,0.20113754272460938,0.19002931118011473,0.19481465816497803,0.1919592022895813,0.20066988468170166,0.19713788032531737,0.20482792854309081,0.19869470596313477,0.19481885433197021,0.19978296756744385,0.2076056718826294,0.19147977828979493,0.18846681118011474,0.2055469274520874,0.1947181463241577,0.18853791952133178,0.19957622289657592,0.20403847694396973,0.19605207443237305,0.20219869613647462,0.20244312286376953,0.19643630981445312,0.20166335105895997,0.2065760850906372,0.19607727527618407,0.19336799383163453,0.19732723236083985,0.19867016077041627,0.2000058650970459,0.19221527576446534,0.20302138328552247,0.2117459297180176,0.19998282194137573,0.20379698276519775,0.19854331016540527,0.20807104110717772,0.1982926607131958,0.19692525863647461,0.18827863931655883,0.1921040654182434,0.19250940084457396,0.2019098997116089,0.1958877444267273,0.20735406875610352,0.1969075918197632,0.19632508754730224,0.1977829933166504,0.19685300588607788,0.19053412675857545,0.20210704803466797,0.20557222366333008,0.19346330165863038,0.20810725688934326,0.19874640703201293,0.19196460247039795,0.1949618339538574,0.19487017393112183,0.1944676160812378,0.18836091756820678,0.2075523853302002,0.20861082077026366,0.19567044973373413,0.20952847003936767,0.19061964750289917,0.2037734031677246,0.18922224044799804,0.21246733665466308,0.19616107940673827,0.19379620552062987,0.20202910900115967,0.19212545156478883,0.20602948665618898,0.2017828941345215,0.19759076833724976,0.1860544800758362,0.19225568771362306,0.1982593536376953,0.1928991675376892,0.19592595100402832,0.2090144634246826,0.19980411529541015,0.20401601791381835,0.1891762375831604,0.20089943408966066,0.1953514814376831,0.19963468313217164,0.18977129459381104,0.20230422019958497,0.206476092338562,0.19730998277664186,0.2012187957763672,0.19927140474319457,0.18898301124572753,0.19841244220733642,0.20362420082092286,0.19562026262283325,0.20190048217773438,0.20156688690185548,0.20346102714538575,0.19052653312683104,0.18820419311523437,0.2151184558868408,0.1929945707321167,0.18284507989883422,0.18881351947784425,0.19533913135528563,0.20061249732971193,0.18764128684997558,0.1907286286354065,0.20253360271453857,0.1901375412940979,0.1999372124671936,0.1874656319618225,0.20441203117370604,0.19472042322158814,0.19647135734558105,0.1977200984954834,0.19203921556472778,0.1922965407371521,0.20732498168945312,0.19046411514282227,0.1981434106826782,0.18822685480117798,0.20180754661560057,0.19283998012542725,0.19355838298797606,0.19687169790267944,0.19795029163360595,0.20028045177459716,0.19096367359161376,0.1938845157623291,0.18896834850311278,0.19495078325271606,0.1982694983482361,0.20815272331237794,0.18820940256118773,0.18366446495056152,0.20013556480407715,0.2035290002822876,0.1896786570549011,0.1919682502746582,0.18356279134750367,0.18627042770385743,0.20127637386322023,0.20120067596435548,0.19049017429351806,0.19361141920089722,0.19236197471618652,0.20283081531524658,0.18966758251190186,0.20132999420166015,0.18628170490264892,0.20061938762664794,0.19705502986907958,0.19243874549865722,0.19149117469787597,0.1885805606842041,0.19966628551483154,0.19563528299331664,0.19522411823272706,0.1870678424835205,0.20233922004699706,0.195018994808197,0.1919098377227783,0.20496172904968263,0.1919936180114746,0.19048521518707276,0.19864174127578735,0.19404885768890381,0.19390808343887328,0.2030019521713257,0.18607383966445923,0.1945786952972412,0.1879488706588745,0.1905691385269165,0.19402432441711426,0.19350090026855468,0.19165698289871216,0.19205180406570435,0.20651702880859374,0.19861009120941162,0.20246450901031493,0.185272741317749,0.20094046592712403,0.1922226905822754,0.20302889347076417,0.20030112266540528,0.19183849096298217,0.18750711679458618,0.21381788253784179,0.19349424839019774,0.19343054294586182,0.19135959148406984,0.18867824077606202,0.19002463817596435,0.19957228899002075,0.18497835397720336,0.1998780131340027,0.1952571988105774,0.1939869999885559,0.1950603246688843,0.2005868673324585,0.2005007028579712,0.20492329597473144,0.1990588665008545,0.20400142669677734,0.18544259071350097,0.20669755935668946,0.19251577854156493,0.19241548776626588,0.1931289553642273,0.1977484941482544,0.18966059684753417,0.19911688566207886,0.18782516717910766,0.1980602264404297,0.19317110776901245,0.18906185626983643,0.19759700298309327,0.2010545015335083,0.1976282238960266,0.19096298217773439,0.19109039306640624,0.1881389856338501,0.18837952613830566,0.19299721717834473,0.1899372935295105,0.18884012699127198,0.19611291885375975,0.1904178261756897,0.18907828330993653,0.19693403244018554,0.19743140935897827,0.1989334225654602,0.19962233304977417,0.195872163772583,0.2014326810836792,0.19107922315597534,0.19476271867752076,0.18569369316101075,0.18980128765106202,0.20287888050079345,0.19890505075454712,0.2058493137359619,0.18344569206237793,0.19513756036758423,0.19648650884628296,0.19603680372238158,0.1929687738418579,0.20471372604370117,0.1880748987197876,0.18627856969833373,0.19871466159820556,0.19977152347564697,0.19417064189910888,0.2007002353668213,0.19097436666488649,0.18923043012619017,0.1924166202545166,0.20185482501983643,0.1973280906677246,0.19788979291915892,0.19043861627578734,0.1835470676422119,0.1829540729522705,0.1948210120201111,0.18924026489257811,0.20332822799682618,0.19045646190643312,0.1870722770690918,0.19382529258728026,0.2024240493774414,0.19036030769348145,0.19881973266601563,0.1954851269721985,0.20265164375305175,0.1863478422164917,0.2039440393447876,0.19002704620361327,0.19233205318450927,0.1999225616455078,0.19807026386260987,0.19200483560562134,0.19439566135406494,0.19196488857269287,0.2044088363647461,0.20316214561462403,0.19509588479995726,0.19264442920684816,0.18343265056610109,0.19553169012069702,0.18913209438323975,0.19464831352233886,0.1964637279510498,0.18462258577346802,0.19888368844985962,0.19472323656082152,0.19205496311187745,0.18837258815765381,0.18890984058380128,0.19517145156860352,0.18605650663375856,0.19125791788101196,0.19544706344604493,0.19094113111495972,0.18801615238189698,0.19568605422973634,0.19468326568603517,0.18239833116531373,0.19908363819122316,0.2004924774169922,0.193082594871521,0.19161741733551024,0.18554245233535765,0.17903282642364501,0.18696377277374268,0.19882433414459227,0.19148625135421754,0.19791125059127807,0.2008202075958252,0.19499369859695434,0.1986379623413086,0.19332138299942017,0.20145821571350098,0.2016286849975586,0.20068705081939697,0.18865723609924318,0.2003119707107544,0.2009514093399048,0.18569419384002686,0.19602031707763673,0.19202080965042115,0.19121748208999634,0.19332478046417237,0.20124440193176268,0.20024592876434327,0.1867907762527466,0.19711694717407227,0.19249554872512817,0.18800121545791626,0.19870448112487793,0.18389480113983153,0.18900609016418457,0.1877872347831726,0.19083123207092284,0.19694876670837402,0.1990714430809021,0.1861741065979004,0.19613717794418334,0.19179402589797973,0.19035015106201172,0.1986081600189209,0.19232194423675536,0.1937711477279663,0.19671275615692138,0.1882235288619995,0.19418506622314452,0.19184569120407105,0.1929224967956543,0.19432035684585572,0.1831258177757263,0.18378721475601195,0.1979574203491211,0.1947875738143921,0.19205354452133178,0.19184935092926025,0.1828611373901367,0.18603142499923705,0.19087527990341185,0.19541001319885254,0.19092910289764403,0.19062628746032714,0.18886735439300537,0.18596209287643434,0.20010292530059814,0.19878939390182496,0.19175946712493896,0.19070472717285156,0.18378878831863404,0.1888439416885376,0.19737205505371094,0.19409018754959106,0.17860789299011232,0.19692302942276002,0.1829731583595276,0.19688388109207153,0.19758729934692382,0.19569357633590698,0.18050963878631593,0.19413177967071532,0.18035695552825928,0.188466215133667,0.19375596046447754,0.17691543102264404,0.1907625675201416,0.181367826461792,0.19201737642288208,0.18498604297637938,0.18236502408981323,0.19887616634368896,0.18514456748962402,0.20098657608032228,0.1976778984069824,0.18947452306747437,0.19834702014923095,0.1830275535583496,0.1908088207244873,0.184019935131073,0.19069381952285766,0.18700761795043946,0.18753397464752197,0.19652166366577148,0.18283278942108155,0.19895079135894775,0.19254117012023925,0.1943755865097046,0.1918630838394165,0.19366713762283325,0.2022146463394165,0.19984123706817628,0.18935444355010986,0.19181478023529053,0.1895066738128662,0.20380854606628418,0.1897963285446167,0.19834530353546143,0.20344827175140381,0.18679096698760986,0.18773105144500732,0.1896073818206787,0.19407949447631836,0.19207077026367186,0.19138784408569337,0.18703696727752686,0.19803634881973267,0.20088047981262208,0.1903249740600586,0.18790080547332763,0.19986599683761597,0.1856825590133667,0.1824846625328064,0.19412510395050048,0.18889234066009522,0.19553713798522948,0.19706394672393798,0.18710815906524658,0.18983011245727538,0.1911052942276001,0.18106510639190673,0.18779165744781495,0.18740296363830566,0.19950284957885742,0.18862855434417725,0.18311731815338134,0.19312448501586915,0.1825890064239502,0.19212522506713867,0.18475277423858644,0.1866359829902649,0.18965470790863037,0.19108185768127442,0.19632747173309326,0.18953632116317748,0.20133473873138427,0.1915709376335144,0.19281268119812012,0.18829573392868043,0.19520277976989747,0.1928207755088806,0.17928495407104492,0.18490389585494996,0.18718076944351197,0.2034470796585083,0.18537048101425171,0.18950040340423585,0.1917944073677063,0.18941173553466797,0.18012373447418212,0.1941748857498169,0.18925175666809083,0.18853110074996948,0.18724061250686647,0.19936716556549072,0.18343380689620972,0.19680430889129638,0.19550323486328125,0.19174007177352906,0.19931399822235107,0.18599528074264526,0.18689839839935302,0.18292388916015626,0.18162503242492675,0.19809108972549438,0.18136024475097656,0.1965132474899292,0.19142768383026124,0.18685858249664306,0.18605028390884398,0.18150513172149657,0.19377754926681517,0.19498636722564697,0.19178558588027955,0.19686659574508666,0.18604938983917235,0.18474384546279907,0.19293618202209473,0.1902424693107605,0.17937111854553223,0.18876341581344605,0.18297934532165527,0.2015451192855835,0.18262619972229005,0.18142392635345458,0.1980663061141968,0.19165399074554443,0.18630261421203614,0.18489183187484742,0.19668562412261964,0.18497624397277831,0.19287630319595336,0.18597190380096434,0.18238710165023803,0.19569063186645508,0.1974535822868347,0.18432338237762452,0.1854855537414551,0.18693890571594238,0.19761090278625487,0.18473975658416747,0.1804870843887329,0.19500405788421632,0.18652610778808593,0.1847579836845398,0.18374032974243165,0.18755931854248048,0.19382392168045043,0.18975214958190917,0.19154962301254272,0.19247748851776122,0.19801329374313353,0.19928568601608276,0.18201112747192383,0.18024809360504152,0.1873108148574829,0.1869171977043152,0.1809455156326294,0.19192221164703369,0.19562358856201173,0.17970904111862182,0.1785213828086853,0.18162033557891846,0.189982008934021,0.18651738166809081,0.18216938972473146,0.19332776069641114,0.1842924475669861,0.18737826347351075,0.18172551393508912,0.19346463680267334,0.18771944046020508,0.18380372524261473,0.1974949598312378,0.17743631601333618,0.19642345905303954,0.18717007637023925,0.18329207897186278,0.19049439430236817,0.1766425609588623,0.19379148483276368,0.1839465856552124,0.1767423629760742,0.19123190641403198,0.1945107936859131,0.18375794887542723,0.18863118886947633,0.1769115447998047,0.1935837984085083,0.1890634536743164,0.1878620982170105,0.1877726435661316,0.1906219720840454,0.17598836421966552,0.19104156494140626,0.1935828447341919,0.19615823030471802,0.19057185649871827,0.19674651622772216,0.18598589897155762,0.18156296014785767,0.19313886165618896,0.19076870679855346,0.18374345302581788,0.17736632823944093,0.18045700788497926,0.18705466985702515,0.18411607742309571,0.1883305788040161,0.1946728467941284,0.1902492642402649,0.19895248413085936,0.1936238646507263,0.18548834323883057,0.1816624641418457,0.18078155517578126,0.19344143867492675,0.1908726930618286,0.18220312595367433,0.18826911449432374,0.19536999464035035,0.18314509391784667,0.1927372097969055,0.18065208196640015,0.1944352149963379,0.1894972324371338,0.18534992933273314,0.19331769943237304,0.1859985113143921,0.19222413301467894,0.18097052574157715,0.18135274648666383,0.18003228902816773,0.18777505159378052,0.18228063583374024,0.18493938446044922,0.1846064567565918,0.17175862789154053,0.1821471333503723,0.18837282657623292,0.20180742740631102,0.18178669214248658,0.17702748775482177,0.18972430229187012,0.18084261417388917,0.19265732765197754,0.18872182369232177,0.1875075101852417,0.1877111554145813,0.19262211322784423,0.19321542978286743,0.1915339231491089,0.1887035846710205,0.18817319869995117,0.18828611373901366,0.19100123643875122,0.18810551166534423,0.1829688787460327,0.18842132091522218,0.18788795471191405,0.1891406536102295,0.19014153480529786,0.18335745334625245,0.1930093765258789,0.18882384300231933,0.18796164989471437,0.18884154558181762,0.18200063705444336,0.18741390705108643,0.18589918613433837,0.18859223127365113,0.18447818756103515,0.18574365377426147,0.19227534532546997,0.17438216209411622,0.18745269775390624,0.17139976024627684,0.1839979648590088,0.19329116344451905,0.1944219470024109,0.185233473777771,0.19537043571472168,0.1863929033279419,0.18674615621566773,0.18627091646194457,0.18840451240539552,0.19808423519134521,0.18422973155975342,0.18911564350128174,0.19777640104293823,0.17834188938140869,0.18352783918380738,0.19244223833084106,0.18764657974243165,0.1856869339942932,0.18699133396148682,0.19382808208465577,0.18967697620391846,0.19088940620422362,0.19157214164733888,0.1820211172103882,0.18617091178894044,0.1792011260986328,0.1913017988204956,0.18478775024414062,0.18761463165283204,0.18015050888061523,0.19609056711196898,0.191627836227417,0.17464011907577515,0.1934237599372864,0.19635435342788696,0.20129168033599854,0.18404603004455566,0.19368360042572022,0.1903182864189148,0.19339275360107422,0.18440788984298706,0.19029324054718016,0.18379054069519044,0.19061028957366943,0.18309156894683837,0.18408286571502686,0.19768931865692138,0.1838786482810974,0.20117921829223634,0.1889331102371216,0.1928488492965698,0.1854250907897949,0.19196487665176393,0.18597764968872071,0.1814357280731201,0.19546246528625488,0.18749451637268066,0.18735153675079347,0.19147272109985353,0.17468700408935547,0.196336567401886,0.1831292152404785,0.18710691928863527,0.19436376094818114,0.189938223361969,0.18874905109405518,0.18617348670959472,0.18369264602661134,0.19030025005340576,0.1838285207748413,0.1914760112762451,0.18892297744750977,0.18038967847824097,0.18042151927947997,0.19042439460754396,0.1818014144897461,0.17029943466186523,0.1798897862434387,0.18641765117645265,0.1864284634590149,0.18003395795822144,0.1972102403640747,0.19383500814437865,0.18432106971740722,0.1701246976852417,0.17950361967086792,0.17768375873565673,0.1865546226501465,0.1910475969314575,0.18324391841888427,0.1953596830368042,0.18172208070755005,0.1856904983520508,0.18391658067703248,0.1841065526008606,0.18843748569488525,0.18483747243881227,0.1853416919708252,0.1901869535446167,0.18172496557235718,0.1803998589515686,0.17803666591644288,0.19304931163787842,0.19513964653015137,0.19122178554534913,0.18978433609008788,0.17967615127563477,0.1847400665283203,0.1788856267929077,0.17576396465301514,0.185233211517334,0.19091465473175048,0.18192341327667236,0.17641034126281738,0.1836638331413269,0.1879685640335083,0.18472785949707032,0.19300308227539062,0.18702682256698608,0.1918842077255249,0.18625043630599974,0.1874643325805664,0.1764054775238037,0.17229874134063722,0.17749781608581544,0.1985952615737915,0.18269723653793335,0.18983614444732666,0.18547251224517822,0.17085580825805663,0.1924631953239441,0.1895977258682251,0.17186462879180908,0.1838434100151062,0.19067063331604003,0.18841670751571654,0.18153822422027588,0.183675217628479,0.1881760001182556,0.19026005268096924,0.17829787731170654,0.18964961767196656,0.17801133394241334,0.19081040620803832,0.18722929954528808,0.17938438653945923,0.1841718554496765,0.18396272659301757,0.19283745288848878,0.18481822013854982,0.18925601243972778,0.18950834274291992,0.18675856590270995,0.18668842315673828,0.20114603042602539,0.19012155532836914,0.1741471290588379,0.1884508490562439,0.2075585126876831,0.17110639810562134,0.19272162914276122,0.19062161445617676,0.19432960748672484,0.1822629451751709,0.19291621446609497,0.18943409919738768,0.1759558916091919,0.18730146884918214,0.17837468385696412,0.19082460403442383,0.19264672994613646,0.17518248558044433,0.17777904272079467,0.18667707443237305,0.17369773387908935,0.18405022621154785,0.18436751365661622,0.1877075433731079,0.1814696431159973,0.17256498336791992,0.19113167524337768,0.19167959690093994,0.1886791467666626,0.18470964431762696,0.18692294359207154,0.1848212957382202,0.1835346817970276,0.1996638536453247,0.17145469188690185,0.1873081684112549,0.1914435625076294,0.19472289085388184,0.19434272050857543,0.189308762550354,0.17843596935272216,0.17197206020355224,0.18366556167602538,0.18980492353439332,0.19194689989089966,0.18796364068984986,0.18432025909423827,0.18566496372222902,0.19034276008605958,0.1773621916770935,0.18865807056427003,0.18877036571502687,0.17761080265045165,0.1854077696800232,0.18784794807434083,0.18591740131378173,0.18350143432617189,0.17557101249694823,0.18148458003997803,0.18985278606414796,0.18126059770584108,0.18181777000427246,0.1861562490463257,0.1870274782180786,0.19605462551116942,0.1769908308982849,0.1889758825302124,0.19040554761886597,0.17999396324157715,0.1888241410255432,0.17844927310943604,0.19466499090194703,0.19153329133987426,0.1852272629737854,0.1833104133605957,0.18584082126617432,0.1730597972869873,0.2025846004486084,0.175803804397583,0.18726160526275634,0.18458194732666017,0.18115991353988647,0.1808137536048889,0.17227165699005126,0.1937563419342041,0.17430486679077148,0.180428147315979,0.17762405872344972,0.1929879903793335,0.18034713268280028,0.1843524694442749,0.18205162286758422,0.18495328426361085,0.18792887926101684,0.1861628532409668,0.1823681116104126,0.18195294141769408,0.1906096816062927,0.1854434370994568,0.18015482425689697,0.18904480934143067,0.17497007846832274,0.19809058904647828,0.18160563707351685,0.1855783224105835,0.19093401432037355,0.19013234376907348,0.17767066955566407,0.17263020277023317,0.1867632746696472,0.18608373403549194,0.18403995037078857,0.1852034091949463,0.19613113403320312,0.18158774375915526,0.1840976357460022,0.19268856048583985,0.17913860082626343,0.17948880195617675,0.18202755451202393,0.17381446361541747,0.18782774209976197,0.18630642890930177,0.1810634732246399,0.18823920488357543,0.19016879796981812,0.18615095615386962,0.1897432565689087,0.17754015922546387,0.18372279405593872,0.18061323165893556,0.18100624084472655,0.18309977054595947,0.1827976942062378,0.18778253793716432,0.18572975397109986,0.18246808052062988,0.18994296789169313,0.18186371326446532,0.1805218815803528,0.18172202110290528,0.1891998291015625,0.1857285976409912,0.1873642921447754,0.18253345489501954,0.18226253986358643,0.17470579147338866,0.18183484077453613,0.18438143730163575,0.18420617580413817,0.17666070461273192,0.1786661386489868,0.18586454391479493,0.18603627681732177,0.18172744512557984,0.18517953157424927,0.1893346428871155,0.1867594003677368,0.18005048036575316,0.18335349559783937,0.18487205505371093,0.19134719371795655,0.18149603605270387,0.19174654483795167,0.18215694427490234,0.17970755100250244,0.1823355197906494,0.18619747161865235,0.1832819938659668,0.1931234359741211,0.18394334316253663,0.19712543487548828,0.1739656448364258,0.18289034366607665,0.18131233453750611,0.19317046403884888,0.18777750730514525,0.18585164546966554,0.17219432592391967,0.18103425502777098,0.183394193649292,0.1969069242477417,0.19022910594940184,0.1757217526435852,0.1769806146621704,0.18521106243133545,0.20292787551879882,0.17855929136276244,0.18327486515045166,0.19217488765716553,0.19273360967636108,0.1807809591293335,0.19992704391479493,0.18034605979919432,0.19600415229797363,0.1863683581352234,0.19084601402282714,0.1869550585746765,0.18886607885360718,0.18758676052093506,0.17733880281448364,0.18057844638824463,0.18530752658843994,0.1794813871383667,0.18485387563705444,0.1826549768447876,0.18538961410522461,0.18676629066467285,0.18588838577270508,0.1828732371330261,0.18189268112182616,0.19615195989608764,0.1860137701034546,0.190021288394928,0.1800314426422119,0.18901528120040895,0.18667315244674682,0.1787724494934082,0.1781760573387146,0.18154439926147461,0.17975707054138185,0.1893762469291687,0.18296151161193847,0.17673935890197753,0.18271849155426026,0.18439629077911376,0.19033652544021606,0.17326390743255615,0.1865938901901245,0.17026478052139282,0.1854628562927246,0.16975419521331786,0.1893574118614197,0.17531803846359253,0.18983619213104247,0.17847867012023927,0.17881349325180054,0.1834794759750366,0.17328245639801027,0.1878791332244873,0.18508923053741455,0.1749240517616272,0.18660171031951905,0.1976879596710205,0.18198769092559813,0.18091871738433837,0.18562365770339967,0.18714916706085205,0.18102092742919923,0.181471848487854,0.18694097995758058,0.184308660030365,0.18153301477432252,0.17882633209228516,0.1763453722000122,0.18852753639221193,0.18854660987854005,0.1813279867172241,0.18842414617538453,0.1827622175216675,0.17779004573822021,0.19342952966690063,0.18591785430908203,0.18173846006393432,0.18777426481246948,0.19025843143463134,0.18799116611480712,0.184279727935791,0.17732644081115723,0.17761549949645997,0.18372255563735962,0.1845625400543213,0.18279728889465333,0.17963534593582153,0.1843427300453186,0.1945611834526062,0.17212152481079102,0.1872786045074463,0.18539035320281982,0.1716475009918213,0.1819223165512085,0.17601544857025148,0.1845913052558899,0.1677032232284546,0.1902567982673645,0.18323323726654053,0.18175871372222902,0.185286283493042,0.1906832456588745,0.18701021671295165,0.19076859951019287,0.1770044207572937,0.17597941160202027,0.18495014905929566,0.18898262977600097,0.181427264213562,0.19581973552703857,0.18290610313415528,0.19651225805282593,0.19133455753326417,0.19099364280700684,0.19597852230072021,0.17681776285171508,0.1872997522354126,0.18400399684906005,0.1876589059829712,0.1815486192703247,0.18878209590911865,0.1840994119644165,0.18780813217163086,0.1766071081161499,0.17993898391723634,0.18625385761260987,0.17995896339416503,0.17693691253662108,0.17947131395339966,0.18945519924163817,0.18173410892486572,0.17786566019058228,0.1920719027519226,0.18208963871002198,0.17597310543060302,0.17933111190795897,0.17935434579849244,0.19296984672546386,0.18136131763458252,0.1847897171974182,0.186402428150177,0.1843844771385193,0.18269587755203248,0.18104994297027588,0.17323111295700072,0.18544316291809082,0.19305452108383178,0.18485550880432128,0.1747444748878479,0.18253638744354247,0.18268129825592042,0.1896427035331726,0.18424696922302247,0.17750287055969238,0.18871110677719116,0.18828682899475097,0.18706774711608887,0.18365873098373414,0.1852602481842041,0.19029171466827394,0.1891653299331665,0.18027780055999756,0.1873496174812317,0.1826695680618286,0.18279714584350587,0.18963274955749512,0.1813082814216614,0.18579459190368652,0.18036314249038696,0.1671452760696411,0.18895180225372316,0.17350196838378906,0.19413907527923585,0.18308579921722412,0.18482707738876342,0.17943476438522338,0.18625028133392335,0.17577435970306396,0.18834311962127687,0.18120315074920654,0.1875750780105591,0.17326028347015382,0.18229305744171143,0.19482650756835937,0.18123998641967773,0.19084137678146362,0.18404754400253295,0.18639779090881348,0.18482786417007446,0.17588218450546264,0.19531713724136351,0.18379999399185182,0.18419320583343507,0.1836523413658142,0.18551263809204102,0.17362160682678224,0.1867983341217041,0.18361220359802247,0.17349762916564943,0.17301089763641359,0.1896902799606323,0.19144362211227417,0.18572049140930175,0.16313499212265015,0.18682522773742677,0.18589553833007813,0.18839797973632813,0.18509488105773925,0.18448193073272706,0.17461881637573243,0.18828217983245848,0.1800256609916687,0.18782755136489868,0.179193115234375,0.18548710346221925,0.1919788360595703,0.17692406177520753,0.18520760536193848,0.18170411586761476,0.19009628295898437,0.17776949405670167,0.17909018993377684,0.17119486331939698,0.19734020233154298,0.17888052463531495,0.18735086917877197,0.1821837067604065,0.18714030981063842,0.18056185245513917,0.184975266456604,0.18443422317504882,0.17797422409057617,0.18117042779922485,0.18143129348754883,0.1823939561843872,0.18268834352493285,0.18684760332107545,0.1785817861557007,0.18005990982055664,0.1786104679107666,0.1927915096282959,0.18359296321868895,0.1828530550003052,0.194583523273468,0.17974897623062133,0.190613055229187,0.18096675872802734,0.19307587146759034,0.1800439953804016,0.18772422075271605,0.18579068183898925,0.17518815994262696,0.18721439838409423,0.18320794105529786,0.18392527103424072,0.18410742282867432,0.181655216217041,0.18355998992919922,0.18788647651672363,0.19166078567504882,0.18401702642440795,0.17707514762878418,0.16945188045501708,0.18977850675582886,0.18037089109420776,0.18165953159332277,0.185028338432312,0.18478925228118898,0.18161758184432983,0.17418832778930665,0.1881244421005249,0.17274922132492065,0.18087009191513062,0.1709747076034546,0.1756427764892578,0.1875462293624878,0.1724878191947937,0.17263624668121338,0.18890393972396852,0.18242449760437013,0.19116930961608886,0.17554683685302735,0.177620267868042,0.19231855869293213,0.17916210889816284,0.1911612033843994,0.1932186961174011,0.18532025814056396,0.1808328628540039,0.1961515426635742,0.1793581485748291,0.18369702100753785,0.17605527639389038,0.17914611101150513,0.2017225742340088,0.18811830282211303,0.18371750116348268,0.18228498697280884,0.1875736355781555,0.17828528881072997,0.1757168412208557,0.1793917417526245,0.19136312007904052,0.18550201654434204,0.18919296264648439,0.1806393265724182,0.1691413402557373,0.18292713165283203,0.18277015686035156,0.19281575679779053,0.18421320915222167,0.19188276529312134,0.17419965267181398,0.18705487251281738,0.18062208890914916,0.18049070835113526,0.18094997406005858,0.18035043478012086,0.16759977340698243,0.19184139966964722,0.1769044041633606,0.18295061588287354,0.18132736682891845,0.18230803012847902,0.19545602798461914,0.18897361755371095,0.18209229707717894,0.1685454249382019,0.1853098154067993,0.1876594066619873,0.18413041830062865,0.1745108962059021,0.17099282741546631,0.18225797414779663,0.1782091498374939,0.1802576184272766,0.18953027725219726,0.17273048162460328,0.1824744462966919,0.18616676330566406,0.18580517768859864,0.1733453631401062,0.18421083688735962,0.18816524744033813,0.1833725690841675,0.17476460933685303,0.18356475830078126,0.17287120819091797,0.17402106523513794,0.1865368366241455,0.17959784269332885,0.17966901063919066,0.17987704277038574,0.18807106018066405,0.17991290092468262,0.184309720993042,0.1811236023902893,0.18522300720214843,0.17753692865371704,0.1926257848739624,0.18069181442260743,0.17603001594543458,0.17728190422058104,0.1843722343444824,0.18007817268371581,0.1784426212310791,0.17866790294647217,0.18373053073883056,0.18911761045455933,0.1906508684158325,0.1785219430923462,0.19011799097061158,0.18164687156677245,0.18527753353118898,0.18192474842071532,0.1730821967124939,0.18145146369934081,0.17497165203094484,0.18881756067276,0.1861112594604492,0.18514766693115234,0.17509257793426514,0.17444533109664917,0.19256927967071533,0.1843555212020874,0.18209389448165894,0.1872983455657959,0.17953135967254638,0.17430260181427001,0.176603102684021,0.16614378690719606,0.18515677452087403,0.19073547124862672,0.18170578479766847,0.18651368618011474,0.19009979963302612,0.1731606364250183,0.18990838527679443,0.19260880947113038,0.18938392400741577,0.17894256114959717,0.19149965047836304,0.17196037769317626,0.18938915729522704,0.1879440188407898,0.18822066783905028,0.18456311225891114,0.1787027597427368,0.18665411472320556,0.18042469024658203,0.1762845993041992,0.17966504096984864,0.1931777000427246,0.17449779510498048,0.17362136840820314,0.17781566381454467,0.1819406270980835,0.18815476894378663,0.17486996650695802,0.1776189088821411,0.19449750185012818,0.1718626856803894,0.1855883479118347,0.1889878749847412,0.18367364406585693,0.1766175627708435,0.17738806009292601,0.1857973098754883,0.17517383098602296,0.18352384567260743,0.18231027126312255,0.17729005813598633,0.17061278820037842,0.18689563274383544,0.16891900300979615,0.17719464302062987,0.17862954139709472,0.17210334539413452,0.18431246280670166,0.1845770835876465,0.1737891674041748,0.17981456518173217,0.18928234577178954,0.1871103525161743,0.1799176573753357,0.17650814056396485,0.17298858165740966,0.1759059190750122,0.17784311771392822,0.1924064874649048,0.18188436031341554,0.17308540344238282,0.17152724266052247,0.18009543418884277,0.17857465744018555,0.172829532623291,0.18010337352752687,0.18526457548141478,0.1857614517211914,0.18793784379959105,0.18450013399124146,0.17641153335571289,0.188296377658844,0.17945575714111328,0.1761581540107727,0.18790743350982667,0.17410311698913575,0.18504350185394286,0.19761184453964234,0.197886323928833,0.18789867162704468,0.1784237504005432,0.180259108543396,0.17215368747711182,0.17762573957443237,0.1745526075363159,0.18561562299728393,0.1865846633911133,0.1813351631164551,0.174544358253479,0.17683351039886475,0.18876612186431885,0.18518965244293212,0.17638130187988282,0.1923420786857605,0.17469635009765624,0.180531907081604,0.1745741844177246,0.1807103395462036,0.1733250617980957,0.18331379890441896,0.1792951464653015,0.17248146533966063,0.1770609736442566,0.17576011419296264,0.18092976808547973,0.17906521558761596,0.18523020744323732,0.18737707138061524,0.17863316535949708,0.18437613248825074,0.18302496671676635,0.178261137008667,0.1798364758491516,0.18623108863830568,0.17901455163955687,0.18935637474060057,0.18092713356018067,0.17443933486938476,0.18819656372070312,0.18473343849182128,0.17449817657470704,0.1821896553039551,0.1763387680053711,0.18333637714385986,0.18618370294570924,0.19252749681472778,0.19223066568374633,0.1797313928604126,0.17800356149673463,0.17189664840698243,0.18063493967056274,0.18791275024414061,0.1890702247619629,0.195961594581604,0.17798357009887694,0.16980162858963013,0.17404038906097413,0.18326587677001954,0.19305217266082764,0.18328728675842285,0.18479135036468505,0.18295741081237793,0.18892594575881957,0.17700403928756714,0.17489405870437622,0.17862144708633423,0.16827104091644288,0.181471586227417,0.17942323684692382,0.17683016061782836,0.18589508533477783,0.186700701713562,0.18263003826141358]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss by time\"};\n",
       "\n",
       "  Plotly.plot('plot-1370164548', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrandom\u001b[39m: \u001b[32mRandom\u001b[39m = scala.util.Random@7549cad6\n",
       "\u001b[36mMiniBatchSize\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m256\u001b[39m\n",
       "\u001b[36mlossSeq\u001b[39m: \u001b[32mIndexedSeq\u001b[39m[\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m.\u001b[32m<refinement>\u001b[39m.this.type.\u001b[32mOutputData\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m0.2686176776885986\u001b[39m,\n",
       "  \u001b[32m0.2694873332977295\u001b[39m,\n",
       "  \u001b[32m0.27004404067993165\u001b[39m,\n",
       "  \u001b[32m0.2747493743896484\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mplot\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mScatter\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mScatter\u001b[39m(\n",
       "    \u001b[33mSome\u001b[39m(\n",
       "      \u001b[33mDoubles\u001b[39m(\n",
       "        \u001b[33mVector\u001b[39m(\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mres8_4\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-1370164548\"\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val random = new Random\n",
    "\n",
    "val MiniBatchSize = 256\n",
    "\n",
    "val lossSeq =\n",
    "  (\n",
    "    for (_ <- 0 to 50) yield {\n",
    "      val randomIndex = random\n",
    "        .shuffle[Int, IndexedSeq](0 until 10000) //https://issues.scala-lang.org/browse/SI-6948\n",
    "        .toArray\n",
    "      for (times <- 0 until 10000 / MiniBatchSize) yield {\n",
    "        val randomIndexArray =\n",
    "          randomIndex.slice(times * MiniBatchSize,\n",
    "                            (times + 1) * MiniBatchSize)\n",
    "        val trainNDArray :: expectLabel :: shapeless.HNil =\n",
    "          ReadCIFAR10ToNDArray.getSGDTrainNDArray(randomIndexArray)\n",
    "        val input =\n",
    "          trainNDArray.reshape(MiniBatchSize, 3072)\n",
    "\n",
    "        val expectLabelVectorized =\n",
    "          Utils.makeVectorized(expectLabel, NumberOfClasses)\n",
    "        val loss = trainer.train(input :: expectLabelVectorized :: HNil)\n",
    "        if(times == 0){\n",
    "          println(loss)\n",
    "        }\n",
    "        loss\n",
    "      }\n",
    "    }\n",
    "  ).flatten\n",
    "\n",
    "val plot = Seq(\n",
    "  Scatter(lossSeq.indices, lossSeq)\n",
    ")\n",
    "\n",
    "plot.plot(\n",
    "  title = \"loss by time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟上一节相同，我们使用测试数据来查看神经网络判断结果并计算准确率。这次准确率应该会上升到51%左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: [[0.01, 0.15, 0.11, 0.40, 0.01, 0.13, 0.12, 0.00, 0.06, 0.00],\n",
      " [0.06, 0.09, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.43, 0.41],\n",
      " [0.17, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.70, 0.06],\n",
      " [0.37, 0.04, 0.03, 0.01, 0.01, 0.00, 0.00, 0.01, 0.52, 0.01],\n",
      " [0.00, 0.01, 0.11, 0.04, 0.62, 0.03, 0.15, 0.03, 0.01, 0.00],\n",
      " [0.00, 0.06, 0.02, 0.18, 0.02, 0.09, 0.60, 0.01, 0.00, 0.02],\n",
      " [0.00, 0.11, 0.01, 0.55, 0.00, 0.23, 0.10, 0.00, 0.00, 0.00],\n",
      " [0.00, 0.00, 0.19, 0.03, 0.34, 0.01, 0.38, 0.05, 0.00, 0.00],\n",
      " [0.01, 0.04, 0.30, 0.18, 0.16, 0.20, 0.03, 0.03, 0.02, 0.00],\n",
      " [0.02, 0.77, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.15, 0.05],\n",
      " [0.21, 0.01, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.76, 0.01],\n",
      " [0.00, 0.22, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.02, 0.75],\n",
      " [0.00, 0.26, 0.02, 0.24, 0.05, 0.12, 0.27, 0.04, 0.01, 0.01],\n",
      " [0.05, 0.44, 0.01, 0.08, 0.00, 0.02, 0.24, 0.06, 0.02, 0.06],\n",
      " [0.05, 0.38, 0.06, 0.03, 0.01, 0.00, 0.02, 0.06, 0.17, 0.21],\n",
      " [0.12, 0.00, 0.14, 0.03, 0.17, 0.03, 0.02, 0.03, 0.44, 0.01],\n",
      " [0.00, 0.01, 0.01, 0.19, 0.00, 0.64, 0.05, 0.07, 0.03, 0.00],\n",
      " [0.13, 0.04, 0.11, 0.21, 0.17, 0.07, 0.12, 0.06, 0.01, 0.07],\n",
      " [0.07, 0.03, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.86, 0.04],\n",
      " [0.00, 0.01, 0.01, 0.04, 0.04, 0.03, 0.74, 0.13, 0.00, 0.01],\n",
      " [0.06, 0.04, 0.08, 0.03, 0.59, 0.03, 0.03, 0.06, 0.02, 0.07],\n",
      " [0.27, 0.00, 0.66, 0.02, 0.01, 0.02, 0.00, 0.02, 0.00, 0.00],\n",
      " [0.65, 0.04, 0.02, 0.01, 0.02, 0.00, 0.00, 0.00, 0.23, 0.02],\n",
      " [0.00, 0.52, 0.00, 0.02, 0.01, 0.01, 0.02, 0.04, 0.00, 0.37],\n",
      " [0.00, 0.00, 0.22, 0.00, 0.62, 0.01, 0.01, 0.13, 0.00, 0.00],\n",
      " [0.01, 0.07, 0.27, 0.01, 0.16, 0.01, 0.36, 0.02, 0.08, 0.02],\n",
      " [0.00, 0.05, 0.08, 0.15, 0.29, 0.06, 0.30, 0.05, 0.00, 0.02],\n",
      " [0.26, 0.02, 0.11, 0.02, 0.13, 0.02, 0.01, 0.22, 0.12, 0.09],\n",
      " [0.01, 0.71, 0.01, 0.02, 0.01, 0.00, 0.01, 0.01, 0.00, 0.22],\n",
      " [0.00, 0.01, 0.13, 0.05, 0.17, 0.03, 0.58, 0.03, 0.00, 0.00],\n",
      " [0.00, 0.13, 0.18, 0.16, 0.09, 0.03, 0.33, 0.06, 0.00, 0.01],\n",
      " [0.00, 0.00, 0.34, 0.09, 0.24, 0.19, 0.07, 0.05, 0.00, 0.00],\n",
      " [0.02, 0.03, 0.13, 0.12, 0.26, 0.14, 0.15, 0.04, 0.10, 0.01],\n",
      " [0.00, 0.06, 0.17, 0.11, 0.02, 0.01, 0.54, 0.08, 0.00, 0.01],\n",
      " [0.08, 0.10, 0.00, 0.01, 0.00, 0.00, 0.00, 0.01, 0.11, 0.69],\n",
      " [0.00, 0.40, 0.08, 0.06, 0.11, 0.04, 0.08, 0.20, 0.01, 0.01],\n",
      " [0.00, 0.01, 0.22, 0.15, 0.23, 0.08, 0.22, 0.08, 0.00, 0.01],\n",
      " [0.01, 0.15, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.13, 0.69],\n",
      " [0.02, 0.63, 0.01, 0.04, 0.00, 0.01, 0.03, 0.01, 0.07, 0.18],\n",
      " [0.03, 0.01, 0.04, 0.13, 0.05, 0.37, 0.03, 0.06, 0.28, 0.00],\n",
      " [0.55, 0.02, 0.03, 0.01, 0.09, 0.00, 0.00, 0.15, 0.13, 0.02],\n",
      " [0.00, 0.00, 0.18, 0.04, 0.19, 0.03, 0.52, 0.03, 0.00, 0.00],\n",
      " [0.00, 0.07, 0.01, 0.49, 0.00, 0.02, 0.01, 0.04, 0.00, 0.35],\n",
      " [0.00, 0.05, 0.11, 0.10, 0.25, 0.06, 0.37, 0.04, 0.00, 0.01],\n",
      " [0.31, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.63, 0.04],\n",
      " [0.08, 0.04, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.41, 0.46],\n",
      " [0.00, 0.02, 0.02, 0.39, 0.04, 0.30, 0.17, 0.02, 0.00, 0.04],\n",
      " [0.07, 0.02, 0.00, 0.02, 0.00, 0.00, 0.00, 0.00, 0.80, 0.09],\n",
      " [0.00, 0.00, 0.15, 0.01, 0.56, 0.01, 0.12, 0.16, 0.00, 0.00],\n",
      " [0.00, 0.00, 0.23, 0.01, 0.34, 0.01, 0.40, 0.01, 0.00, 0.00],\n",
      " [0.03, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.13, 0.77],\n",
      " [0.14, 0.07, 0.11, 0.01, 0.21, 0.01, 0.02, 0.10, 0.25, 0.09],\n",
      " [0.00, 0.08, 0.03, 0.07, 0.04, 0.05, 0.68, 0.03, 0.01, 0.01],\n",
      " [0.01, 0.07, 0.02, 0.38, 0.04, 0.20, 0.11, 0.09, 0.01, 0.06],\n",
      " [0.03, 0.03, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.89, 0.04],\n",
      " [0.21, 0.07, 0.02, 0.01, 0.02, 0.01, 0.00, 0.00, 0.62, 0.05],\n",
      " [0.02, 0.00, 0.06, 0.16, 0.15, 0.20, 0.03, 0.35, 0.02, 0.01],\n",
      " [0.01, 0.14, 0.02, 0.04, 0.02, 0.02, 0.56, 0.03, 0.09, 0.08],\n",
      " [0.05, 0.06, 0.03, 0.06, 0.04, 0.03, 0.06, 0.02, 0.45, 0.21],\n",
      " [0.05, 0.01, 0.40, 0.10, 0.30, 0.05, 0.05, 0.03, 0.01, 0.00],\n",
      " [0.00, 0.00, 0.15, 0.00, 0.48, 0.01, 0.11, 0.24, 0.00, 0.00],\n",
      " [0.00, 0.00, 0.03, 0.49, 0.18, 0.14, 0.14, 0.02, 0.00, 0.00],\n",
      " [0.00, 0.08, 0.02, 0.02, 0.10, 0.01, 0.63, 0.13, 0.01, 0.00],\n",
      " [0.03, 0.30, 0.03, 0.05, 0.03, 0.04, 0.01, 0.03, 0.03, 0.46],\n",
      " [0.00, 0.06, 0.17, 0.18, 0.12, 0.03, 0.38, 0.02, 0.00, 0.02],\n",
      " [0.00, 0.00, 0.32, 0.07, 0.35, 0.04, 0.13, 0.07, 0.00, 0.00],\n",
      " [0.01, 0.74, 0.01, 0.00, 0.01, 0.00, 0.01, 0.02, 0.02, 0.18],\n",
      " [0.50, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.50, 0.00],\n",
      " [0.01, 0.05, 0.02, 0.37, 0.04, 0.24, 0.10, 0.06, 0.03, 0.09],\n",
      " [0.04, 0.12, 0.00, 0.01, 0.00, 0.00, 0.00, 0.03, 0.04, 0.76],\n",
      " [0.06, 0.01, 0.35, 0.08, 0.17, 0.08, 0.14, 0.06, 0.04, 0.01],\n",
      " [0.00, 0.02, 0.05, 0.07, 0.26, 0.08, 0.26, 0.24, 0.00, 0.01],\n",
      " [0.13, 0.10, 0.03, 0.02, 0.02, 0.02, 0.02, 0.00, 0.59, 0.06],\n",
      " [0.18, 0.01, 0.01, 0.00, 0.00, 0.01, 0.00, 0.00, 0.77, 0.01],\n",
      " [0.33, 0.04, 0.00, 0.01, 0.02, 0.00, 0.01, 0.09, 0.18, 0.33],\n",
      " [0.00, 0.00, 0.41, 0.01, 0.40, 0.00, 0.14, 0.03, 0.00, 0.00],\n",
      " [0.02, 0.05, 0.01, 0.03, 0.02, 0.01, 0.04, 0.02, 0.04, 0.77],\n",
      " [0.02, 0.08, 0.01, 0.40, 0.00, 0.22, 0.02, 0.01, 0.19, 0.05],\n",
      " [0.00, 0.09, 0.02, 0.44, 0.01, 0.11, 0.27, 0.01, 0.00, 0.05],\n",
      " [0.09, 0.58, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.25, 0.07],\n",
      " [0.21, 0.02, 0.02, 0.01, 0.00, 0.01, 0.00, 0.01, 0.67, 0.04],\n",
      " [0.00, 0.82, 0.00, 0.07, 0.00, 0.03, 0.04, 0.01, 0.00, 0.03],\n",
      " [0.00, 0.06, 0.61, 0.07, 0.08, 0.03, 0.15, 0.00, 0.00, 0.01],\n",
      " [0.31, 0.01, 0.16, 0.05, 0.01, 0.04, 0.01, 0.01, 0.37, 0.03],\n",
      " [0.51, 0.02, 0.21, 0.07, 0.01, 0.06, 0.00, 0.00, 0.09, 0.02],\n",
      " [0.17, 0.01, 0.02, 0.01, 0.05, 0.00, 0.00, 0.32, 0.10, 0.32],\n",
      " [0.34, 0.01, 0.04, 0.23, 0.01, 0.04, 0.00, 0.03, 0.25, 0.03],\n",
      " [0.33, 0.03, 0.00, 0.00, 0.00, 0.01, 0.00, 0.00, 0.54, 0.08],\n",
      " [0.27, 0.00, 0.01, 0.02, 0.01, 0.01, 0.00, 0.01, 0.66, 0.01],\n",
      " [0.12, 0.08, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.24, 0.53],\n",
      " [0.19, 0.06, 0.00, 0.00, 0.01, 0.00, 0.00, 0.00, 0.39, 0.33],\n",
      " [0.00, 0.00, 0.23, 0.05, 0.38, 0.07, 0.23, 0.03, 0.00, 0.00],\n",
      " [0.06, 0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.93, 0.01],\n",
      " [0.02, 0.01, 0.34, 0.06, 0.14, 0.01, 0.28, 0.12, 0.02, 0.02],\n",
      " [0.05, 0.02, 0.20, 0.03, 0.40, 0.02, 0.11, 0.13, 0.02, 0.02],\n",
      " [0.02, 0.01, 0.16, 0.24, 0.12, 0.14, 0.16, 0.05, 0.06, 0.03],\n",
      " [0.00, 0.01, 0.16, 0.06, 0.35, 0.04, 0.33, 0.04, 0.00, 0.00],\n",
      " [0.46, 0.01, 0.07, 0.03, 0.03, 0.03, 0.00, 0.01, 0.35, 0.01],\n",
      " [0.25, 0.00, 0.48, 0.11, 0.01, 0.02, 0.04, 0.08, 0.00, 0.00],\n",
      " [0.02, 0.01, 0.03, 0.23, 0.02, 0.07, 0.01, 0.31, 0.00, 0.30]]\n",
      "the result is 51.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mresult\u001b[39m: \u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m.\u001b[32m<refinement>\u001b[39m.this.type.\u001b[32mOutputData\u001b[39m = [[0.01, 0.15, 0.11, 0.40, 0.01, 0.13, 0.12, 0.00, 0.06, 0.00],\n",
       " [0.06, 0.09, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.43, 0.41],\n",
       " [0.17, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.70, 0.06],\n",
       " [0.37, 0.04, 0.03, 0.01, 0.01, 0.00, 0.00, 0.01, 0.52, 0.01],\n",
       " [0.00, 0.01, 0.11, 0.04, 0.62, 0.03, 0.15, 0.03, 0.01, 0.00],\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mright\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m51.0\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = predictor.predict(testData)\n",
    "println(s\"result: $result\") //输出判断结果\n",
    "\n",
    "val right = Utils.getAccuracy(result, testExpectResult)\n",
    "println(s\"the result is $right %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[完整代码](https://github.com/izhangzhihao/deeplearning-tutorial/blob/master/src/main/scala/com/thoughtworks/deeplearning/tutorial/TwoLayerNet.scala)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
