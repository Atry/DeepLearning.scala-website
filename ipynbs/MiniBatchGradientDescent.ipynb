{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景\n",
    "\n",
    "在大规模数据训练时，数据可以达到百万级量级。如果计算整个训练集，来获得仅仅一个参数的更新速度就太慢了。一个常用的方法是计算训练集中的小批量min-batche）数据随机梯度下降快速实现神经网络参数更新。这节我们将通过使用[Mini-Batch Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) 来实现小批量数据随机梯度下降快速更新网络参数，这样神经网络的准确率可以达到40%。\n",
    "\n",
    "参考：\n",
    "\n",
    "[Mini-Batch Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent): 在大规模数据训练时，数据可以达到百万级量级。如果计算整个训练集，来获得仅仅一个参数的更新速度就太慢了。一个常用的方法是计算训练集中的小批量（batches）数据以提升参数更新速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 引入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                 \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.{FileInputStream, InputStream}\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableHList._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableDouble._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableAny._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Layer.Batch\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.{\n",
       "  DifferentiableHList,\n",
       "  DifferentiableINDArray,\n",
       "  Layer,\n",
       "  Symbolic\n",
       "}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathFunctions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathMethods./\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathOps\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.cpu.nativecpu.NDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.ops.transforms.Transforms\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4s.Implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mshapeless._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.immutable.IndexedSeq\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$    \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $plugin.$ivy.`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:2.0.0`\n",
    "\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableany:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablenothing:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableseq:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiabledouble:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablefloat:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablehlist:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableindarray:1.0.0-RC7`\n",
    "import $ivy.`org.rauschig:jarchivelib:0.5.0`\n",
    "\n",
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.0`\n",
    "\n",
    "import java.io.{FileInputStream, InputStream}\n",
    "\n",
    "\n",
    "import com.thoughtworks.deeplearning\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import com.thoughtworks.deeplearning.DifferentiableHList._\n",
    "import com.thoughtworks.deeplearning.DifferentiableDouble._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray._\n",
    "import com.thoughtworks.deeplearning.DifferentiableAny._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
    "import com.thoughtworks.deeplearning.Layer.Batch\n",
    "import com.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
    "import com.thoughtworks.deeplearning.Symbolic._\n",
    "import com.thoughtworks.deeplearning.{\n",
    "  DifferentiableHList,\n",
    "  DifferentiableINDArray,\n",
    "  Layer,\n",
    "  Symbolic\n",
    "}\n",
    "import com.thoughtworks.deeplearning.Poly.MathFunctions._\n",
    "import com.thoughtworks.deeplearning.Poly.MathMethods./\n",
    "import com.thoughtworks.deeplearning.Poly.MathOps\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.cpu.nativecpu.NDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
    "import org.nd4j.linalg.ops.transforms.Transforms\n",
    "import org.nd4s.Implicits._\n",
    "import shapeless._\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._\n",
    "\n",
    "import scala.collection.immutable.IndexedSeq\n",
    "import scala.util.Random\n",
    "\n",
    "pprintConfig() = pprintConfig().copy(height = 2)//减少输出的行数，避免页面输出太长\n",
    "\n",
    "import $file.ReadCIFAR10ToNDArray\n",
    "import $file.Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备和处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似[前一节](https://thoughtworksinc.github.io/DeepLearning.scala/demo/SoftmaxLinearClassifier.html)，我们从CIFAR10 database中读取和处理测试数据的图片和对应的标签信息。但是这次我们在这里只读取测试数据即可，训练数据会在训练时随机读取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mNumberOfClasses\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36mtestNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.0\u001b[33m...\u001b[39m\n",
       "\u001b[36mvectorizedTestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CIFAR10中的图片共有10个分类(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
    "val NumberOfClasses: Int = 10\n",
    "\n",
    "//加载测试数据，我们读取100条作为测试数据\n",
    "val testNDArray =\n",
    "   ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/test_batch.bin\", 100)\n",
    "\n",
    "val testData = testNDArray.head\n",
    "\n",
    "val testExpectResult = testNDArray.tail.head\n",
    "\n",
    "val vectorizedTestExpectResult = Utils.makeVectorized(testExpectResult, NumberOfClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟前一节相同，我们需要编写softmax函数，设置学习率和初始化Weight并编写LossFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msoftmax\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36moptimizer\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcreateMyNeuralNetwork\u001b[39m\n",
       "\u001b[36mmyNeuralNetwork\u001b[39m: (\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m[\u001b[32mINDArray\u001b[39m]{type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray;type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray})#\u001b[32m@\u001b[39m = Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum(Exp(Identity()),WrappedArray(1)))),Dot(Identity(),Weight([[-0.00, -0.00, -0.00, 0.00, -0.00, 0.00, 0.00,\u001b[33m...\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlossFunction\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(implicit scores: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val expScores = exp(scores)\n",
    "  expScores / expScores.sum(1)\n",
    "}\n",
    "\n",
    "implicit def optimizer: Optimizer = new LearningRate {\n",
    "  def currentLearningRate() = 0.00001\n",
    "}\n",
    "\n",
    "def createMyNeuralNetwork(implicit input: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val initialValueOfWeight = Nd4j.randn(3072, NumberOfClasses) * 0.001\n",
    "  val weight: INDArray @Symbolic = initialValueOfWeight.toWeight\n",
    "  val result: INDArray @Symbolic = input dot weight\n",
    "  softmax.compose(result)\n",
    "}\n",
    "val myNeuralNetwork = createMyNeuralNetwork\n",
    "\n",
    "def lossFunction(implicit pair: (INDArray :: INDArray :: HNil) @Symbolic): Double @Symbolic = {\n",
    "  val input = pair.head\n",
    "  val expectedOutput = pair.tail.head\n",
    "  val probabilities = myNeuralNetwork.compose(input)\n",
    "\n",
    "  -(expectedOutput * log(probabilities)).mean //此处和准备一节中的交叉熵损失对应\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似前一节我们需要训练神经网络，但是跟上一节不同的是，这次我们的训练数据是随机读取的，上一节是反复训练同一批数据集。训练神经网络并观察每次训练loss的变化，loss的变化趋势是降低，但是不是每次都降低(前途是光明的，道路是曲折的)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据随机数组读取和处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mMiniBatchSize\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m256\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtrainData\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val MiniBatchSize = 256\n",
    "\n",
    "def trainData(randomIndexArray: Array[Int]): Double = {\n",
    "  val trainNDArray :: expectLabel :: shapeless.HNil =\n",
    "    ReadCIFAR10ToNDArray.getSGDTrainNDArray(randomIndexArray)\n",
    "\n",
    "  val input =\n",
    "    trainNDArray.reshape(MiniBatchSize, 3072)\n",
    "\n",
    "  val expectLabelVectorized =\n",
    "    Utils.makeVectorized(expectLabel, NumberOfClasses)\n",
    "\n",
    "  lossFunction.train(input :: expectLabelVectorized :: HNil)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 每个[epoch](http://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks)打乱一次数组,根据随机数组训练神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1 loss is :0.21111876964569093\n",
      "at epoch 2 loss is :0.2084895133972168\n",
      "at epoch 3 loss is :0.19478811025619508\n",
      "at epoch 4 loss is :0.1909475326538086\n",
      "at epoch 5 loss is :0.1919918179512024\n",
      "at epoch 6 loss is :0.18776063919067382\n",
      "at epoch 7 loss is :0.18520112037658693\n",
      "at epoch 8 loss is :0.184558641910553\n",
      "at epoch 9 loss is :0.1930071473121643\n",
      "at epoch 10 loss is :0.19292012453079224\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-870570541\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0],\"y\":[0.22984604835510253,0.22996740341186522,0.22945213317871094,0.22896261215209962,0.22849829196929933,0.2291388988494873,0.22822129726409912,0.22858805656433107,0.2286916732788086,0.22855110168457032,0.22791478633880616,0.2275008201599121,0.22789936065673827,0.22830021381378174,0.22775068283081054,0.22668471336364746,0.22723541259765626,0.22683732509613036,0.22661612033843995,0.22584166526794433,0.22695236206054686,0.22557144165039061,0.22574973106384277,0.2248993396759033,0.2268235206604004,0.22459180355072023,0.22448015213012695,0.22486388683319092,0.22409443855285643,0.22448935508728027,0.22520666122436522,0.22526650428771972,0.22284433841705323,0.2234377384185791,0.22536258697509765,0.22422685623168945,0.22400617599487305,0.22287578582763673,0.2240143299102783,0.22470715045928955,0.2236469030380249,0.22302496433258057,0.2217780590057373,0.22329039573669435,0.22242522239685059,0.2220992088317871,0.2190955638885498,0.22198963165283203,0.2201385974884033,0.22288765907287597,0.22196755409240723,0.22174062728881835,0.22112889289855958,0.21915555000305176,0.22193245887756347,0.220267915725708,0.21961894035339355,0.22140040397644042,0.22121977806091309,0.21978516578674318,0.21845855712890624,0.2196873903274536,0.22219715118408204,0.22130870819091797,0.2187361240386963,0.22035603523254393,0.21860308647155763,0.22043962478637696,0.2191077470779419,0.2213263988494873,0.2175036907196045,0.21948490142822266,0.21996326446533204,0.2196401596069336,0.21907508373260498,0.21949379444122313,0.21865522861480713,0.22001829147338867,0.21651144027709962,0.21728618144989015,0.217995023727417,0.21568515300750732,0.21928021907806397,0.21671757698059083,0.2173450469970703,0.21512422561645508,0.2164764404296875,0.21675333976745606,0.2184380531311035,0.21542630195617676,0.21572494506835938,0.21516518592834472,0.21633734703063964,0.21760332584381104,0.21716008186340333,0.2159132957458496,0.21515088081359862,0.21814827919006347,0.21433348655700685,0.21611478328704833,0.2173065185546875,0.21595537662506104,0.21596064567565917,0.21283822059631347,0.21709084510803223,0.2170029640197754,0.21448283195495604,0.21403417587280274,0.2153010845184326,0.2166149616241455,0.21485090255737305,0.21372673511505128,0.2137585163116455,0.21370747089385986,0.21302170753479005,0.21381759643554688,0.2143834114074707,0.2131227970123291,0.21506862640380858,0.21440680027008058,0.21639676094055177,0.21527605056762694,0.21415891647338867,0.21524276733398437,0.21205813884735109,0.2125035285949707,0.2159644842147827,0.21349656581878662,0.21441514492034913,0.21241230964660646,0.21222639083862305,0.2144141674041748,0.21203837394714356,0.21454956531524658,0.21198587417602538,0.2150090217590332,0.20953032970428467,0.21345021724700927,0.21211671829223633,0.21275513172149657,0.21289174556732177,0.21158227920532227,0.21313247680664063,0.21390531063079835,0.21261029243469237,0.21313233375549318,0.21411516666412353,0.21035616397857665,0.2109528064727783,0.2085162878036499,0.21200356483459473,0.21224408149719237,0.21419458389282225,0.21242163181304932,0.20737795829772948,0.21071805953979492,0.21517550945281982,0.2123197555541992,0.21317930221557618,0.21111876964569093,0.2108987808227539,0.21125645637512208,0.21316232681274414,0.21258325576782228,0.20866546630859376,0.21131281852722167,0.21235592365264894,0.21018538475036622,0.209580135345459,0.20685310363769532,0.20877504348754883,0.21175141334533693,0.20996904373168945,0.21267447471618653,0.20772099494934082,0.20889592170715332,0.2101062297821045,0.21143198013305664,0.21189141273498535,0.20911355018615724,0.20827407836914064,0.20961446762084962,0.21172711849212647,0.21018295288085936,0.21100943088531493,0.2085371732711792,0.20326364040374756,0.20849080085754396,0.21285481452941896,0.20356719493865966,0.20975980758666993,0.20569934844970703,0.20709726810455323,0.21163947582244874,0.20925674438476563,0.21020925045013428,0.20858607292175294,0.20648460388183593,0.20941567420959473,0.2093285083770752,0.2089841365814209,0.2102466344833374,0.20596978664398194,0.20701773166656495,0.20581486225128173,0.21211137771606445,0.20756916999816893,0.20886614322662353,0.20803980827331542,0.20774006843566895,0.2064335823059082,0.21210622787475586,0.20799081325531005,0.20645294189453126,0.21145057678222656,0.2068115234375,0.20724101066589357,0.20838549137115478,0.20941729545593263,0.20672154426574707,0.20724821090698242,0.20479402542114258,0.20754389762878417,0.2102099895477295,0.2055060863494873,0.2069462537765503,0.20643270015716553,0.20609617233276367,0.20831022262573243,0.20408234596252442,0.2048922061920166,0.20790481567382812,0.20495500564575195,0.20977606773376464,0.20956013202667237,0.20619544982910157,0.204052734375,0.20778708457946776,0.20533392429351807,0.20585556030273439,0.20688304901123047,0.209228253364563,0.20351927280426024,0.2030651807785034,0.209328031539917,0.20715157985687255,0.204689621925354,0.20523643493652344,0.20384652614593507,0.206239652633667,0.20544142723083497,0.2113265037536621,0.20408287048339843,0.203084135055542,0.21118147373199464,0.20637612342834472,0.2111668586730957,0.20536270141601562,0.20373048782348632,0.20133619308471679,0.20381288528442382,0.20300543308258057,0.20566072463989257,0.20617275238037108,0.20932726860046386,0.2033789873123169,0.20763678550720216,0.20698223114013672,0.20544276237487794,0.20293788909912108,0.20116822719573973,0.20626416206359863,0.20593676567077637,0.20428864955902098,0.20386078357696533,0.21083059310913085,0.20358631610870362,0.20316667556762696,0.20783958435058594,0.20256381034851073,0.20290143489837648,0.20159704685211183,0.20519757270812988,0.20002281665802002,0.19876320362091066,0.20396690368652343,0.20439457893371582,0.20201616287231444,0.20216352939605714,0.2023510217666626,0.20735716819763184,0.2014781951904297,0.20369195938110352,0.20660104751586914,0.20329127311706544,0.2013883113861084,0.1964721202850342,0.20437216758728027,0.2042319059371948,0.2036745071411133,0.20066218376159667,0.20317890644073486,0.20519893169403075,0.2055877447128296,0.20663838386535643,0.20867176055908204,0.20212805271148682,0.20350451469421388,0.20586848258972168,0.2058941602706909,0.2053922653198242,0.20000200271606444,0.20537931919097902,0.19703059196472167,0.20173795223236085,0.19950757026672364,0.20478625297546388,0.20222604274749756,0.20153515338897704,0.2054499626159668,0.20377719402313232,0.204748272895813,0.20000386238098145,0.20187454223632811,0.20525624752044677,0.20333750247955323,0.20062332153320311,0.20126771926879883,0.20727219581604003,0.2051100730895996,0.2037440299987793,0.20054125785827637,0.1994483709335327,0.1991185426712036,0.2030266761779785,0.19792551994323732,0.205504846572876,0.20864734649658204,0.20266175270080566,0.20539932250976561,0.20322027206420898,0.19974675178527831,0.20363850593566896,0.20127482414245607,0.1986438274383545,0.2038583755493164,0.2017608642578125,0.2023183822631836,0.2017507553100586,0.20741634368896483,0.20340349674224853,0.2060680627822876,0.19968867301940918,0.20275332927703857,0.2084895133972168,0.20612924098968505,0.19797883033752442,0.20328571796417236,0.20247149467468262,0.20583558082580566,0.1989452362060547,0.20352649688720703,0.20307743549346924,0.19995591640472413,0.20190985202789308,0.19885984659194947,0.20348925590515138,0.20255203247070314,0.2038708209991455,0.20155885219573974,0.19610012769699098,0.2006629228591919,0.20143604278564453,0.200209379196167,0.2049072265625,0.20161795616149902,0.2009298324584961,0.19678845405578613,0.1981912612915039,0.20404427051544188,0.20393471717834472,0.19715423583984376,0.19681241512298583,0.20127296447753906,0.1986031174659729,0.20135302543640138,0.20164868831634522,0.20336713790893554,0.20895023345947267,0.20086221694946288,0.19780803918838502,0.20187463760375976,0.20128235816955567,0.19855722188949584,0.20701174736022948,0.1952362060546875,0.19760143756866455,0.19618418216705322,0.20128202438354492,0.19671276807785035,0.2047797203063965,0.19632465839385987,0.19940469264984131,0.19609510898590088,0.1979234218597412,0.20021908283233641,0.20017051696777344,0.1995890498161316,0.19775809049606324,0.20261526107788086,0.20059378147125245,0.19752938747406007,0.1976475238800049,0.2007850170135498,0.19769999980926514,0.2021780490875244,0.198101007938385,0.20191679000854493,0.1996871829032898,0.19965771436691285,0.20340464115142823,0.20121428966522217,0.2005624771118164,0.20395936965942382,0.20160694122314454,0.20236420631408691,0.1979440450668335,0.19982258081436158,0.19504685401916505,0.19660874605178832,0.2016456365585327,0.19823105335235597,0.20479540824890136,0.19655872583389283,0.2056881904602051,0.19149484634399414,0.19988174438476564,0.20414767265319825,0.1988711714744568,0.20164527893066406,0.20080966949462892,0.19411778450012207,0.20154666900634766,0.20193924903869628,0.2023320436477661,0.19658706188201905,0.19850013256072999,0.20072331428527831,0.2017207622528076,0.2029651403427124,0.19437973499298095,0.20060210227966307,0.20310509204864502,0.19975277185440063,0.19811975955963135,0.20573673248291016,0.1977766513824463,0.19555537700653075,0.19884099960327148,0.2005993366241455,0.19819921255111694,0.20225076675415038,0.19836028814315795,0.2017061948776245,0.19823219776153564,0.1965203642845154,0.20399174690246583,0.19708054065704345,0.2024137258529663,0.20338311195373535,0.20494489669799804,0.20335533618927001,0.20255632400512696,0.1997436285018921,0.19453091621398927,0.19929230213165283,0.19982168674468995,0.2000889301300049,0.20224814414978026,0.19590805768966674,0.19563668966293335,0.20129592418670655,0.19312052726745604,0.20501067638397216,0.19714982509613038,0.2011685848236084,0.19915999174118043,0.2030691146850586,0.19663634300231933,0.1953211784362793,0.19295501708984375,0.19918035268783568,0.1993177652359009,0.19688072204589843,0.20095038414001465,0.19925965070724488,0.20196115970611572,0.19754927158355712,0.19532783031463624,0.19990774393081664,0.20149614810943603,0.20077931880950928,0.2003265380859375,0.19491775035858155,0.19536591768264772,0.19994735717773438,0.1991979718208313,0.19389526844024657,0.19761569499969484,0.20185999870300292,0.19409769773483276,0.20101943016052246,0.2039790391921997,0.19658082723617554,0.20034725666046144,0.19609878063201905,0.1975545048713684,0.19598064422607422,0.1976356863975525,0.20519368648529052,0.19676228761672973,0.19349997043609618,0.19138615131378173,0.19534339904785156,0.19727692604064942,0.19744712114334106,0.19677819013595582,0.19951701164245605,0.19913954734802247,0.20317692756652833,0.19640347957611085,0.19541571140289307,0.19995065927505493,0.1968203067779541,0.19808181524276733,0.19442867040634154,0.1906234383583069,0.19403108358383178,0.1894252896308899,0.1930924892425537,0.1914271116256714,0.19782215356826782,0.1992408037185669,0.19870816469192504,0.19759128093719483,0.19819347858428954,0.18793466091156005,0.1961497187614441,0.1976142168045044,0.19478811025619508,0.20152082443237304,0.2001819133758545,0.19852300882339477,0.20166826248168945,0.19109015464782714,0.1958280920982361,0.19858574867248535,0.1928347110748291,0.19954639673233032,0.1993410110473633,0.19733805656433107,0.20297751426696778,0.1952308177947998,0.20197436809539795,0.1963646650314331,0.20239338874816895,0.19649100303649902,0.19199072122573851,0.19874124526977538,0.19233909845352173,0.19863542318344116,0.1958548307418823,0.19718168973922728,0.19591714143753053,0.19375487565994262,0.1952212333679199,0.19617873430252075,0.1930299997329712,0.20057144165039062,0.19610636234283446,0.19662730693817138,0.19612033367156984,0.1980282783508301,0.19912642240524292,0.1937199354171753,0.19831931591033936,0.19916083812713622,0.19565062522888182,0.20026907920837403,0.1921423316001892,0.19452664852142335,0.1936720609664917,0.20002307891845703,0.19539165496826172,0.19922688007354736,0.19454914331436157,0.1942416548728943,0.2004533290863037,0.1977224349975586,0.20055689811706542,0.1968257784843445,0.19523496627807618,0.1977594017982483,0.19108670949935913,0.19519778490066528,0.19528898000717163,0.19461067914962768,0.19433777332305907,0.19787969589233398,0.19652912616729737,0.19770987033843995,0.19707096815109254,0.1933786988258362,0.19199578762054442,0.19928518533706666,0.19494717121124266,0.19364985227584838,0.19809755086898803,0.19187546968460084,0.1940296173095703,0.20010752677917482,0.19598344564437867,0.1934154987335205,0.1970335364341736,0.19377799034118653,0.19530634880065917,0.19590216875076294,0.19033800363540648,0.20209207534790039,0.2015545606613159,0.1989891290664673,0.19918826818466187,0.1923568367958069,0.19623411893844606,0.19369256496429443,0.20175209045410156,0.1907264232635498,0.1972102403640747,0.19275554418563842,0.19711179733276368,0.20019421577453614,0.18549642562866211,0.19408559799194336,0.19277372360229492,0.19752063751220703,0.19745774269104005,0.18765226602554322,0.1938326358795166,0.19111627340316772,0.19948508739471435,0.2010882616043091,0.1992809295654297,0.19885145425796508,0.19806790351867676,0.19470282793045043,0.19628007411956788,0.20194032192230224,0.19636499881744385,0.19887442588806153,0.1985710382461548,0.1939177393913269,0.2017338752746582,0.19091211557388305,0.19826679229736327,0.20025303363800048,0.19595627784729003,0.20001437664031982,0.1888383150100708,0.19623929262161255,0.20084018707275392,0.19456689357757567,0.1928442358970642,0.19403916597366333,0.19930827617645264,0.19834779500961303,0.19526586532592774,0.1960408329963684,0.19493191242218016,0.20535659790039062,0.1959634780883789,0.1971975803375244,0.20172808170318604,0.1912368893623352,0.19463642835617065,0.19414664506912233,0.1936873435974121,0.19583609104156494,0.19885631799697875,0.19492274522781372,0.1877842664718628,0.1981262445449829,0.19387543201446533,0.19229867458343505,0.1936369776725769,0.19650843143463134,0.1945856809616089,0.19441139698028564,0.1947169303894043,0.19274145364761353,0.19439785480499266,0.1949878454208374,0.19569991827011107,0.20097758769989013,0.1918647527694702,0.20042173862457274,0.20082392692565917,0.19432998895645143,0.18559107780456544,0.1967526078224182,0.2035978317260742,0.19760938882827758,0.19560747146606444,0.18999706506729125,0.2018118381500244,0.19857726097106934,0.19387369155883788,0.19424269199371338,0.20174932479858398,0.18978086709976197,0.19160451889038085,0.19287505149841308,0.19046555757522582,0.19331010580062866,0.1955294728279114,0.18805782794952391,0.1929415464401245,0.1972139835357666,0.1941206693649292,0.2012721061706543,0.19981842041015624,0.191742467880249,0.1932956576347351,0.19763200283050536,0.1945709228515625,0.1987176775932312,0.19444693326950074,0.19369959831237793,0.1906180739402771,0.1984145760536194,0.1902127146720886,0.18883821964263917,0.1926912784576416,0.194411301612854,0.19229251146316528,0.1909475326538086,0.19719462394714354,0.19505459070205688,0.1990295648574829,0.1955322504043579,0.19144419431686402,0.19204995632171631,0.1918615221977234,0.19426288604736328,0.1992244839668274,0.1919943571090698,0.19578272104263306,0.19056482315063478,0.19565752744674683,0.19384161233901978,0.1922311782836914,0.19859931468963624,0.1954679012298584,0.19552228450775147,0.19882192611694335,0.1952143907546997,0.19351892471313475,0.19882363080978394,0.19957658052444457,0.1903603792190552,0.19126346111297607,0.18840463161468507,0.1952211618423462,0.1954587459564209,0.1990411639213562,0.19828460216522217,0.19553682804107667,0.19310302734375,0.19730839729309083,0.19855700731277465,0.18990025520324708,0.19422229528427123,0.19342336654663086,0.1957780361175537,0.1940567374229431,0.19423036575317382,0.19247834682464598,0.19565601348876954,0.19716248512268067,0.19153940677642822,0.19969369173049928,0.19643125534057618,0.1903705358505249,0.1965196967124939,0.19853050708770753,0.18464767932891846,0.20226426124572755,0.1955925464630127,0.19884626865386962,0.19372360706329345,0.20078153610229493,0.19469352960586547,0.19021027088165282,0.1926356554031372,0.18817516565322875,0.1990481734275818,0.2002185583114624,0.19147729873657227,0.19645884037017822,0.1957300305366516,0.19481958150863649,0.1873887896537781,0.20947628021240233,0.18792259693145752,0.19973535537719728,0.1898127555847168,0.19564054012298585,0.19365274906158447,0.18578503131866456,0.19016988277435304,0.19241660833358765,0.19333552122116088,0.19089727401733397,0.1937979578971863,0.19640777111053467,0.1916203022003174,0.19174000024795532,0.1897743821144104,0.19843509197235107,0.18938487768173218,0.19171230792999266,0.1960212230682373,0.19704257249832152,0.18772732019424437,0.19022301435470582,0.20222892761230468,0.19945508241653442,0.18654143810272217,0.19560487270355226,0.18680764436721803,0.1941443920135498,0.18999605178833007,0.19412928819656372,0.19757299423217772,0.19298843145370484,0.18323235511779784,0.18914599418640138,0.18546632528305054,0.1875972032546997,0.19465816020965576,0.1968577027320862,0.18848248720169067,0.19083389043807983,0.18923479318618774,0.1855339527130127,0.19090478420257567,0.19289053678512574,0.19636285305023193,0.19649865627288818,0.19100675582885743,0.19162697792053224,0.19250061511993408,0.196230685710907,0.19049923419952391,0.19296376705169677,0.19150159358978272,0.19700759649276733,0.19928376674652098,0.1909721851348877,0.19482542276382447,0.19240424633026124,0.18888869285583496,0.1899104595184326,0.19898070096969606,0.18868610858917237,0.1946008324623108,0.1998847484588623,0.19059962034225464,0.19301873445510864,0.1914036750793457,0.19964940547943116,0.19230552911758422,0.1971902370452881,0.19456945657730101,0.19215949773788452,0.1931810736656189,0.19596421718597412,0.18672292232513427,0.1918849229812622,0.18893871307373047,0.19277799129486084,0.1902287244796753,0.18714824914932252,0.18866242170333863,0.19461357593536377,0.19756978750228882,0.18506720066070556,0.19211859703063966,0.18928184509277343,0.19805762767791749,0.19166979789733887,0.19687317609786986,0.19009913206100465,0.19571343660354615,0.18474637269973754,0.1998945951461792,0.19639097452163695,0.18904995918273926,0.19802881479263307,0.18679933547973632,0.1995127558708191,0.19225925207138062,0.18786722421646118,0.19236137866973876,0.19479509592056274,0.19977185726165772,0.19276512861251832,0.18730920553207397,0.19218616485595702,0.18985388278961182,0.1924959421157837,0.19710443019866944,0.18979305028915405,0.19557176828384398,0.18479397296905517,0.18909661769866942,0.18662669658660888,0.19217920303344727,0.19724638462066652,0.19390466213226318,0.19557298421859742,0.19322376251220702,0.20118930339813232,0.18834458589553832,0.19463366270065308,0.18673512935638428,0.18841660022735596,0.19254714250564575,0.19084954261779785,0.18641765117645265,0.1919918179512024,0.18739144802093505,0.19639980792999268,0.19893350601196289,0.19226030111312867,0.18953384160995485,0.18204035758972167,0.20312941074371338,0.18909118175506592,0.19459350109100343,0.19637227058410645,0.18834503889083862,0.20183582305908204,0.19608874320983888,0.1907211422920227,0.19019935131072999,0.19596971273422242,0.18053510189056396,0.19764699935913085,0.19218299388885499,0.19704315662384034,0.19717215299606322,0.19258875846862794,0.19257400035858155,0.19409910440444947,0.18228329420089723,0.1882117509841919,0.18922363519668578,0.1937534213066101,0.19246129989624022,0.1950096845626831,0.18990447521209716,0.19947516918182373,0.18808965682983397,0.18949379920959472,0.18654448986053468,0.19281549453735353,0.1950850248336792,0.18792567253112794,0.1963404417037964,0.19384684562683105,0.19012351036071778,0.19181153774261475,0.19457471370697021,0.19412540197372435,0.18744230270385742,0.19071332216262818,0.18966896533966066,0.18524502515792846,0.1980930209159851,0.19041429758071898,0.1928833842277527,0.18729133605957032,0.19372286796569824,0.19781312942504883,0.1902374029159546,0.18679461479187012,0.1935397505760193,0.19445582628250122,0.18719995021820068,0.19396841526031494,0.18522881269454955,0.19458370208740233,0.19545973539352418,0.19303491115570068,0.18658018112182617,0.19389379024505615,0.18984400033950805,0.18292484283447266,0.19376165866851808,0.19025819301605223,0.19364240169525146,0.19413046836853026,0.19820184707641603,0.18666001558303832,0.19335864782333373,0.1817386269569397,0.19007017612457275,0.19413151741027831,0.19192752838134766,0.18372132778167724,0.18935340642929077,0.19343005418777465,0.19436565637588502,0.1935112237930298,0.1911391258239746,0.19529469013214112,0.18799587488174438,0.19036132097244263,0.20040724277496338,0.19047834873199462,0.19028152227401735,0.19538695812225343,0.19231468439102173,0.19055277109146118,0.19438728094100952,0.19180283546447754,0.19207525253295898,0.1936332941055298,0.19376380443573,0.19654794931411743,0.19119251966476442,0.1831316113471985,0.19053062200546264,0.19229847192764282,0.1868906259536743,0.19319818019866944,0.1933234453201294,0.1970285415649414,0.19229693412780763,0.1881244421005249,0.19038877487182618,0.18883421421051025,0.18245552778244017,0.18777741193771363,0.18700318336486815,0.19191765785217285,0.1975497007369995,0.190989351272583,0.19204238653182984,0.1889325737953186,0.19605956077575684,0.18512444496154784,0.1936097502708435,0.19663416147232055,0.1912667989730835,0.19250288009643554,0.1902157783508301,0.19122849702835082,0.18563246726989746,0.192874014377594,0.197118878364563,0.1922908306121826,0.1896466851234436,0.18855429887771608,0.18794634342193603,0.17950155735015869,0.189703369140625,0.19159609079360962,0.19368326663970947,0.19182655811309815,0.19634933471679689,0.19317781925201416,0.19726020097732544,0.18817250728607177,0.19423974752426149,0.1916325330734253,0.19104034900665284,0.18754618167877196,0.18753221035003662,0.19635688066482543,0.18610639572143556,0.19510494470596312,0.1878008484840393,0.19288485050201415,0.18834919929504396,0.19645745754241944,0.19144694805145263,0.19317190647125243,0.19591014385223388,0.18999552726745605,0.18313472270965575,0.18585194349288942,0.18543261289596558,0.19173845052719116,0.19532346725463867,0.19364545345306397,0.1903160572052002,0.19543211460113524,0.1905573010444641,0.20365984439849855,0.18843562602996827,0.18419904708862306,0.19279444217681885,0.18751035928726195,0.19451351165771485,0.1916070818901062,0.19132165908813475,0.1891101360321045,0.18890533447265626,0.19681134223937988,0.19359537363052368,0.18935859203338623,0.18323230743408203,0.1927149176597595,0.18400635719299316,0.1900748610496521,0.18758087158203124,0.18362441062927246,0.19379333257675171,0.18502271175384521,0.19740015268325806,0.18686118125915527,0.18661723136901856,0.18294910192489625,0.18776063919067382,0.1837073564529419,0.18259053230285643,0.1931973934173584,0.19377197027206422,0.19642715454101561,0.18770962953567505,0.1914955496788025,0.18736861944198607,0.19823747873306274,0.18542788028717042,0.1899903416633606,0.19021878242492676,0.19410842657089233,0.19859029054641725,0.18824398517608643,0.18484771251678467,0.19258592128753663,0.19641853570938111,0.19218039512634277,0.19152388572692872,0.19411392211914064,0.1841724157333374,0.18787579536437987,0.19092458486557007,0.19731802940368653,0.19418692588806152,0.18768811225891113,0.1871967315673828,0.18758105039596557,0.1930469274520874,0.19099760055541992,0.19551931619644164,0.19225459098815917,0.18188204765319824,0.18643662929534913,0.18815920352935792,0.19713547229766845,0.19208205938339235,0.19151451587677001,0.19379990100860595,0.19194450378417968,0.19158544540405273,0.18894593715667723,0.18944412469863892,0.18633246421813965,0.19473023414611818,0.19454381465911866,0.18960683345794677,0.18477635383605956,0.1885819435119629,0.1942565083503723,0.1886815309524536,0.19175212383270263,0.19253685474395751,0.1852182626724243,0.18443663120269777,0.18825132846832277,0.18933809995651246,0.18680399656295776,0.19366509914398194,0.186561918258667,0.188466477394104,0.18794665336608887,0.18753407001495362,0.18591560125350953,0.19174360036849974,0.1863224744796753,0.1893297791481018,0.1920480966567993,0.1965517520904541,0.19486550092697144,0.19179706573486327,0.19155642986297608,0.18591699600219727,0.19218424558639527,0.18786382675170898,0.182414448261261,0.19306758642196656,0.18768734931945802,0.18013497591018676,0.18474740982055665,0.19861066341400146,0.1930415153503418,0.19138524532318116,0.18684513568878175,0.18981313705444336,0.19423187971115113,0.1898800849914551,0.1879109263420105,0.18783109188079833,0.18710176944732665,0.190321946144104,0.1833072304725647,0.18548003435134888,0.19198765754699706,0.1938326358795166,0.1791488766670227,0.18255198001861572,0.1915497899055481,0.19119222164154054,0.19207212924957276,0.18522024154663086,0.19413024187088013,0.19067668914794922,0.19433436393737794,0.19325656890869142,0.19448858499526978,0.189901602268219,0.1857212543487549,0.19158685207366943,0.18450028896331788,0.1890384316444397,0.1821619153022766,0.19537390470504762,0.1906030535697937,0.1931592583656311,0.19285595417022705,0.19040794372558595,0.18620624542236328,0.1867824077606201,0.19537342786788942,0.19364969730377196,0.19391361474990845,0.18859002590179444,0.18960196971893312,0.18573737144470215,0.18833930492401124,0.1799934983253479,0.19450960159301758,0.19069263935089112,0.1827777862548828,0.18257081508636475,0.18169775009155273,0.1811532497406006,0.18264129161834716,0.20175790786743164,0.18432111740112306,0.1917871356010437,0.18693772554397584,0.1943744421005249,0.18861908912658693,0.18877673149108887,0.18645994663238524,0.1911869764328003,0.19107484817504883,0.18088918924331665,0.1898789405822754,0.18506414890289308,0.18723301887512206,0.19068126678466796,0.1984267234802246,0.1884225845336914,0.18556768894195558,0.1845053553581238,0.18920694589614867,0.187201988697052,0.1893226146697998,0.1869513511657715,0.18425278663635253,0.19547958374023439,0.1913096070289612,0.1845906138420105,0.18637218475341796,0.18054100275039672,0.19039729833602906,0.1867274522781372,0.18909497261047364,0.19393736124038696,0.18420668840408325,0.1873214602470398,0.19142163991928102,0.18385103940963746,0.19504034519195557,0.19450360536575317,0.18475403785705566,0.18635592460632325,0.19331787824630736,0.19012167453765869,0.19988046884536742,0.18352692127227782,0.18680799007415771,0.18660848140716552,0.1899866580963135,0.1828757643699646,0.1921946406364441,0.18960394859313964,0.18587746620178222,0.1882750868797302,0.18226748704910278,0.18876688480377196,0.19252331256866456,0.19236288070678711,0.1847623586654663,0.18853567838668822,0.18520112037658693,0.1895401120185852,0.19014225006103516,0.19900163412094116,0.18303585052490234,0.18497169017791748,0.19506235122680665,0.19339786767959594,0.18366326093673707,0.1914857029914856,0.1905410885810852,0.19009876251220703,0.18836159706115724,0.18486593961715697,0.18677619695663453,0.19421591758728027,0.18842647075653077,0.19637641906738282,0.1897526979446411,0.18424352407455444,0.1942634701728821,0.18698803186416627,0.1869917631149292,0.18234187364578247,0.18448731899261475,0.1927588701248169,0.19404044151306152,0.1879369616508484,0.18512613773345948,0.1902850866317749,0.18018263578414917,0.19061930179595948,0.17929511070251464,0.19404038190841674,0.19049057960510254,0.19267987012863158,0.18863929510116578,0.18877612352371215,0.18431928157806396,0.18806432485580443,0.19174724817276,0.19011218547821046,0.18687297105789186,0.18409823179244994,0.18316905498504638,0.18642067909240723,0.19308929443359374,0.18870184421539307,0.18820106983184814,0.186165452003479,0.19077708721160888,0.19113070964813234,0.1948476791381836,0.17894562482833862,0.18940850496292114,0.18482345342636108,0.1864227294921875,0.19473299980163575,0.1855595588684082,0.1879499912261963,0.1830121874809265,0.19187958240509034,0.19078123569488525,0.1845261812210083,0.1902720808982849,0.18863970041275024,0.18450595140457154,0.19765074253082277,0.19258288145065308,0.19225306510925294,0.1949075937271118,0.1901394844055176,0.1831073522567749,0.19280945062637328,0.1858429193496704,0.1870260238647461,0.18630918264389038,0.18858503103256224,0.18594731092453004,0.18513884544372558,0.18869407176971437,0.18817217350006105,0.19659152030944824,0.19042006731033326,0.19100915193557738,0.1910191297531128,0.1930492639541626,0.18562614917755127,0.18499524593353273,0.1920359492301941,0.18398436307907104,0.185830020904541,0.189927077293396,0.18839195966720582,0.1836012125015259,0.19566428661346436,0.19302889108657836,0.18772194385528565,0.19179190397262574,0.19264482259750365,0.18601233959198,0.18544917106628417,0.18836636543273927,0.20012245178222657,0.18501466512680054,0.18365030288696288,0.18636007308959962,0.1905294418334961,0.1986892342567444,0.1891230821609497,0.1880742907524109,0.1872799038887024,0.19606398344039916,0.1873963475227356,0.19042048454284669,0.18720353841781617,0.18753225803375245,0.19262974262237548,0.1928707480430603,0.18609635829925536,0.18333630561828612,0.18926795721054077,0.194488787651062,0.18576476573944092,0.17814233303070068,0.18107439279556276,0.18716057538986205,0.18780165910720825,0.19414396286010743,0.18365099430084228,0.19594919681549072,0.18211696147918702,0.18526595830917358,0.19019992351531984,0.1854097366333008,0.18270256519317626,0.1798357605934143,0.19600703716278076,0.18729557991027831,0.1860622525215149,0.19236143827438354,0.1935972213745117,0.18851819038391113,0.1852697968482971,0.18470206260681152,0.18380792140960694,0.1869446039199829,0.188630211353302,0.19538336992263794,0.18932952880859374,0.19086236953735353,0.18030699491500854,0.18934295177459717,0.19394975900650024,0.19133676290512086,0.18512762784957887,0.18494827747344972,0.1879361867904663,0.19222842454910277,0.19317113161087035,0.18768706321716308,0.19445477724075316,0.18224704265594482,0.19027799367904663,0.19082887172698976,0.18369368314743043,0.18761738538742065,0.19370137453079223,0.18080708980560303,0.18712704181671141,0.18684881925582886,0.19145772457122803,0.19331514835357666,0.18855830430984497,0.19030492305755614,0.1914939761161804,0.18123189210891724,0.1855158805847168,0.19149339199066162,0.18866174221038817,0.18369473218917848,0.18728129863739013,0.17669297456741334,0.1846709132194519,0.19190678596496583,0.19719581604003905,0.1900642156600952,0.19202835559844972,0.18985309600830078,0.1852359414100647,0.195123028755188,0.184264612197876,0.19681978225708008,0.19313447475433348,0.1832745313644409,0.184558641910553,0.19112017154693603,0.19396445751190186,0.18558802604675292,0.19068384170532227,0.18716365098953247,0.19501835107803345,0.1830403208732605,0.187231707572937,0.18567979335784912,0.19272029399871826,0.187935209274292,0.18256359100341796,0.19365493059158326,0.18558509349823,0.19215928316116332,0.18801982402801515,0.18999992609024047,0.18790746927261354,0.18741002082824706,0.18575336933135986,0.1865241527557373,0.17987384796142578,0.1879828929901123,0.19431209564208984,0.19524108171463012,0.18878309726715087,0.18998985290527343,0.19304463863372803,0.19215836524963378,0.18802337646484374,0.1874215006828308,0.18427786827087403,0.1756659984588623,0.17966548204421998,0.18557958602905272,0.19332412481307984,0.1903565287590027,0.1882538914680481,0.18451768159866333,0.18402618169784546,0.1834176301956177,0.18544137477874756,0.18317019939422607,0.18125025033950806,0.1828540086746216,0.18658922910690307,0.18418192863464355,0.19021953344345094,0.1892424702644348,0.17868101596832275,0.19368839263916016,0.19235132932662963,0.18146036863327025,0.1879570484161377,0.18129700422286987,0.1861480951309204,0.19280650615692138,0.18340914249420165,0.19324467182159424,0.19067548513412474,0.18746767044067383,0.18369061946868898,0.1874905824661255,0.19388213157653808,0.18661009073257445,0.19430117607116698,0.19018962383270263,0.18161396980285643,0.17977076768875122,0.1812804937362671,0.19384543895721434,0.1921877980232239,0.18966598510742189,0.18823281526565552,0.18370475769042968,0.18205819129943848,0.1918390393257141,0.1865288257598877,0.1954951047897339,0.19442439079284668,0.17849299907684327,0.1873142123222351,0.18575490713119508,0.18757326602935792,0.19335771799087526,0.18877347707748413,0.18608392477035524,0.1868058681488037,0.18612295389175415,0.1881049394607544,0.1782346248626709,0.1906699061393738,0.18581552505493165,0.18231213092803955,0.18250023126602172,0.18343472480773926,0.18654003143310546,0.19445101022720337,0.18850321769714357,0.19240162372589112,0.17953916788101196,0.1855115532875061,0.18215150833129884,0.19136030673980714,0.18217726945877075,0.18579561710357667,0.19943901300430297,0.18308969736099243,0.18131303787231445,0.18433668613433837,0.18577961921691893,0.1889827013015747,0.1840275764465332,0.18991281986236572,0.18325971364974974,0.18942657709121705,0.1830137014389038,0.19263534545898436,0.18586409091949463,0.18473572731018068,0.18278921842575074,0.1861482858657837,0.18651304244995118,0.19180408716201783,0.1901067852973938,0.18373262882232666,0.1849126935005188,0.18200466632843018,0.1960005283355713,0.19580459594726562,0.19159610271453859,0.19615092277526855,0.18697056770324708,0.18708635568618776,0.18598209619522094,0.1881560802459717,0.18331215381622315,0.18845359086990357,0.193011474609375,0.18507741689682006,0.1843369722366333,0.18728338479995726,0.18711209297180176,0.20522608757019042,0.1934446096420288,0.1917425274848938,0.19041121006011963,0.1861335039138794,0.18240094184875488,0.19081729650497437,0.18189938068389894,0.18286830186843872,0.1855867624282837,0.18821363449096679,0.1846580386161804,0.1858716607093811,0.18845043182373047,0.18203617334365846,0.18978667259216309,0.1839083433151245,0.1943458080291748,0.18583269119262696,0.18415942192077636,0.18848586082458496,0.18532376289367675,0.1814342260360718,0.19509221315383912,0.18655495643615722,0.1810091257095337,0.1924428105354309,0.18155450820922853,0.18183757066726686,0.17996209859848022,0.18274080753326416,0.1952521562576294,0.19276118278503418,0.18696317672729493,0.1917797327041626,0.18616366386413574,0.18579014539718627,0.19520989656448365,0.18907452821731568,0.18943463563919066,0.18396220207214356,0.19555463790893554,0.18541163206100464,0.1851220726966858,0.1795665740966797,0.18108701705932617,0.19338090419769288,0.1939866065979004,0.18715100288391112,0.18811697959899903,0.18899028301239013,0.1930071473121643,0.19409348964691162,0.18571572303771972,0.18361635208129884,0.18558404445648194,0.1826937437057495,0.1870497941970825,0.1932762384414673,0.18330152034759523,0.1890457510948181,0.18343186378479004,0.18639196157455445,0.18488994836807252,0.1882631778717041,0.18435291051864625,0.18293516635894774,0.1889381766319275,0.19559128284454347,0.18794237375259398,0.1937318205833435,0.18379204273223876,0.18803079128265382,0.18730616569519043,0.18812013864517213,0.18216536045074463,0.18250133991241455,0.19002507925033568,0.1861724853515625,0.18547446727752687,0.1933843731880188,0.1824297070503235,0.17857506275177001,0.18142132759094237,0.1896812677383423,0.190435528755188,0.1895221948623657,0.19374527931213378,0.1855924606323242,0.1894310712814331,0.19177292585372924,0.18231842517852784,0.1839194655418396,0.18363590240478517,0.18500733375549316,0.18517687320709228,0.1949727177619934,0.1883235454559326,0.18537267446517944,0.17417581081390382,0.18380041122436525,0.18496670722961425,0.1812988519668579,0.19014146327972412,0.19221004247665405,0.18336813449859618,0.18377282619476318,0.18073720932006837,0.18759915828704835,0.19187813997268677,0.1813110113143921,0.18472154140472413,0.18288761377334595,0.1853528618812561,0.18462753295898438,0.19445680379867553,0.1872260808944702,0.18143032789230346,0.18141815662384034,0.18123148679733275,0.18359754085540772,0.19237928390502929,0.18616771697998047,0.1843359112739563,0.19021937847137452,0.18701976537704468,0.1834941864013672,0.18167437314987184,0.1757589101791382,0.18092808723449708,0.1935839533805847,0.18828506469726564,0.19248664379119873,0.18876935243606568,0.1858041524887085,0.19128179550170898,0.18472189903259278,0.18228312730789184,0.18733384609222412,0.19056845903396608,0.18159470558166504,0.19053449630737304,0.1873150110244751,0.1832177996635437,0.18249180316925048,0.18463070392608644,0.18557902574539184,0.18401211500167847,0.18859312534332276,0.19380816221237182,0.18648433685302734,0.1854872465133667,0.18807243108749389,0.1872919797897339,0.186826491355896,0.19082000255584716,0.1829046845436096,0.18986914157867432,0.18151454925537108,0.18082261085510254,0.18618487119674682,0.18378087282180786,0.1838637113571167,0.1937313675880432,0.1810774564743042,0.18685804605484008,0.18863471746444702,0.18870667219161988,0.18614779710769652,0.19213281869888305,0.1864213228225708,0.18356430530548096,0.18454025983810424,0.18643226623535156,0.18634332418441774,0.18618737459182738,0.17580931186676024,0.18164573907852172,0.18623559474945067,0.1792404532432556,0.18801573514938355,0.18435933589935302,0.18298060894012452,0.1896735906600952,0.19121382236480713,0.177528977394104,0.18435266017913818,0.18300106525421142,0.1852729558944702,0.18738932609558107,0.18659147024154663,0.18225858211517335,0.18701599836349486,0.18358949422836304,0.1914395809173584,0.18608229160308837,0.18828291893005372,0.18457272052764892,0.19361836910247804,0.17661409378051757,0.183302903175354,0.19073882102966308,0.18798670768737794,0.18991769552230836,0.17962812185287474,0.18710520267486572,0.19326250553131102,0.1946885585784912,0.18806639909744263,0.1917806625366211,0.19515647888183593,0.18246562480926515,0.18423802852630616,0.19140496253967285,0.17715837955474853,0.18524001836776732,0.1817346453666687,0.18885571956634523,0.188670015335083,0.19384559392929077,0.1841359853744507,0.17774674892425538,0.1902909278869629,0.18568036556243897,0.18134336471557616,0.18663265705108642,0.1808338761329651,0.1810204029083252,0.1900464653968811,0.1941893219947815,0.1836084842681885,0.181755530834198,0.18570196628570557,0.18611156940460205,0.18259952068328858,0.186933696269989,0.18900989294052123,0.19652737379074098,0.18074052333831786,0.18078657388687133,0.183644700050354,0.18689547777175902,0.17733802795410156,0.18721742630004884,0.18661987781524658,0.1843034267425537,0.19292012453079224,0.18015697002410888,0.1839251160621643,0.17978219985961913,0.18242748975753784,0.18667649030685424,0.18897337913513185,0.18735685348510742,0.18346662521362306,0.18769683837890624,0.1891692042350769,0.18769311904907227,0.18187671899795532,0.1861751675605774,0.18085792064666747,0.18320531845092775,0.17942345142364502,0.18414535522460937,0.18538159132003784,0.18513723611831664,0.1851803183555603,0.1880430221557617,0.18932507038116456,0.17707659006118776,0.18404862880706788,0.18077330589294432,0.1873491406440735,0.1854128360748291,0.18113142251968384,0.18306994438171387,0.18195966482162476,0.1869178295135498,0.1857814073562622,0.19411628246307372,0.18869351148605346,0.18440821170806884,0.18650107383728026,0.18443024158477783,0.18875004053115846,0.1814176082611084,0.1954406976699829,0.17672173976898192,0.17952677011489868,0.18921729326248168,0.17606695890426635,0.18902733325958251,0.2001786231994629,0.18805618286132814,0.20301599502563478,0.18568787574768067,0.18813717365264893,0.19323015213012695,0.19219181537628174,0.18982939720153807,0.18239905834197997,0.18639765977859496,0.18861212730407714,0.19226452112197875,0.18460003137588502,0.1870213508605957,0.1853504180908203,0.17815924882888795,0.17713501453399658,0.18743343353271485,0.1835721492767334,0.18210822343826294,0.1766057252883911,0.18815066814422607,0.18761755228042604,0.18660290241241456,0.18791481256484985,0.1846662163734436,0.18149741888046264,0.18321985006332397,0.17880430221557617]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss by time\"};\n",
       "\n",
       "  Plotly.plot('plot-870570541', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrandom\u001b[39m: \u001b[32mRandom\u001b[39m = scala.util.Random@6f3e5371\n",
       "\u001b[36mlossSeq\u001b[39m: \u001b[32mIndexedSeq\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m0.22984604835510253\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mplot\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mScatter\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mScatter\u001b[39m(\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mres4_3\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-870570541\"\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val random = new Random\n",
    "\n",
    "val lossSeq =\n",
    "  (\n",
    "    for (iteration <- 0 to 50) yield {\n",
    "      val randomIndex = random\n",
    "        .shuffle[Int, IndexedSeq](0 until 10000) //https://issues.scala-lang.org/browse/SI-6948\n",
    "        .toArray\n",
    "      for (times <- 0 until 10000 / MiniBatchSize) yield {\n",
    "        val randomIndexArray =\n",
    "          randomIndex.slice(times * MiniBatchSize,\n",
    "                            (times + 1) * MiniBatchSize)\n",
    "          val loss = trainData(randomIndexArray)\n",
    "          if(times == 3 & iteration % 5 == 4){\n",
    "            println(\"at epoch \" + (iteration / 5 + 1) + \" loss is :\" + loss)\n",
    "          }\n",
    "          loss\n",
    "      }\n",
    "    }\n",
    "  ).flatten\n",
    "\n",
    "val plot = Seq(\n",
    "  Scatter(lossSeq.indices, lossSeq)\n",
    ")\n",
    "\n",
    "plot.plot(\n",
    "  title = \"loss by time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟上一节相同，我们使用测试数据来查看神经网络判断结果并计算准确率。这次准确率应该会有所上升，最终结果在40%左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result is 38.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mright\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m38.0\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val right = Utils.getAccuracy(myNeuralNetwork.predict(testData), testExpectResult)\n",
    "println(s\"the result is $right %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "在这节中我们学到了：\n",
    "\n",
    "* Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[完整代码](https://github.com/izhangzhihao/deeplearning-tutorial/blob/master/src/main/scala/com/thoughtworks/deeplearning/tutorial/MiniBatchGradientDescent.scala)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
