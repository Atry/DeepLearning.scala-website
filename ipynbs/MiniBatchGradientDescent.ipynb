{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景\n",
    "\n",
    "在大规模数据训练时，数据可以达到百万级量级。如果计算整个训练集，来获得仅仅一个参数的更新速度就太慢了。一个常用的方法是计算训练集中的小批量min-batche）数据随机梯度下降快速实现神经网络参数更新。这节我们将通过使用[Mini-Batch Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) 来实现小批量数据随机梯度下降快速更新网络参数，这样神经网络的准确率可以达到40%。\n",
    "\n",
    "参考：\n",
    "\n",
    "[Mini-Batch Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent): 在大规模数据训练时，数据可以达到百万级量级。如果计算整个训练集，来获得仅仅一个参数的更新速度就太慢了。一个常用的方法是计算训练集中的小批量（batches）数据以提升参数更新速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling ReadCIFAR10ToNDArray.sc\n",
      "Compiling Utils.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                 \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.{FileInputStream, InputStream}\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableHList._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableDouble._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableAny._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Layer.Batch\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.{\n",
       "  DifferentiableHList,\n",
       "  DifferentiableINDArray,\n",
       "  Layer,\n",
       "  Symbolic\n",
       "}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathFunctions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathMethods./\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathOps\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.cpu.nativecpu.NDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.ops.transforms.Transforms\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4s.Implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mshapeless._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.immutable.IndexedSeq\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$    \u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $plugin.$ivy.`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:2.0.0`\n",
    "\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableany:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablenothing:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableseq:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiabledouble:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablefloat:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablehlist:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableindarray:1.0.0-RC7`\n",
    "import $ivy.`org.rauschig:jarchivelib:0.5.0`\n",
    "\n",
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.0`\n",
    "\n",
    "import java.io.{FileInputStream, InputStream}\n",
    "\n",
    "\n",
    "import com.thoughtworks.deeplearning\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import com.thoughtworks.deeplearning.DifferentiableHList._\n",
    "import com.thoughtworks.deeplearning.DifferentiableDouble._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray._\n",
    "import com.thoughtworks.deeplearning.DifferentiableAny._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
    "import com.thoughtworks.deeplearning.Layer.Batch\n",
    "import com.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
    "import com.thoughtworks.deeplearning.Symbolic._\n",
    "import com.thoughtworks.deeplearning.{\n",
    "  DifferentiableHList,\n",
    "  DifferentiableINDArray,\n",
    "  Layer,\n",
    "  Symbolic\n",
    "}\n",
    "import com.thoughtworks.deeplearning.Poly.MathFunctions._\n",
    "import com.thoughtworks.deeplearning.Poly.MathMethods./\n",
    "import com.thoughtworks.deeplearning.Poly.MathOps\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.cpu.nativecpu.NDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
    "import org.nd4j.linalg.ops.transforms.Transforms\n",
    "import org.nd4s.Implicits._\n",
    "import shapeless._\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._\n",
    "\n",
    "import scala.collection.immutable.IndexedSeq\n",
    "import scala.util.Random\n",
    "\n",
    "pprintConfig() = pprintConfig().copy(height = 2)//减少输出的行数，避免页面输出太长\n",
    "\n",
    "import $file.ReadCIFAR10ToNDArray\n",
    "import $file.Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似[前一节](https://thoughtworksinc.github.io/DeepLearning.scala/demo/SoftmaxLinearClassifier.html)，我们从CIFAR10 database中读取和处理测试数据的图片和对应的标签信息。但是这次我们在这里只读取测试数据即可，训练数据会在训练时随机读取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mNumberOfClasses\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36mtestNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.0\u001b[33m...\u001b[39m\n",
       "\u001b[36mvectorizedTestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CIFAR10中的图片共有10个分类(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
    "val NumberOfClasses: Int = 10\n",
    "\n",
    "//加载测试数据，我们读取100条作为测试数据\n",
    "val testNDArray =\n",
    "   ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/test_batch.bin\", 100)\n",
    "\n",
    "val testData = testNDArray.head\n",
    "\n",
    "val testExpectResult = testNDArray.tail.head\n",
    "\n",
    "val vectorizedTestExpectResult = Utils.makeVectorized(testExpectResult, NumberOfClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟前一节相同，我们需要编写softmax函数，设置学习率和初始化Weight并编写LossFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msoftmax\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36moptimizer\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcreateMyNeuralNetwork\u001b[39m\n",
       "\u001b[36mmyNeuralNetwork\u001b[39m: (\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m[\u001b[32mINDArray\u001b[39m]{type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray;type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray})#\u001b[32m@\u001b[39m = Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum(Exp(Identity()),WrappedArray(1)))),Dot(Identity(),Weight([[0.00, -0.00, -0.00, 0.00, 0.00, 0.00, -0.00, \u001b[33m...\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlossFunction\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(implicit scores: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val expScores = exp(scores)\n",
    "  expScores / expScores.sum(1)\n",
    "}\n",
    "\n",
    "implicit def optimizer: Optimizer = new LearningRate {\n",
    "  def currentLearningRate() = 0.00001\n",
    "}\n",
    "\n",
    "def createMyNeuralNetwork(implicit input: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val initialValueOfWeight = Nd4j.randn(3072, NumberOfClasses) * 0.001\n",
    "  val weight: INDArray @Symbolic = initialValueOfWeight.toWeight\n",
    "  val result: INDArray @Symbolic = input dot weight\n",
    "  softmax.compose(result)\n",
    "}\n",
    "val myNeuralNetwork = createMyNeuralNetwork\n",
    "\n",
    "def lossFunction(implicit pair: (INDArray :: INDArray :: HNil) @Symbolic): Double @Symbolic = {\n",
    "  val input = pair.head\n",
    "  val expectedOutput = pair.tail.head\n",
    "  val probabilities = myNeuralNetwork.compose(input)\n",
    "\n",
    "  -(expectedOutput * log(probabilities)).mean //此处和准备一节中的交叉熵损失对应\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似前一节我们需要训练神经网络，但是跟上一节不同的是，这次我们的训练数据是随机读取的，上一节是反复训练同一批数据集。训练神经网络并观察每次训练loss的变化，loss的变化趋势是降低，但是不是每次都降低(前途是光明的，道路是曲折的)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23026542663574218\n",
      "0.2240816354751587\n",
      "0.2184659481048584\n",
      "0.21519021987915038\n",
      "0.20921459197998046\n",
      "0.20835773944854735\n",
      "0.20596656799316407\n",
      "0.20402472019195556\n",
      "0.19586119651794434\n",
      "0.20250749588012695\n",
      "0.19943023920059205\n",
      "0.19915710687637328\n",
      "0.19605278968811035\n",
      "0.19943703413009645\n",
      "0.19809952974319459\n",
      "0.2017662286758423\n",
      "0.1932112455368042\n",
      "0.19416286945343017\n",
      "0.19546313285827638\n",
      "0.19444431066513063\n",
      "0.20109293460845948\n",
      "0.20299816131591797\n",
      "0.19243981838226318\n",
      "0.19276008605957032\n",
      "0.1847397804260254\n",
      "0.18774752616882323\n",
      "0.18981533050537108\n",
      "0.19522281885147094\n",
      "0.18960124254226685\n",
      "0.18378946781158448\n",
      "0.1919804334640503\n",
      "0.19413912296295166\n",
      "0.18835822343826295\n",
      "0.19397008419036865\n",
      "0.1892086982727051\n",
      "0.18813304901123046\n",
      "0.18986120223999023\n",
      "0.190701687335968\n",
      "0.19284905195236207\n",
      "0.1895303726196289\n",
      "0.18327500820159912\n",
      "0.1910465955734253\n",
      "0.18699681758880615\n",
      "0.18618117570877074\n",
      "0.18339040279388427\n",
      "0.18660414218902588\n",
      "0.18499038219451905\n",
      "0.19101003408432007\n",
      "0.18396372795104982\n",
      "0.18879698514938353\n",
      "0.18523693084716797\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-1408521785\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0],\"y\":[0.23026542663574218,0.23010568618774413,0.22981162071228028,0.2299876928329468,0.2296765089035034,0.2297236442565918,0.22943491935729982,0.22876393795013428,0.22787246704101563,0.22747395038604737,0.2276679277420044,0.2279639482498169,0.228272008895874,0.22799243927001953,0.22735514640808105,0.2272050380706787,0.22713584899902345,0.22605495452880858,0.22644619941711425,0.22698559761047363,0.22579498291015626,0.22569742202758789,0.22555041313171387,0.22581515312194825,0.22375109195709228,0.22563409805297852,0.22714452743530272,0.22504618167877197,0.22483019828796386,0.22518796920776368,0.22412490844726562,0.22418551445007323,0.22585158348083495,0.2236401081085205,0.22450532913208007,0.22362174987792968,0.22396438121795653,0.22262439727783204,0.22187113761901855,0.2240816354751587,0.22349176406860352,0.2219468832015991,0.223653244972229,0.22386388778686522,0.22197771072387695,0.22210071086883545,0.22331686019897462,0.2216564416885376,0.22149133682250977,0.2208730936050415,0.22129900455474855,0.22171370983123778,0.22188305854797363,0.22160091400146484,0.22254018783569335,0.22051362991333007,0.22046728134155275,0.22217140197753907,0.22024407386779785,0.22084612846374513,0.22052617073059083,0.2228400230407715,0.22093300819396972,0.22120356559753418,0.2200525999069214,0.2209458827972412,0.22042880058288575,0.21891674995422364,0.21783504486083985,0.21679084300994872,0.21990385055541992,0.21959195137023926,0.21769211292266846,0.21996533870697021,0.22078180313110352,0.22167630195617677,0.22048838138580323,0.2173229217529297,0.2184659481048584,0.2182685613632202,0.21771254539489746,0.21753063201904296,0.21585674285888673,0.2173213005065918,0.2189176321029663,0.21684389114379882,0.21852731704711914,0.21660382747650148,0.21697182655334474,0.21789653301239015,0.21524665355682374,0.2184776782989502,0.2193929672241211,0.21861844062805175,0.21508407592773438,0.21671476364135742,0.21709537506103516,0.215740966796875,0.21385545730590821,0.21563096046447755,0.21412992477416992,0.21442878246307373,0.21838932037353515,0.2170487403869629,0.21520419120788575,0.21571364402770996,0.21420443058013916,0.21551501750946045,0.21290807723999022,0.21769437789916993,0.21828887462615967,0.21381306648254395,0.21425323486328124,0.21463570594787598,0.2144526243209839,0.21590676307678222,0.21492414474487304,0.21519021987915038,0.21627614498138428,0.21645770072937012,0.21564955711364747,0.21186580657958984,0.21459765434265138,0.2174283504486084,0.2138857364654541,0.2127843379974365,0.21227819919586183,0.21026787757873536,0.21278095245361328,0.213677978515625,0.21721670627593995,0.21398310661315917,0.21645135879516603,0.21516876220703124,0.21688337326049806,0.2111753463745117,0.2142481565475464,0.2123699426651001,0.21481046676635743,0.21102685928344728,0.21295573711395263,0.21552953720092774,0.2147979259490967,0.21412622928619385,0.2119065761566162,0.21041436195373536,0.21130688190460206,0.2106560230255127,0.21338517665863038,0.214339280128479,0.21026873588562012,0.21130504608154296,0.20926201343536377,0.21296849250793456,0.21175847053527833,0.21029431819915773,0.20921459197998046,0.21067895889282226,0.20992960929870605,0.21821250915527343,0.21242737770080566,0.21256461143493652,0.21003940105438232,0.21289558410644532,0.21124639511108398,0.2129474401473999,0.210675048828125,0.20979948043823243,0.20765244960784912,0.21153011322021484,0.21046247482299804,0.2085732936859131,0.21110591888427735,0.21245856285095216,0.20981206893920898,0.2126692295074463,0.21253085136413574,0.20877177715301515,0.21307203769683838,0.21015758514404298,0.2093101978302002,0.21393241882324218,0.21382482051849366,0.210668683052063,0.21185646057128907,0.21053557395935057,0.2089104175567627,0.20682356357574463,0.21249761581420898,0.209572434425354,0.21022133827209472,0.2088836908340454,0.2107618570327759,0.21041502952575683,0.20866069793701172,0.20835773944854735,0.21003737449645996,0.2094789505004883,0.20945279598236083,0.21061830520629882,0.20920698642730712,0.2055262804031372,0.21010158061981202,0.20680713653564453,0.20976653099060058,0.20959763526916503,0.20490765571594238,0.20844087600708008,0.20815789699554443,0.20750079154968262,0.20855050086975097,0.20679824352264403,0.20465312004089356,0.2083688497543335,0.20963654518127442,0.21118175983428955,0.2069796085357666,0.210394024848938,0.2032604694366455,0.20364842414855958,0.20852220058441162,0.2052086114883423,0.2065418243408203,0.20844657421112062,0.2075181484222412,0.20889949798583984,0.20637109279632568,0.21141452789306642,0.20831899642944335,0.20750532150268555,0.20488238334655762,0.206001877784729,0.20914390087127685,0.20425574779510497,0.20596656799316407,0.20574915409088135,0.20668928623199462,0.20569887161254882,0.20895228385925294,0.20538046360015869,0.20261940956115723,0.2061671257019043,0.2067880630493164,0.20393714904785157,0.20514154434204102,0.20617351531982422,0.2062899351119995,0.2063981533050537,0.20510764122009278,0.20454201698303223,0.20636405944824218,0.208099365234375,0.20359516143798828,0.20475971698760986,0.20687651634216309,0.19856475591659545,0.20804758071899415,0.20676202774047853,0.2054471492767334,0.20234642028808594,0.20235228538513184,0.20524678230285645,0.20907771587371826,0.20868659019470215,0.20698058605194092,0.20411975383758546,0.20666611194610596,0.20277910232543944,0.20359816551208496,0.20453472137451173,0.20384955406188965,0.20709629058837892,0.21011171340942383,0.20402472019195556,0.20623209476470947,0.20373737812042236,0.20550718307495117,0.2011936664581299,0.210204553604126,0.2040546417236328,0.20351901054382324,0.2049476385116577,0.20708179473876953,0.20286283493041993,0.2054598808288574,0.2042010545730591,0.2081557035446167,0.20728390216827391,0.20273821353912352,0.21050639152526857,0.20269079208374025,0.20772714614868165,0.20984954833984376,0.20273449420928955,0.2048166275024414,0.20165843963623048,0.20484447479248047,0.20617423057556153,0.20768957138061522,0.2077122926712036,0.20505402088165284,0.20737838745117188,0.20445818901062013,0.20276293754577637,0.2028196334838867,0.2019374132156372,0.19981107711791993,0.19988476037979125,0.20517477989196778,0.20405082702636718,0.2048433780670166,0.1968446731567383,0.19586119651794434,0.2009032964706421,0.20385568141937255,0.2058117389678955,0.20624520778656005,0.20127015113830565,0.20268149375915528,0.19687719345092775,0.20482349395751953,0.1984410762786865,0.20046494007110596,0.20596208572387695,0.2004236698150635,0.20538811683654784,0.20028319358825683,0.19938002824783324,0.20524463653564454,0.206192684173584,0.20202174186706542,0.20152065753936768,0.20690336227416992,0.20094585418701172,0.20560746192932128,0.2024254322052002,0.20184845924377443,0.20317356586456298,0.19943540096282958,0.20170061588287352,0.20514307022094727,0.20368812084197999,0.19711742401123047,0.2086253881454468,0.20757465362548827,0.2008030891418457,0.2024094581604004,0.20315299034118653,0.20295100212097167,0.19902381896972657,0.2059645175933838,0.20250749588012695,0.2044426679611206,0.20382380485534668,0.2026878833770752,0.206288743019104,0.20123419761657715,0.2034811019897461,0.20620243549346923,0.19848363399505614,0.20085701942443848,0.20531187057495118,0.1974367618560791,0.20380911827087403,0.20611093044281006,0.19732069969177246,0.20369300842285157,0.19728586673736573,0.19923217296600343,0.20452418327331542,0.2007312774658203,0.19642094373703003,0.20262043476104735,0.1984938383102417,0.20226809978485108,0.20539302825927735,0.20236902236938475,0.20213868618011474,0.20513014793395995,0.20173680782318115,0.2006377696990967,0.1966536521911621,0.2051548480987549,0.20064620971679686,0.1985402822494507,0.19699015617370605,0.19804561138153076,0.2022383213043213,0.20815587043762207,0.20569539070129395,0.19943023920059205,0.20126428604125976,0.20617740154266356,0.2008213520050049,0.19801632165908814,0.19573562145233153,0.202919864654541,0.20053138732910156,0.1998171925544739,0.2002847671508789,0.20589041709899902,0.19460082054138184,0.19893620014190674,0.19845638275146485,0.19769556522369386,0.19646031856536866,0.20106773376464843,0.2004756212234497,0.20204951763153076,0.2030125379562378,0.2005694627761841,0.20664143562316895,0.20070724487304686,0.19608467817306519,0.20161795616149902,0.20018439292907714,0.1991696238517761,0.206380033493042,0.19699580669403077,0.1982369065284729,0.2021629810333252,0.20299005508422852,0.19722416400909423,0.20759499073028564,0.20256097316741944,0.20109667778015136,0.19640758037567138,0.19738105535507203,0.19875845909118653,0.19915710687637328,0.19748202562332154,0.19738376140594482,0.1966722011566162,0.19674650430679322,0.19754273891448976,0.19939568042755126,0.20155925750732423,0.19516538381576537,0.19871194362640382,0.1945065975189209,0.2073296546936035,0.19950917959213257,0.1988161563873291,0.19860740900039672,0.19769618511199952,0.19714211225509642,0.20030298233032226,0.20024945735931396,0.1961742162704468,0.2017892360687256,0.2018738031387329,0.2044145107269287,0.19723060131072997,0.19734158515930175,0.19124042987823486,0.2049170732498169,0.2029658794403076,0.19747521877288818,0.20594427585601807,0.19678570032119752,0.20483896732330323,0.20066637992858888,0.19975064992904662,0.20546979904174806,0.2007746458053589,0.19719972610473632,0.20016272068023683,0.19935826063156128,0.19605278968811035,0.1964864730834961,0.20224876403808595,0.19980325698852539,0.20231094360351562,0.20206713676452637,0.20324668884277344,0.20130574703216553,0.19888169765472413,0.19629313945770263,0.19904112815856934,0.1963857889175415,0.2049560546875,0.2062225818634033,0.19864379167556762,0.19490677118301392,0.20198547840118408,0.20637917518615723,0.19838956594467164,0.19983009099960328,0.19833412170410156,0.20148029327392578,0.1926811933517456,0.1997140884399414,0.19531991481781005,0.2035959005355835,0.195887291431427,0.19788358211517335,0.19666805267333984,0.19653834104537965,0.19784290790557862,0.19585983753204345,0.19575252532958984,0.20000429153442384,0.20119976997375488,0.19650938510894775,0.2004420280456543,0.19790658950805665,0.19749656915664673,0.19943703413009645,0.19186489582061766,0.20128540992736815,0.20049650669097902,0.19829788208007812,0.1990067720413208,0.20348889827728273,0.1971351385116577,0.19536747932434081,0.1992206573486328,0.19319050312042235,0.19952890872955323,0.20139923095703124,0.19113363027572633,0.20110177993774414,0.20171699523925782,0.19589452743530272,0.19514474868774415,0.2042611837387085,0.20364081859588623,0.200812029838562,0.19173349142074586,0.1984349250793457,0.1988288164138794,0.20206775665283203,0.200327467918396,0.20060088634490966,0.19605543613433837,0.195762038230896,0.20070173740386962,0.20770370960235596,0.1998763084411621,0.1964549422264099,0.19641175270080566,0.19571512937545776,0.19896438121795654,0.19615576267242432,0.19505149126052856,0.20108942985534667,0.19809952974319459,0.19556323289871216,0.1954312801361084,0.19372196197509767,0.19889868497848512,0.19638569355010987,0.20002660751342774,0.18946045637130737,0.1956297516822815,0.19934873580932616,0.2025749683380127,0.19683318138122557,0.20067646503448486,0.2051367998123169,0.1913985252380371,0.1966652750968933,0.1994231104850769,0.19169464111328124,0.19489508867263794,0.2006075382232666,0.19846166372299195,0.19245995283126832,0.19413652420043945,0.19866178035736085,0.1946011185646057,0.1878812074661255,0.19526270627975464,0.19653668403625488,0.20000174045562744,0.19269589185714722,0.19894745349884033,0.19339873790740966,0.19536736011505126,0.19752418994903564,0.19941030740737914,0.19804247617721557,0.19098609685897827,0.19271827936172486,0.2028197765350342,0.2017662286758423,0.19696414470672607,0.20056891441345215,0.1929907202720642,0.1944815993309021,0.19419643878936768,0.19415681362152098,0.20331330299377443,0.19547626972198487,0.19494658708572388,0.20053040981292725,0.20067672729492186,0.19451868534088135,0.19701130390167237,0.19425222873687745,0.19616343975067138,0.1936996340751648,0.2013704776763916,0.19010379314422607,0.19494251012802125,0.19559979438781738,0.19889017343521118,0.20240449905395508,0.2022979736328125,0.19865860939025878,0.19452145099639892,0.19569791555404664,0.19711567163467408,0.19385143518447875,0.20054521560668945,0.1991976022720337,0.19372832775115967,0.20144226551055908,0.19736053943634033,0.19873050451278687,0.19954273700714112,0.19419941902160645,0.20324864387512206,0.19794176816940307,0.1932112455368042,0.18912191390991212,0.1954971194267273,0.19089633226394653,0.19700881242752075,0.19346258640289307,0.19778928756713868,0.20255799293518068,0.19586377143859862,0.19348756074905396,0.1959100604057312,0.18904943466186525,0.19520987272262574,0.19799656867980958,0.1989063024520874,0.20813500881195068,0.19293627738952637,0.1976740002632141,0.19141483306884766,0.19071745872497559,0.18988301753997802,0.1946919322013855,0.19515023231506348,0.1988425612449646,0.19789307117462157,0.20064058303833007,0.1964048385620117,0.195403790473938,0.19638116359710694,0.1924574851989746,0.20017616748809813,0.19128555059432983,0.19659711122512818,0.19427034854888917,0.19704751968383788,0.20030908584594725,0.19354478120803834,0.19715549945831298,0.18838884830474853,0.19416286945343017,0.19016402959823608,0.19683616161346434,0.19569294452667235,0.1918305993080139,0.1880557656288147,0.19360460042953492,0.19468085765838622,0.19883229732513427,0.19688384532928466,0.19960458278656007,0.19253727197647094,0.1937488317489624,0.20198354721069336,0.20124506950378418,0.19639827013015748,0.19887781143188477,0.1919912576675415,0.19315617084503173,0.19092570543289183,0.19344305992126465,0.19901400804519653,0.19376657009124756,0.19363458156585694,0.19973992109298705,0.20264167785644532,0.1937435269355774,0.19733281135559083,0.1951951265335083,0.1934604525566101,0.20239157676696778,0.1937557578086853,0.19704960584640502,0.19800753593444825,0.19713116884231568,0.19803134202957154,0.19607943296432495,0.19513137340545655,0.19107645750045776,0.19546313285827638,0.20046117305755615,0.19684956073760987,0.20004820823669434,0.19619601964950562,0.19143596887588502,0.18381434679031372,0.18796950578689575,0.1929019570350647,0.1959884524345398,0.20208840370178222,0.1950498104095459,0.1920359253883362,0.19310307502746582,0.19269168376922607,0.1996893048286438,0.19309002161026,0.19061801433563233,0.19537947177886963,0.20102858543395996,0.19939944744110108,0.1992802858352661,0.19106700420379638,0.1905883550643921,0.19270049333572387,0.1945844292640686,0.19667575359344483,0.19449867010116578,0.18946436643600464,0.19199507236480712,0.19346351623535157,0.2004782438278198,0.18515141010284425,0.19896352291107178,0.1911146640777588,0.19531471729278566,0.1947772979736328,0.19258676767349242,0.1934814929962158,0.19444431066513063,0.18925048112869264,0.1969442844390869,0.1913282036781311,0.1964379668235779,0.1996822476387024,0.1880840539932251,0.20073423385620118,0.19741681814193726,0.20442705154418944,0.19741684198379517,0.1924516439437866,0.19387969970703126,0.19024642705917358,0.19385911226272584,0.19786311388015748,0.19785768985748292,0.19055402278900146,0.19837307929992676,0.19627605676651,0.1920934200286865,0.19225573539733887,0.1952134132385254,0.19315977096557618,0.19735440015792846,0.19001326560974122,0.1916085362434387,0.19830148220062255,0.19806119203567504,0.18987135887145995,0.19125220775604249,0.19247980117797853,0.18998907804489135,0.19785598516464234,0.18665285110473634,0.19205303192138673,0.1918914794921875,0.18958797454833984,0.19888091087341309,0.20109293460845948,0.1907564163208008,0.19509847164154054,0.1982651948928833,0.190544056892395,0.19491307735443114,0.19569333791732788,0.19164175987243653,0.19119038581848144,0.19687572717666627,0.1842094898223877,0.19356558322906495,0.193582284450531,0.18621432781219482,0.20304837226867675,0.19671918153762818,0.1905628561973572,0.1966426134109497,0.19548454284667968,0.19308440685272216,0.19277364015579224,0.19321761131286622,0.1859985589981079,0.19829721450805665,0.193653666973114,0.19169330596923828,0.20124964714050292,0.19188919067382812,0.18789095878601075,0.19459772109985352,0.190313720703125,0.1936332941055298,0.18955590724945068,0.19289963245391845,0.1897546887397766,0.1947322368621826,0.18920423984527587,0.18786700963973998,0.19045857191085816,0.20299816131591797,0.1925589084625244,0.19039604663848878,0.19532943964004518,0.19439114332199098,0.19357538223266602,0.19045896530151368,0.20093157291412353,0.19437117576599122,0.19134188890457154,0.19507417678833008,0.1931986093521118,0.18846700191497803,0.19269881248474122,0.1957475423812866,0.19938284158706665,0.19701999425888062,0.1957334518432617,0.19324625730514527,0.18540698289871216,0.19352612495422364,0.1939631462097168,0.19282839298248292,0.19937096834182738,0.192415988445282,0.18563516139984132,0.1971033811569214,0.1942988872528076,0.18828816413879396,0.19495941400527955,0.18748773336410524,0.20143506526947022,0.20002593994140624,0.19033807516098022,0.1882164478302002,0.19504389762878419,0.19009498357772828,0.18950169086456298,0.1895579695701599,0.19243981838226318,0.19173884391784668,0.18915796279907227,0.1950697898864746,0.19798765182495118,0.18827497959136963,0.1971452236175537,0.19557459354400636,0.19604551792144775,0.19599878787994385,0.19062998294830322,0.19837799072265624,0.18930087089538575,0.19013553857803345,0.19502737522125244,0.19414153099060058,0.18660945892333985,0.19204673767089844,0.19331353902816772,0.1959235429763794,0.20035829544067382,0.19792006015777588,0.19641404151916503,0.19711432456970215,0.18804771900177003,0.18529514074325562,0.19507865905761718,0.19188182353973388,0.1959691286087036,0.18881683349609374,0.19709317684173583,0.19680390357971192,0.1895349144935608,0.1940123438835144,0.1927224278450012,0.18479640483856202,0.19115681648254396,0.19431426525115966,0.19050477743148803,0.19276008605957032,0.19847168922424316,0.1901547431945801,0.19085729122161865,0.19098284244537353,0.1985682249069214,0.1979779839515686,0.1901235818862915,0.192244553565979,0.189949631690979,0.1966289758682251,0.19926347732543945,0.19538592100143432,0.19524333477020264,0.19116106033325195,0.19117820262908936,0.19376975297927856,0.19853475093841552,0.19835788011550903,0.20068731307983398,0.1876606225967407,0.19195858240127564,0.18961275815963746,0.1900365710258484,0.18994204998016356,0.19035332202911376,0.1955565929412842,0.19183430671691895,0.19757884740829468,0.19108519554138184,0.1898923397064209,0.1930652976036072,0.19626764059066773,0.19297052621841432,0.185569167137146,0.18669309616088867,0.19344193935394288,0.19016659259796143,0.18407413959503174,0.1847397804260254,0.19239954948425292,0.19099411964416504,0.2016843557357788,0.19298261404037476,0.19249844551086426,0.19090394973754882,0.19371029138565063,0.19320781230926515,0.18455933332443236,0.19204213619232177,0.19121115207672118,0.18727396726608275,0.1917191982269287,0.1890254259109497,0.18790066242218018,0.19287495613098143,0.18786189556121827,0.18952631950378418,0.1961592197418213,0.1909242868423462,0.18965108394622804,0.19400379657745362,0.19717713594436645,0.19172896146774293,0.1853264331817627,0.19319010972976686,0.1855151891708374,0.18541067838668823,0.18385496139526367,0.19046247005462646,0.19435529708862304,0.1901235818862915,0.18647726774215698,0.19500333070755005,0.18932995796203614,0.19572354555130006,0.19065542221069337,0.19467493295669555,0.18774752616882323,0.19097352027893066,0.19132620096206665,0.19926837682724,0.19651963710784912,0.18618419170379638,0.19479459524154663,0.19729361534118653,0.19370778799057006,0.18769978284835814,0.1904125213623047,0.19865682125091552,0.1983005166053772,0.18864248991012572,0.17957953214645386,0.1940455913543701,0.18893759250640868,0.188407564163208,0.19856786727905273,0.19218857288360597,0.19615228176116944,0.18134069442749023,0.188395893573761,0.19056715965270996,0.19012107849121093,0.19076616764068605,0.18885388374328613,0.19070649147033691,0.18754489421844484,0.1910192847251892,0.1921602964401245,0.19291812181472778,0.18856559991836547,0.19407068490982055,0.1952804684638977,0.19290305376052858,0.1926661729812622,0.1955682396888733,0.19703304767608643,0.18981533050537108,0.19415857791900634,0.19245142936706544,0.1876848578453064,0.18300927877426149,0.19048845767974854,0.1851280450820923,0.19602470397949218,0.18683605194091796,0.18950743675231935,0.19014277458190917,0.18715691566467285,0.1868961811065674,0.19360647201538086,0.19818754196166993,0.1836178183555603,0.18604222536087037,0.19030120372772216,0.1909189224243164,0.1808635950088501,0.1919942617416382,0.18936773538589477,0.19379947185516358,0.19522593021392823,0.1936441421508789,0.19318001270294188,0.18929020166397095,0.18989474773406984,0.19173232316970826,0.19188669919967652,0.19028377532958984,0.19744197130203248,0.18787260055541993,0.19495091438293458,0.19062962532043456,0.1876461386680603,0.18682124614715576,0.18329899311065673,0.19190455675125123,0.19522281885147094,0.18728113174438477,0.19758951663970947,0.19215359687805175,0.18494949340820313,0.18856983184814452,0.1850508451461792,0.1919344186782837,0.197179114818573,0.18582724332809447,0.18580212593078613,0.19911218881607057,0.19343736171722412,0.18680281639099122,0.1882805347442627,0.19275435209274291,0.1916093111038208,0.1889681339263916,0.19884235858917237,0.19200804233551025,0.1951590061187744,0.19755932092666625,0.18819997310638428,0.19153225421905518,0.18781028985977172,0.18876924514770507,0.19173169136047363,0.18560352325439453,0.19966142177581786,0.1958057165145874,0.18978835344314576,0.1873831868171692,0.1862668514251709,0.18815361261367797,0.18973982334136963,0.18382177352905274,0.19471606016159057,0.18934392929077148,0.1920325756072998,0.18960124254226685,0.18565192222595214,0.1895500659942627,0.18599940538406373,0.19681503772735595,0.18621399402618408,0.1954297184944153,0.18804124593734742,0.18899843692779542,0.1963346242904663,0.1839166283607483,0.19165830612182616,0.18295111656188964,0.19383046627044678,0.1936131715774536,0.19165802001953125,0.19554868936538697,0.18563480377197267,0.19937942028045655,0.18295421600341796,0.19340975284576417,0.1934262990951538,0.19278141260147094,0.18756927251815797,0.19166595935821534,0.19626283645629883,0.1920868992805481,0.19722273349761962,0.19284236431121826,0.1913796544075012,0.18852646350860597,0.18037208318710327,0.1898277997970581,0.18985073566436766,0.18488532304763794,0.18107304573059083,0.18785665035247803,0.1973314642906189,0.1964103102684021,0.18378946781158448,0.18845967054367066,0.19407501220703124,0.18770260810852052,0.19024133682250977,0.19304414987564086,0.1902213215827942,0.19221196174621583,0.19168975353240966,0.1962423086166382,0.19394066333770751,0.18885813951492308,0.19312503337860107,0.19509785175323485,0.1904050350189209,0.18606094121932984,0.1868297815322876,0.19708930253982543,0.18426408767700195,0.19086406230926514,0.1842191696166992,0.19383984804153442,0.18969932794570923,0.19396940469741822,0.185048508644104,0.19435418844223024,0.19336445331573487,0.18980071544647217,0.19035680294036866,0.18809558153152467,0.1922408938407898,0.19313861131668092,0.18905208110809327,0.18660647869110109,0.1933324456214905,0.20166659355163574,0.19406405687332154,0.19178428649902343,0.18492681980133058,0.1919804334640503,0.18592586517333984,0.18897488117218017,0.18800764083862304,0.19650182723999024,0.18455352783203124,0.18505719900131226,0.19370263814926147,0.18679163455963135,0.18754334449768068,0.18725242614746093,0.18885602951049804,0.1914912223815918,0.18985402584075928,0.1904706120491028,0.18540364503860474,0.1872016668319702,0.18450634479522704,0.19614765644073487,0.18888726234436035,0.1885761499404907,0.19191720485687255,0.19562740325927735,0.19267632961273193,0.1876125693321228,0.18772926330566406,0.1833242416381836,0.1901752233505249,0.18649041652679443,0.20314266681671142,0.19173483848571776,0.19285664558410645,0.18947638273239137,0.18486957550048827,0.1899275541305542,0.18956520557403564,0.18952033519744874,0.19121987819671632,0.19023675918579103,0.19413912296295166,0.1972738265991211,0.18924438953399658,0.1805800437927246,0.19089311361312866,0.1975041151046753,0.18965365886688232,0.1912912130355835,0.19340020418167114,0.19057846069335938,0.19279085397720336,0.19110862016677857,0.19693665504455565,0.18845593929290771,0.18809497356414795,0.18501503467559816,0.19137239456176758,0.1921844959259033,0.1845219612121582,0.19515293836593628,0.18474605083465576,0.1918490409851074,0.18675503730773926,0.1888843297958374,0.18518331050872802,0.19215946197509765,0.18489587306976318,0.1955728769302368,0.18780996799468994,0.19242118597030639,0.18520970344543458,0.19654545783996583,0.18571269512176514,0.1787409543991089,0.20050303936004638,0.18964773416519165,0.1852388024330139,0.18984706401824952,0.19100239276885986,0.18835822343826295,0.18982690572738647,0.18973169326782227,0.18510349988937377,0.19541839361190796,0.19543899297714235,0.1903888463973999,0.1858230471611023,0.193967866897583,0.19781466722488403,0.1871999979019165,0.1889824390411377,0.19034749269485474,0.1895847201347351,0.19211572408676147,0.1860732078552246,0.17929587364196778,0.1906541109085083,0.18852778673171997,0.18869941234588622,0.190712308883667,0.1902020215988159,0.18505575656890869,0.19576785564422608,0.19466936588287354,0.18849306106567382,0.19467060565948485,0.191848087310791,0.18664876222610474,0.19123351573944092,0.18528140783309938,0.19313695430755615,0.1851423978805542,0.1876904010772705,0.1858069658279419,0.1858322501182556,0.18877882957458497,0.187307870388031,0.18120713233947755,0.19397008419036865,0.1896706700325012,0.18533930778503419,0.1899282693862915,0.19197843074798585,0.1896423101425171,0.1930303692817688,0.18800361156463624,0.18119369745254515,0.18878649473190307,0.1875463008880615,0.18018748760223388,0.1885831117630005,0.19047918319702148,0.1894500732421875,0.1884856104850769,0.18527045249938964,0.19167706966400147,0.19401175975799562,0.1884305238723755,0.1857372760772705,0.19119445085525513,0.19548306465148926,0.1919190526008606,0.18726673126220703,0.1887826442718506,0.1917298436164856,0.18751611709594726,0.19236066341400146,0.18542392253875734,0.19338909387588502,0.1935083508491516,0.18729223012924195,0.1869664430618286,0.19653379917144775,0.1877066969871521,0.18855159282684325,0.1889522910118103,0.19448035955429077,0.1892086982727051,0.1841609001159668,0.1895223617553711,0.19078383445739747,0.18420885801315307,0.19071131944656372,0.18458101749420167,0.18767049312591552,0.19650652408599853,0.18743009567260743,0.18615483045578002,0.19102280139923095,0.18760855197906495,0.19436607360839844,0.1889812469482422,0.1907341241836548,0.19838790893554686,0.18682780265808105,0.185494327545166,0.18519575595855714,0.18544762134552,0.18799450397491455,0.1891664981842041,0.1926115036010742,0.18883209228515624,0.19529119729995728,0.19337719678878784,0.17877588272094727,0.1919718027114868,0.17998796701431274,0.184531569480896,0.18451523780822754,0.1844409465789795,0.1915399670600891,0.1815665602684021,0.18988232612609862,0.18517818450927734,0.18907305002212524,0.18762811422348022,0.18813304901123046,0.18626294136047364,0.18339911699295045,0.18751871585845947,0.1935095429420471,0.1875994920730591,0.19230444431304933,0.19277077913284302,0.19452260732650756,0.19020067453384398,0.18942734003067016,0.17908086776733398,0.18709917068481446,0.1865278959274292,0.1867668628692627,0.18769803047180175,0.18992934226989747,0.18416669368743896,0.1907390236854553,0.18570139408111572,0.19070322513580323,0.18777596950531006,0.18801043033599854,0.1856331706047058,0.18458389043807982,0.18593919277191162,0.19354554414749145,0.19094895124435424,0.19212608337402343,0.1926330327987671,0.18471840620040894,0.18305341005325318,0.18976237773895263,0.188588285446167,0.1850555419921875,0.18722834587097167,0.18516032695770263,0.18790723085403443,0.18421903848648072,0.18986120223999023,0.18567318916320802,0.1826709032058716,0.1849273920059204,0.18515305519104003,0.18368823528289796,0.18554322719573973,0.1906743288040161,0.1899394989013672,0.18092145919799804,0.18285057544708253,0.1863866329193115,0.1895408511161804,0.18861134052276612,0.17872416973114014,0.18535165786743163,0.1875445604324341,0.19012153148651123,0.18839628696441652,0.18952698707580568,0.18550102710723876,0.194429612159729,0.1907965898513794,0.18686538934707642,0.19190036058425902,0.18462824821472168,0.18501176834106445,0.17997751235961915,0.18101857900619506,0.19243522882461547,0.18866167068481446,0.1793235182762146,0.18642046451568603,0.18714258670806885,0.18666155338287355,0.1896405339241028,0.18703598976135255,0.18976502418518065,0.18985310792922974,0.190701687335968,0.18335943222045897,0.1899051070213318,0.18287270069122313,0.19761173725128173,0.19154419898986816,0.19758715629577636,0.19087703227996827,0.1913595676422119,0.1918697953224182,0.19245512485504152,0.184186589717865,0.19288555383682252,0.1877354860305786,0.19735932350158691,0.18930480480194092,0.19246306419372558,0.19271939992904663,0.18398438692092894,0.18679525852203369,0.185294246673584,0.18576077222824097,0.18228040933609008,0.18679587841033934,0.19917103052139282,0.18351945877075196,0.18401176929473878,0.19500582218170165,0.199295973777771,0.20041155815124512,0.18445584774017335,0.18349688053131102,0.17873337268829345,0.1828696012496948,0.1925214171409607,0.19061239957809448,0.18615418672561646,0.1789208769798279,0.18985264301300048,0.19284905195236207,0.18101632595062256,0.19264572858810425,0.19996137619018556,0.1867121458053589,0.18869302272796631,0.19194116592407226,0.18395856618881226,0.18480707406997682,0.18666614294052125,0.18504934310913085,0.1800644040107727,0.18300410509109497,0.18190066814422606,0.18197083473205566,0.19492676258087158,0.1893675446510315,0.18902189731597902,0.18453648090362548,0.18839633464813232,0.18631035089492798,0.17849538326263428,0.1962471842765808,0.18632453680038452,0.19018056392669677,0.18887665271759033,0.17810938358306885,0.17894017696380615,0.1892995595932007,0.18283565044403077,0.18996505737304686,0.18885232210159303,0.18676304817199707,0.18304572105407715,0.18984826803207397,0.18792022466659547,0.18466097116470337,0.18152389526367188,0.19062414169311523,0.1895303726196289,0.19094518423080445,0.19158976078033446,0.18596900701522828,0.17684872150421144,0.1827799439430237,0.18999890089035035,0.1938018321990967,0.18730528354644777,0.19217910766601562,0.18035604953765869,0.18612501621246338,0.18756427764892578,0.19287478923797607,0.17571239471435546,0.18718039989471436,0.17723733186721802,0.1924393892288208,0.19041697978973388,0.18569958209991455,0.187906813621521,0.1917084574699402,0.19251229763031005,0.19323413372039794,0.19106824398040773,0.18978041410446167,0.18469220399856567,0.19424903392791748,0.1822504758834839,0.18785886764526366,0.18780462741851806,0.18761205673217773,0.1993544101715088,0.19085259437561036,0.19939111471176146,0.18678759336471557,0.18842052221298217,0.19023571014404297,0.1868952512741089,0.18327500820159912,0.18761105537414552,0.19881366491317748,0.18742454051971436,0.19036991596221925,0.17978695631027222,0.18603910207748414,0.18647997379302977,0.18420910835266113,0.181572425365448,0.19581084251403807,0.1849550724029541,0.1840347409248352,0.19305765628814697,0.18914344310760497,0.18464770317077636,0.19059948921203612,0.18728984594345094,0.18964120149612426,0.18748489618301392,0.18122589588165283,0.1876556396484375,0.1871593952178955,0.18668067455291748,0.18112761974334718,0.19118058681488037,0.17820992469787597,0.19033439159393312,0.18754079341888427,0.188959801197052,0.18201494216918945,0.1828112483024597,0.18861182928085327,0.18186901807785033,0.18397985696792601,0.18470823764801025,0.18589682579040528,0.1975712299346924,0.19901098012924195,0.1910465955734253,0.18537535667419433,0.18880162239074708,0.19829567670822143,0.1928795576095581,0.17812259197235109,0.18460090160369874,0.1845238208770752,0.18692622184753419,0.19077799320220948,0.18267877101898194,0.1911557912826538,0.1891288638114929,0.18256466388702391,0.18629224300384523,0.1875152826309204,0.17849022150039673,0.18293534517288207,0.18956658840179444,0.19515825510025026,0.18504998683929444,0.18311308622360228,0.18859350681304932,0.18389919996261597,0.18038620948791503,0.1872767448425293,0.18428906202316284,0.17828190326690674,0.17856287956237793,0.1888031244277954,0.18325194120407104,0.19047937393188477,0.1895918369293213,0.1880566120147705,0.18726511001586915,0.18368775844573976,0.19077181816101074,0.18362073898315429,0.18490736484527587,0.18699681758880615,0.1872728705406189,0.18551232814788818,0.18444674015045165,0.18301421403884888,0.19772715568542482,0.19069617986679077,0.1882076621055603,0.18953614234924315,0.18015817403793336,0.1831668496131897,0.1832705020904541,0.1948901057243347,0.18192682266235352,0.17990217208862305,0.19197797775268555,0.18964370489120483,0.1805531620979309,0.19244470596313476,0.1938732385635376,0.1913423180580139,0.18456469774246215,0.18749849796295165,0.18153923749923706,0.18465955257415773,0.1880261182785034,0.18817434310913086,0.19065234661102295,0.1885233163833618,0.18815065622329713,0.18029706478118895,0.19449595212936402,0.18055741786956786,0.1890597939491272,0.1886386513710022,0.19041422605514527,0.1913212537765503,0.1928417682647705,0.18648114204406738,0.18618117570877074,0.17829716205596924,0.18129234313964843,0.18950014114379882,0.1786506414413452,0.1850941777229309,0.18622226715087892,0.1868286609649658,0.17786548137664795,0.18542225360870362,0.18795713186264038,0.19168365001678467,0.1804222822189331,0.19347304105758667,0.18755590915679932,0.18284187316894532,0.181028151512146,0.18745849132537842,0.18437621593475342,0.19115041494369506,0.18616847991943358,0.18720216751098634,0.19177281856536865,0.18858375549316406,0.183687162399292,0.1881155252456665,0.18041489124298096,0.18665043115615845,0.1880113124847412,0.18441901206970215,0.1966543197631836,0.19325164556503296,0.19576297998428344,0.19146519899368286,0.18609033823013305,0.19094421863555908,0.1828855037689209,0.1979135274887085,0.19057724475860596,0.18339040279388427,0.19135074615478515,0.18703532218933105,0.18776867389678956,0.18750321865081787,0.1759483814239502,0.1846928596496582,0.19195395708084106,0.1857641577720642,0.18868894577026368,0.19434428215026855,0.1935521125793457,0.18136531114578247,0.1841113567352295,0.18765562772750854,0.1824953317642212,0.18435741662979127,0.18589664697647096,0.18561465740203859,0.180816125869751,0.1873847246170044,0.18569573163986205,0.17806326150894164,0.1839299201965332,0.19026485681533814,0.18645572662353516,0.18579654693603515,0.1920522689819336,0.18050565719604492,0.19492121934890747,0.1845048189163208,0.18808714151382447,0.19272363185882568,0.18955790996551514,0.18096086978912354,0.18360882997512817,0.19168739318847655,0.18829140663146973,0.18418149948120116,0.18660414218902588,0.18476234674453734,0.1796216130256653,0.1867426633834839,0.18591797351837158,0.19377907514572143,0.1819508671760559,0.1874970555305481,0.1780298352241516,0.19155796766281127,0.1926701307296753,0.1786892056465149,0.184730064868927,0.19276506900787355,0.18904348611831664,0.18507305383682252,0.1885140895843506,0.18018193244934083,0.18565068244934083,0.18411636352539062,0.18484512567520142,0.1875048041343689,0.18789138793945312,0.18869178295135497,0.18680119514465332,0.1846606969833374,0.18613071441650392,0.19305269718170165,0.18763562440872192,0.19005249738693236,0.19715285301208496,0.1814936637878418,0.1941503643989563,0.18920738697052003,0.19038925170898438,0.17576286792755128,0.19311660528182983,0.18522002696990966,0.18724329471588136,0.18499038219451905,0.1784893751144409,0.18042445182800293,0.19163856506347657,0.18148481845855713,0.1849538803100586,0.1874774694442749,0.17942545413970948,0.18683329820632935,0.18708198070526122,0.18656474351882935,0.18922265768051147,0.1867189049720764,0.18143383264541627,0.18064618110656738,0.17769742012023926,0.2019521713256836,0.18230427503585817,0.19418768882751464,0.1845881700515747,0.1887553572654724,0.18546099662780763,0.18580058813095093,0.190855872631073,0.17694568634033203,0.18593964576721192,0.1821354627609253,0.18721766471862794,0.18221458196640014,0.18667232990264893,0.19476580619812012,0.18499674797058105,0.18179450035095215,0.18612842559814452,0.19106130599975585,0.18450608253479003,0.18710572719573976,0.18627331256866456,0.18587203025817872,0.19101003408432007,0.19439489841461183,0.1830738067626953,0.18737460374832154,0.1850982427597046,0.18706365823745727,0.18610117435455323,0.1837064504623413,0.19256625175476075,0.1863795757293701,0.18102734088897704,0.18873015642166138,0.18155055046081542,0.18170109987258912,0.1822758913040161,0.18121289014816283,0.1861758828163147,0.18966367244720458,0.19173598289489746,0.1895212411880493,0.18143997192382813,0.18942325115203856,0.18284380435943604,0.182945716381073,0.1899533152580261,0.18468999862670898,0.1873805046081543,0.18700926303863524,0.1841438055038452,0.1917789936065674,0.18452297449111937,0.18572283983230592,0.18388855457305908,0.18653564453125,0.18344802856445314,0.1840593099594116,0.18850032091140748,0.18154252767562867,0.19327826499938966,0.18396372795104982,0.192272412776947,0.18782124519348145,0.1849796175956726,0.1869663953781128,0.18194971084594727,0.1866147518157959,0.1955707550048828,0.18758201599121094,0.1800561308860779,0.1825082778930664,0.1888020396232605,0.1851372480392456,0.17992675304412842,0.18242330551147462,0.18192708492279053,0.18913244009017943,0.18175764083862306,0.18408368825912474,0.17862460613250733,0.18363772630691527,0.1853269100189209,0.18570520877838134,0.18881572484970094,0.18221887350082397,0.18899731636047362,0.18125354051589965,0.18976346254348755,0.18476946353912355,0.1826733946800232,0.19121413230895995,0.18183780908584596,0.17941195964813234,0.17723013162612916,0.1887085556983948,0.18271793127059938,0.1875782608985901,0.19223980903625487,0.1844740629196167,0.18879698514938353,0.189203941822052,0.1777260422706604,0.19176852703094482,0.1812930703163147,0.18727259635925292,0.1823434829711914,0.1958681344985962,0.18485474586486816,0.1862800359725952,0.18117496967315674,0.17815496921539306,0.18376097679138184,0.18010499477386474,0.17871110439300536,0.19623087644577025,0.19356493949890136,0.19498281478881835,0.1872388243675232,0.18982839584350586,0.19134246110916137,0.18832719326019287,0.19978688955307006,0.18685345649719237,0.18667776584625245,0.1774766445159912,0.18086564540863037,0.19421787261962892,0.1828979730606079,0.18581757545471192,0.19537030458450316,0.17401715517044067,0.18702549934387208,0.1846449613571167,0.18185560703277587,0.19194560050964354,0.1851633071899414,0.1917165994644165,0.18305777311325072,0.18523693084716797,0.18862004280090333,0.1846139669418335,0.185693097114563,0.18668968677520753,0.1821949601173401,0.1797874689102173,0.19205875396728517,0.18044095039367675,0.1922750473022461,0.18471057415008546,0.18810570240020752,0.18692647218704223,0.19077104330062866,0.18683899641036988,0.1834045171737671,0.18043115139007568,0.18744497299194335,0.19063459634780883,0.18407195806503296,0.17942895889282226,0.18957598209381105,0.1858723521232605,0.1870497703552246,0.18175067901611328,0.18466320037841796,0.1793805718421936,0.18717970848083496,0.18026703596115112,0.19328099489212036,0.1846792221069336,0.18267924785614015,0.1913455605506897,0.18150067329406738,0.18506120443344115,0.1894014835357666,0.183494234085083,0.17695379257202148,0.17798159122467042]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss by time\"};\n",
       "\n",
       "  Plotly.plot('plot-1408521785', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mMiniBatchSize\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m256\u001b[39m\n",
       "\u001b[36mrandom\u001b[39m: \u001b[32mRandom\u001b[39m = scala.util.Random@236f974c\n",
       "\u001b[36mlossSeq\u001b[39m: \u001b[32mIndexedSeq\u001b[39m[\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m.\u001b[32m<refinement>\u001b[39m.this.type.\u001b[32mOutputData\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m0.23026542663574218\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mplot\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mScatter\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mScatter\u001b[39m(\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mres17_4\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-1408521785\"\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val MiniBatchSize = 256\n",
    "\n",
    "val random = new Random\n",
    "\n",
    "val lossSeq =\n",
    "  (\n",
    "    for (_ <- 0 to 50) yield {\n",
    "      val randomIndex = random\n",
    "        .shuffle[Int, IndexedSeq](0 until 10000) //https://issues.scala-lang.org/browse/SI-6948\n",
    "        .toArray\n",
    "      for (times <- 0 until 10000 / MiniBatchSize) yield {\n",
    "        val randomIndexArray =\n",
    "          randomIndex.slice(times * MiniBatchSize,\n",
    "                            (times + 1) * MiniBatchSize)\n",
    "          val trainNDArray :: expectLabel :: shapeless.HNil =\n",
    "            ReadCIFAR10ToNDArray.getSGDTrainNDArray(randomIndexArray)\n",
    "\n",
    "          val input =\n",
    "            trainNDArray.reshape(MiniBatchSize, 3072)\n",
    "\n",
    "          val expectLabelVectorized =\n",
    "            Utils.makeVectorized(expectLabel, NumberOfClasses)\n",
    "          val loss = lossFunction.train(input :: expectLabelVectorized :: HNil)\n",
    "          if(times == 0){\n",
    "            println(loss)\n",
    "          }\n",
    "          loss\n",
    "      }\n",
    "    }\n",
    "  ).flatten\n",
    "\n",
    "val plot = Seq(\n",
    "  Scatter(lossSeq.indices, lossSeq)\n",
    ")\n",
    "\n",
    "plot.plot(\n",
    "  title = \"loss by time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟上一节相同，我们使用测试数据来查看神经网络判断结果并计算准确率。这次准确率应该会有所上升，最终结果在40%左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: [[0.08, 0.10, 0.14, 0.17, 0.06, 0.16, 0.13, 0.03, 0.10, 0.02],\n",
      " [0.06, 0.21, 0.02, 0.01, 0.00, 0.01, 0.01, 0.01, 0.26, 0.42],\n",
      " [0.13, 0.07, 0.03, 0.02, 0.01, 0.02, 0.01, 0.01, 0.55, 0.16],\n",
      " [0.25, 0.08, 0.09, 0.04, 0.02, 0.03, 0.01, 0.04, 0.38, 0.05],\n",
      " [0.04, 0.04, 0.15, 0.14, 0.20, 0.16, 0.13, 0.09, 0.03, 0.02],\n",
      " [0.01, 0.11, 0.06, 0.15, 0.08, 0.13, 0.32, 0.06, 0.01, 0.06],\n",
      " [0.01, 0.05, 0.05, 0.31, 0.03, 0.33, 0.15, 0.03, 0.02, 0.02],\n",
      " [0.03, 0.03, 0.19, 0.09, 0.19, 0.11, 0.19, 0.12, 0.03, 0.02],\n",
      " [0.06, 0.06, 0.18, 0.16, 0.13, 0.20, 0.08, 0.08, 0.03, 0.01],\n",
      " [0.18, 0.26, 0.05, 0.03, 0.01, 0.01, 0.01, 0.03, 0.24, 0.17],\n",
      " [0.25, 0.09, 0.06, 0.08, 0.04, 0.08, 0.03, 0.03, 0.29, 0.05],\n",
      " [0.02, 0.26, 0.02, 0.01, 0.01, 0.01, 0.02, 0.04, 0.06, 0.53],\n",
      " [0.03, 0.11, 0.10, 0.17, 0.12, 0.20, 0.16, 0.06, 0.04, 0.02],\n",
      " [0.07, 0.28, 0.05, 0.07, 0.05, 0.07, 0.11, 0.02, 0.18, 0.08],\n",
      " [0.12, 0.14, 0.08, 0.06, 0.04, 0.04, 0.04, 0.13, 0.17, 0.18],\n",
      " [0.15, 0.04, 0.14, 0.06, 0.08, 0.09, 0.05, 0.08, 0.26, 0.05],\n",
      " [0.05, 0.04, 0.08, 0.11, 0.07, 0.32, 0.15, 0.11, 0.05, 0.01],\n",
      " [0.12, 0.05, 0.12, 0.16, 0.15, 0.11, 0.11, 0.11, 0.02, 0.06],\n",
      " [0.08, 0.10, 0.01, 0.00, 0.00, 0.00, 0.00, 0.01, 0.45, 0.34],\n",
      " [0.01, 0.07, 0.07, 0.12, 0.14, 0.14, 0.31, 0.10, 0.01, 0.03],\n",
      " [0.15, 0.07, 0.12, 0.06, 0.15, 0.09, 0.06, 0.13, 0.05, 0.12],\n",
      " [0.23, 0.00, 0.33, 0.08, 0.11, 0.09, 0.01, 0.13, 0.01, 0.01],\n",
      " [0.43, 0.05, 0.05, 0.03, 0.02, 0.04, 0.01, 0.03, 0.28, 0.05],\n",
      " [0.03, 0.15, 0.04, 0.10, 0.08, 0.10, 0.15, 0.15, 0.02, 0.19],\n",
      " [0.05, 0.03, 0.22, 0.05, 0.25, 0.08, 0.07, 0.20, 0.02, 0.03],\n",
      " [0.09, 0.17, 0.16, 0.03, 0.10, 0.03, 0.14, 0.07, 0.11, 0.09],\n",
      " [0.06, 0.05, 0.11, 0.11, 0.18, 0.12, 0.17, 0.15, 0.01, 0.04],\n",
      " [0.16, 0.06, 0.13, 0.04, 0.07, 0.05, 0.04, 0.13, 0.13, 0.18],\n",
      " [0.11, 0.24, 0.09, 0.06, 0.08, 0.04, 0.05, 0.07, 0.03, 0.23],\n",
      " [0.01, 0.04, 0.11, 0.09, 0.13, 0.11, 0.36, 0.10, 0.01, 0.04],\n",
      " [0.03, 0.10, 0.12, 0.12, 0.12, 0.12, 0.18, 0.14, 0.02, 0.06],\n",
      " [0.05, 0.02, 0.18, 0.14, 0.15, 0.21, 0.11, 0.12, 0.02, 0.02],\n",
      " [0.06, 0.05, 0.14, 0.15, 0.11, 0.20, 0.12, 0.09, 0.05, 0.02],\n",
      " [0.05, 0.16, 0.18, 0.06, 0.12, 0.03, 0.24, 0.12, 0.02, 0.03],\n",
      " [0.14, 0.09, 0.02, 0.03, 0.01, 0.02, 0.01, 0.04, 0.20, 0.44],\n",
      " [0.05, 0.16, 0.13, 0.10, 0.11, 0.15, 0.11, 0.12, 0.05, 0.03],\n",
      " [0.04, 0.05, 0.17, 0.14, 0.12, 0.12, 0.17, 0.11, 0.03, 0.06],\n",
      " [0.04, 0.23, 0.02, 0.03, 0.01, 0.03, 0.02, 0.02, 0.11, 0.49],\n",
      " [0.10, 0.22, 0.07, 0.08, 0.04, 0.05, 0.07, 0.08, 0.09, 0.19],\n",
      " [0.11, 0.02, 0.15, 0.14, 0.10, 0.27, 0.05, 0.08, 0.08, 0.01],\n",
      " [0.32, 0.07, 0.10, 0.04, 0.09, 0.03, 0.04, 0.09, 0.16, 0.05],\n",
      " [0.04, 0.03, 0.15, 0.08, 0.19, 0.14, 0.18, 0.14, 0.03, 0.02],\n",
      " [0.11, 0.08, 0.07, 0.19, 0.04, 0.09, 0.05, 0.08, 0.10, 0.21],\n",
      " [0.04, 0.05, 0.11, 0.14, 0.16, 0.14, 0.20, 0.12, 0.01, 0.03],\n",
      " [0.41, 0.03, 0.01, 0.00, 0.01, 0.00, 0.00, 0.04, 0.40, 0.09],\n",
      " [0.13, 0.13, 0.03, 0.06, 0.02, 0.05, 0.02, 0.05, 0.18, 0.34],\n",
      " [0.03, 0.09, 0.06, 0.17, 0.09, 0.22, 0.19, 0.07, 0.03, 0.06],\n",
      " [0.19, 0.08, 0.03, 0.09, 0.03, 0.06, 0.03, 0.05, 0.33, 0.11],\n",
      " [0.02, 0.03, 0.16, 0.07, 0.26, 0.10, 0.21, 0.13, 0.01, 0.01],\n",
      " [0.02, 0.03, 0.15, 0.08, 0.20, 0.08, 0.28, 0.11, 0.01, 0.04],\n",
      " [0.15, 0.11, 0.02, 0.01, 0.01, 0.00, 0.00, 0.01, 0.36, 0.32],\n",
      " [0.14, 0.13, 0.10, 0.03, 0.07, 0.04, 0.04, 0.09, 0.23, 0.12],\n",
      " [0.02, 0.10, 0.08, 0.13, 0.10, 0.16, 0.24, 0.08, 0.06, 0.04],\n",
      " [0.04, 0.08, 0.06, 0.13, 0.05, 0.15, 0.13, 0.11, 0.10, 0.15],\n",
      " [0.07, 0.07, 0.01, 0.01, 0.00, 0.01, 0.00, 0.00, 0.67, 0.14],\n",
      " [0.16, 0.16, 0.08, 0.04, 0.04, 0.04, 0.03, 0.03, 0.27, 0.15],\n",
      " [0.09, 0.02, 0.14, 0.11, 0.20, 0.21, 0.08, 0.11, 0.04, 0.01],\n",
      " [0.07, 0.10, 0.09, 0.07, 0.09, 0.12, 0.15, 0.05, 0.21, 0.05],\n",
      " [0.08, 0.13, 0.08, 0.11, 0.06, 0.09, 0.08, 0.09, 0.11, 0.17],\n",
      " [0.11, 0.04, 0.17, 0.13, 0.15, 0.15, 0.07, 0.11, 0.03, 0.03],\n",
      " [0.01, 0.01, 0.16, 0.08, 0.27, 0.10, 0.18, 0.18, 0.00, 0.01],\n",
      " [0.01, 0.01, 0.07, 0.23, 0.15, 0.23, 0.18, 0.10, 0.00, 0.01],\n",
      " [0.03, 0.16, 0.08, 0.09, 0.13, 0.09, 0.23, 0.09, 0.06, 0.04],\n",
      " [0.08, 0.10, 0.09, 0.04, 0.06, 0.07, 0.05, 0.07, 0.06, 0.37],\n",
      " [0.06, 0.11, 0.12, 0.15, 0.12, 0.08, 0.15, 0.10, 0.02, 0.09],\n",
      " [0.05, 0.03, 0.14, 0.12, 0.18, 0.13, 0.17, 0.13, 0.03, 0.02],\n",
      " [0.06, 0.48, 0.06, 0.02, 0.05, 0.02, 0.05, 0.04, 0.06, 0.17],\n",
      " [0.49, 0.01, 0.03, 0.01, 0.01, 0.01, 0.00, 0.02, 0.39, 0.02],\n",
      " [0.07, 0.06, 0.05, 0.14, 0.05, 0.19, 0.11, 0.10, 0.09, 0.14],\n",
      " [0.04, 0.11, 0.02, 0.02, 0.01, 0.01, 0.02, 0.04, 0.16, 0.56],\n",
      " [0.08, 0.07, 0.15, 0.12, 0.11, 0.12, 0.11, 0.15, 0.04, 0.05],\n",
      " [0.03, 0.05, 0.10, 0.12, 0.20, 0.18, 0.19, 0.10, 0.01, 0.02],\n",
      " [0.12, 0.19, 0.10, 0.08, 0.05, 0.06, 0.07, 0.05, 0.14, 0.15],\n",
      " [0.24, 0.04, 0.07, 0.04, 0.03, 0.07, 0.01, 0.04, 0.42, 0.04],\n",
      " [0.16, 0.06, 0.04, 0.02, 0.03, 0.02, 0.03, 0.08, 0.13, 0.43],\n",
      " [0.05, 0.05, 0.22, 0.05, 0.24, 0.06, 0.16, 0.11, 0.03, 0.04],\n",
      " [0.09, 0.17, 0.06, 0.07, 0.05, 0.06, 0.08, 0.07, 0.07, 0.29],\n",
      " [0.09, 0.09, 0.06, 0.15, 0.04, 0.23, 0.05, 0.04, 0.21, 0.05],\n",
      " [0.02, 0.12, 0.05, 0.21, 0.06, 0.19, 0.18, 0.06, 0.04, 0.08],\n",
      " [0.13, 0.38, 0.04, 0.03, 0.02, 0.02, 0.03, 0.03, 0.07, 0.25],\n",
      " [0.21, 0.05, 0.10, 0.05, 0.03, 0.06, 0.02, 0.05, 0.30, 0.13],\n",
      " [0.03, 0.20, 0.07, 0.14, 0.06, 0.19, 0.11, 0.05, 0.10, 0.06],\n",
      " [0.05, 0.12, 0.24, 0.09, 0.15, 0.10, 0.16, 0.03, 0.02, 0.04],\n",
      " [0.19, 0.08, 0.14, 0.11, 0.06, 0.12, 0.03, 0.09, 0.10, 0.08],\n",
      " [0.26, 0.07, 0.19, 0.09, 0.05, 0.09, 0.02, 0.07, 0.09, 0.06],\n",
      " [0.31, 0.03, 0.08, 0.02, 0.05, 0.02, 0.01, 0.14, 0.15, 0.20],\n",
      " [0.35, 0.02, 0.09, 0.05, 0.04, 0.09, 0.01, 0.06, 0.26, 0.04],\n",
      " [0.11, 0.11, 0.07, 0.04, 0.02, 0.04, 0.01, 0.03, 0.38, 0.19],\n",
      " [0.20, 0.03, 0.11, 0.07, 0.03, 0.06, 0.03, 0.07, 0.33, 0.07],\n",
      " [0.15, 0.17, 0.04, 0.03, 0.04, 0.04, 0.03, 0.04, 0.16, 0.30],\n",
      " [0.17, 0.17, 0.05, 0.04, 0.04, 0.05, 0.02, 0.04, 0.21, 0.23],\n",
      " [0.02, 0.03, 0.16, 0.11, 0.18, 0.18, 0.17, 0.11, 0.02, 0.02],\n",
      " [0.17, 0.10, 0.04, 0.03, 0.01, 0.03, 0.01, 0.01, 0.52, 0.09],\n",
      " [0.18, 0.04, 0.23, 0.05, 0.11, 0.04, 0.08, 0.08, 0.11, 0.09],\n",
      " [0.13, 0.07, 0.12, 0.08, 0.12, 0.10, 0.09, 0.11, 0.11, 0.08],\n",
      " [0.06, 0.05, 0.11, 0.18, 0.08, 0.14, 0.12, 0.12, 0.07, 0.07],\n",
      " [0.03, 0.05, 0.14, 0.14, 0.17, 0.14, 0.20, 0.11, 0.02, 0.02],\n",
      " [0.26, 0.05, 0.12, 0.10, 0.05, 0.12, 0.03, 0.07, 0.17, 0.04],\n",
      " [0.16, 0.03, 0.25, 0.11, 0.10, 0.07, 0.03, 0.20, 0.02, 0.02],\n",
      " [0.10, 0.06, 0.10, 0.10, 0.07, 0.07, 0.07, 0.19, 0.04, 0.21]]\n",
      "the result is 38.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mresult\u001b[39m: \u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m.\u001b[32m<refinement>\u001b[39m.this.type.\u001b[32mOutputData\u001b[39m = [[0.08, 0.10, 0.14, 0.17, 0.06, 0.16, 0.13, 0.03, 0.10, 0.02],\n",
       " [0.06, 0.21, 0.02, 0.01, 0.00, 0.01, 0.01, 0.01, 0.26, 0.42],\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mright\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m38.0\u001b[39m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = myNeuralNetwork.predict(testData)\n",
    "println(s\"result: $result\") //输出判断结果\n",
    "\n",
    "val right = Utils.getAccuracy(result, testExpectResult)\n",
    "println(s\"the result is $right %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[完整代码](https://github.com/izhangzhihao/deeplearning-tutorial/blob/master/src/main/scala/com/thoughtworks/deeplearning/tutorial/MiniBatchGradientDescent.scala)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
