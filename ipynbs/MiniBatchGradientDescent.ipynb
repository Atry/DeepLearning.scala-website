{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景\n",
    "\n",
    "通过使用小批量数据随机梯度下降快速实现神经网络参数更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "\n",
    "1.创建一个SBT项目，并引入相关依赖（参照[Getting Started](https://github.com/ThoughtWorksInc/DeepLearning.scala/wiki/Getting-Started) 或者将下面的依赖引入build.sbt, 注意DeepLearning.scala暂不支持scala2.12.X )\n",
    "```\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiableany\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiablenothing\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiableseq\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiabledouble\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiablefloat\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiablehlist\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiablecoproduct\" % \"latest.release\"\n",
    "\n",
    "libraryDependencies += \"com.thoughtworks.deeplearning\" %% \"differentiableindarray\" % \"latest.release\"\n",
    "\n",
    "addCompilerPlugin(\"com.thoughtworks.implicit-dependent-type\" %% \"implicit-dependent-type\" % \"latest.release\")\n",
    "\n",
    "addCompilerPlugin(\"org.scalamacros\" % \"paradise\" % \"2.1.0\" cross CrossVersion.full)\n",
    "\n",
    "fork := true\n",
    "```\n",
    "2.[下载CIFAR-10 binary version (suitable for C programs)](https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz)，文件大小162 MB，md5sum：c32a1d4ab5d03f1284b67883e8d87530\n",
    "\n",
    "3.将下载好的文件解压到src/main/resources目录。\n",
    "\n",
    "4.新建一个Scala类ReadCIFAR10ToNDArray,这个类用于从上面的文件中读取图片及其标签数据并做归一化处理（[更多信息](https://www.cs.toronto.edu/~kriz/cifar.html)）,并增加若干方法和属性。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                 \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.{FileInputStream, InputStream}\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableHList._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableDouble._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableAny._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.{DifferentiableHList, DifferentiableINDArray, Layer}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Layer.Batch\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Lift.Layers.Identity\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Lift._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathFunctions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathMethods./\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathOps\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.cpu.nativecpu.NDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.ops.transforms.Transforms\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4s.Implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mshapeless._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.immutable.IndexedSeq\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mobject\u001b[39m \u001b[36mReadCIFAR10ToNDArray\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $plugin.$ivy.`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:1.0.0`\n",
    "\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableany:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablenothing:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableseq:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiabledouble:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablefloat:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablehlist:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0-RC5`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableindarray:1.0.0-RC5`\n",
    "\n",
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.0`\n",
    "\n",
    "import java.io.{FileInputStream, InputStream}\n",
    "\n",
    "\n",
    "import com.thoughtworks.deeplearning\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import com.thoughtworks.deeplearning.DifferentiableHList._\n",
    "import com.thoughtworks.deeplearning.DifferentiableDouble._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray._\n",
    "import com.thoughtworks.deeplearning.DifferentiableAny._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
    "import com.thoughtworks.deeplearning.{DifferentiableHList, DifferentiableINDArray, Layer}\n",
    "import com.thoughtworks.deeplearning.Layer.Batch\n",
    "import com.thoughtworks.deeplearning.Lift.Layers.Identity\n",
    "import com.thoughtworks.deeplearning.Lift._\n",
    "import com.thoughtworks.deeplearning.Poly.MathFunctions._\n",
    "import com.thoughtworks.deeplearning.Poly.MathMethods./\n",
    "import com.thoughtworks.deeplearning.Poly.MathOps\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.cpu.nativecpu.NDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
    "import org.nd4j.linalg.ops.transforms.Transforms\n",
    "import org.nd4s.Implicits._\n",
    "import shapeless._\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._\n",
    "\n",
    "import scala.collection.immutable.IndexedSeq\n",
    "\n",
    "pprintConfig() = pprintConfig().copy(height = 5)//减少输出的行数，避免页面输出太长\n",
    "\n",
    "object ReadCIFAR10ToNDArray {\n",
    "\n",
    "  /**\n",
    "    * 从CIFAR10文件中读图片和其对应的标签\n",
    "    *\n",
    "    * @param fileName CIFAR10文件名\n",
    "    * @param count    要读取多少个图片和其标签\n",
    "    * @return input :: expectedOutput :: HNil\n",
    "    */\n",
    "  def readFromResource(fileName: String,\n",
    "                       count: Int): INDArray :: INDArray :: HNil = {\n",
    "    //if you are using IDE\n",
    "    //val inputStream = getClass.getResourceAsStream(fileName)\n",
    "\n",
    "    //if you are using jupyter notebook,please use this\n",
    "    val inputStream = new FileInputStream(sys.env(\"PWD\") + \"/src/main/resources\" + fileName)\n",
    "    try {\n",
    "      val bytes = Array.range(0, 3073 * count).map(_.toByte)\n",
    "      inputStream.read(bytes)\n",
    "\n",
    "      val labels: Seq[Double] = for {\n",
    "        index <- 0 until count\n",
    "      } yield bytes(index * 3073).toDouble\n",
    "\n",
    "      val pixels: Seq[Seq[Double]] =\n",
    "        for (index <- 0 until count)\n",
    "          yield {\n",
    "            for {\n",
    "              item <- 1 until 3073\n",
    "            } yield normalizePixel(bytes(index * 3073 + item).toDouble)\n",
    "          }\n",
    "\n",
    "      val labelsArray = labels.toNDArray.reshape(count, 1)\n",
    "      val pixelsArray = pixels.toNDArray\n",
    "\n",
    "      pixelsArray :: labelsArray :: HNil\n",
    "    } finally {\n",
    "      inputStream.close()\n",
    "    }\n",
    "  }\n",
    "\n",
    "    /**\n",
    "    * 归一化pixel数据\n",
    "    *\n",
    "    * @param pixel\n",
    "    * @return\n",
    "    */\n",
    "  def normalizePixel(pixel: Double): Double = {\n",
    "    (if (pixel < 0) {\n",
    "      pixel + 256\n",
    "    } else {\n",
    "      pixel\n",
    "    }) / 256\n",
    "  }\n",
    "\n",
    "  /**\n",
    "    * 目的是将存储数据的文件读取到内存中，以后就从这里拿数据，避免了频繁的读取文件。\n",
    "    *\n",
    "    */\n",
    "  lazy val fileBytesSeq: IndexedSeq[Array[Byte]] = {\n",
    "    for {\n",
    "      fileIndex <- 1 to 5\n",
    "      //if you are using IDE\n",
    "      //inputStream = getClass.getResourceAsStream(\"/cifar-10-batches-bin/data_batch_\" + fileIndex + \".bin\")\n",
    "\n",
    "      //if you are using jupyter notebook,please use this\n",
    "      inputStream = new FileInputStream(sys.env(\"PWD\") + \"/src/main/resources\" + \"/cifar-10-batches-bin/data_batch_\" + fileIndex + \".bin\")\n",
    "    } yield readFromInputStream(inputStream)\n",
    "  }\n",
    "\n",
    "  /**\n",
    "    * 从inputStream中读取byte\n",
    "    *\n",
    "    * @param inputStream\n",
    "    * @return\n",
    "    */\n",
    "  def readFromInputStream(inputStream: InputStream): Array[Byte] = {\n",
    "    try {\n",
    "      val bytes = Array.range(0, 3073 * 10000).map(_.toByte)\n",
    "      inputStream.read(bytes)\n",
    "      bytes\n",
    "    } finally {\n",
    "      inputStream.close()\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /**\n",
    "    * 随机获取count个train数据\n",
    "    *\n",
    "    * @return\n",
    "    */\n",
    "  def getSGDTrainNDArray(count: Int): INDArray :: INDArray :: HNil = {\n",
    "    //生成0到4的随机数\n",
    "    val randomIndex = (new util.Random).nextInt(5)\n",
    "\n",
    "    val bytes = fileBytesSeq(randomIndex)\n",
    "\n",
    "    val indexList = randomList(10000, count)\n",
    "\n",
    "    val labels: Seq[Double] = for (index <- 0 until count)\n",
    "      yield bytes(indexList(index) * 3073).toDouble\n",
    "\n",
    "    val pixels: Seq[Seq[Double]] = for (index <- 0 until count)\n",
    "      yield {\n",
    "        for (pixelItem <- 1 until 3073)\n",
    "          yield normalizePixel(bytes(indexList(index) * 3073 + pixelItem).toDouble)\n",
    "      }\n",
    "\n",
    "    val labelsNDArray = labels.toNDArray.reshape(count, 1)\n",
    "    val pixelsNDArray = pixels.toNDArray\n",
    "\n",
    "    pixelsNDArray :: labelsNDArray :: HNil\n",
    "  }\n",
    "\n",
    "  /**\n",
    "    * 从固定范围内获取count个数字组成的集合\n",
    "    *\n",
    "    * @param arrange 范围\n",
    "    * @param count   个数\n",
    "    * @return\n",
    "    */\n",
    "  def randomList(arrange: Int, count: Int): List[Int] = {\n",
    "    var resultList: List[Int] = Nil\n",
    "    while (resultList.length < count) {\n",
    "      val randomNum = (new util.Random).nextInt(arrange)\n",
    "      if (!resultList.contains(randomNum)) {\n",
    "        resultList = resultList ::: List(randomNum)\n",
    "      }\n",
    "    }\n",
    "    resultList\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "5.[Mini-Batch Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent): 在大规模数据训练时，数据可以达到百万级量级。如果计算整个训练集，来获得仅仅一个参数的更新速度就太慢了。一个常用的方法是计算训练集中的小批量（batches）数据以提升参数更新速度。\n",
    "\n",
    "   \n",
    "6.如果你使用IntelliJ或者eclipse等其它IDE，智能提示可能会失效，代码有部分可能会爆红，这是IDE的问题，代码本身并无问题。\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 构建神经网络\n",
    "\n",
    "1.新建一个Scala类MiniBatchGradientDescent\n",
    "\n",
    "2.从CIFAR10 database中读取测试数据的图片和标签信息，注意：这里和与SoftmaxLinearClassifier中不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mCLASSES\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36mtestNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.55, 0.55, 0.56, 0.54, 0.49, 0.45, 0.59, 0.59, 0.62, 0.65, 0.63, 0.62, 0.64, 0.63, 0.64, 0.61, 0.61, 0.62, 0.64, 0.66, 0.67, 0.67, 0.66, 0.62, 0.60, 0.59, 0.57, 0.54, 0.55, 0.55, 0.58, 0.57, 0.57, 0.55, 0.56, 0.53, 0.49, 0.46, 0.59, 0.59, 0.\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CIFAR10中的图片共有10个分类(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
    "  val CLASSES: Int = 10\n",
    "\n",
    "  //加载测试数据，我们读取100条作为测试数据\n",
    "  val testNDArray =\n",
    "    ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/test_batch.bin\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.编写处理标签数据的工具方法，将N行一列的NDArray转换为N行CLASSES列的NDArray，每行对应的正确分类的值为1，其它列的值为0。这样做是为了向cross-entropy loss公式靠拢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmakeVectorized\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  /**\n",
    "    * 处理标签数据：将N行一列的NDArray转换为N行CLASSES列的NDArray，每行对应的正确分类的值为1，其它列的值为0\n",
    "    *\n",
    "    * @param ndArray 标签数据\n",
    "    * @return N行CLASSES列的NDArray\n",
    "    */\n",
    "  def makeVectorized(ndArray: INDArray): INDArray = {\n",
    "    val shape = ndArray.shape()\n",
    "\n",
    "    val p = Nd4j.zeros(shape(0), CLASSES)\n",
    "    for (i <- 0 until shape(0)) {\n",
    "      val double = ndArray.getDouble(i, 0)\n",
    "      val column = double.toInt\n",
    "      p.put(i, column, 1)\n",
    "    }\n",
    "    p\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.分离和处理图像和标签数据，注意：这里和与SoftmaxLinearClassifier中不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtest_data\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.55, 0.55, 0.56, 0.54, 0.49, 0.45, 0.59, 0.59, 0.62, 0.65, 0.63, 0.62, 0.64, 0.63, 0.64, 0.61, 0.61, 0.62, 0.64, 0.66, 0.67, 0.67, 0.66, 0.62, 0.60, 0.59, 0.57, 0.54, 0.55, 0.55, 0.58, 0.57, 0.57, 0.55, 0.56, 0.53, 0.49, 0.46, 0.59, 0.59, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtest_expect_result\u001b[39m: \u001b[32mINDArray\u001b[39m = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.00, 0.00, 9.00, 6.00, 6.00, 5.00, 4.00, 5.00, 9.00, 2.00, 4.00, 1.00, 9.00, 5.00, 4.00, 6.00, 5.00, 6.00, 0.00, 9.00, 3.00, 9.00, 7.00, 6.00, 9.00, 8.00, 0.00, 3.00, 8.00, 8.00, 7.00, 7.00, 4.00, 6.00, 7.00, 3.00, 6.00, 3.00, 6.00, 2.00, 1.0\u001b[33m...\u001b[39m\n",
       "\u001b[36mtest_p\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       " [1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  val test_data = testNDArray.head\n",
    "\n",
    "  val test_expect_result = testNDArray.tail.head\n",
    "  \n",
    "  val test_p = makeVectorized(test_expect_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.编写softmax函数,和准备一节中的softmax公式对应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msoftmax\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(implicit scores: From[INDArray] ##T): To[INDArray] ##T = {\n",
    "  val expScores = exp(scores)\n",
    "  expScores / expScores.sum(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.设置学习率，学习率是Weight变化的快慢的直观描述，学习率设置的过小会导致loss下降的很慢，需要更长时间来训练，学习率设置的过大虽然刚开始下降很快但是会导致在接近最低点的时候在附近徘徊loss下降会非常慢。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36moptimizer\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit def optimizer: Optimizer = new LearningRate {\n",
    "    def currentLearningRate() = 0.00001\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.跟定义一个方法一样定义一个神经网络并初始化Weight，Weight应该是一个N*CLASSES的INDArray,每个图片对应每个分类都有一个评分。[什么是Weight](https://github.com/ThoughtWorksInc/DeepLearning.scala/wiki/Getting-Started#231--weight-intialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcreateMyNeuralNetwork\u001b[39m\n",
       "\u001b[36mmyNeuralNetwork\u001b[39m: (\u001b[32mFromTo\u001b[39m[\u001b[32mINDArray\u001b[39m, \u001b[32mINDArray\u001b[39m]{type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray;type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray})#\u001b[32mT\u001b[39m = Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum(Exp(Identity()),WrappedArray(1)))),Dot(Identity(),Weight([[0.00, 0.00, -0.00, -0.00, -0.00, 0.00, -0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, -0.00, -0.00, 0.00, -0.00, 0.00, 0.00, 0.00, -0.00, 0.00],\n",
       " [0.00, -0.00, 0.00, 0.00, 0.00, 0.00, -0.00, -0.00, 0.00, 0.00],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  def createMyNeuralNetwork(implicit input: From[INDArray] ##T): To[INDArray] ##T = {\n",
    "    val initialValueOfWeight = Nd4j.randn(3072, CLASSES) * 0.001\n",
    "    val weight: To[INDArray] ##T = initialValueOfWeight.toWeight\n",
    "    val result: To[INDArray] ##T = input dot weight\n",
    "    softmax.compose(result) //对结果调用softmax方法，压缩结果值在0到1之间方便处理\n",
    "  }\n",
    "  val myNeuralNetwork: FromTo[INDArray, INDArray] ##T = createMyNeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.编写损失函数Loss Function，将此次判断的结果和真实结果进行计算得出cross-entropy loss并返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlossFunction\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  def lossFunction(implicit pair: From[INDArray :: INDArray :: HNil] ##T): To[Double] ##T = {\n",
    "    val input = pair.head\n",
    "    val expectedOutput = pair.tail.head\n",
    "    val probabilities = myNeuralNetwork.compose(input)\n",
    "\n",
    "    -(expectedOutput * log(probabilities)).sum //此处和准备一节中的交叉熵损失对应\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.训练神经网络并观察每次训练loss的变化，loss的变化趋势是降低，但是不是每次都降低(前途是光明的，道路是曲折的)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-787930328\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0,1989.0,1990.0,1991.0,1992.0,1993.0,1994.0,1995.0,1996.0,1997.0,1998.0,1999.0],\"y\":[588.948974609375,587.7423095703125,588.75634765625,588.4892578125,586.6365356445312,586.7796630859375,585.5419311523438,585.3945922851562,584.4492797851562,581.2523193359375,580.7616577148438,582.463623046875,582.5064086914062,581.2138671875,582.6673583984375,581.189208984375,578.5361328125,580.666015625,578.47802734375,580.7716064453125,578.5517578125,579.4537353515625,579.6082153320312,578.6353759765625,576.0484619140625,575.9508056640625,576.0737915039062,575.02685546875,574.9696655273438,574.3417358398438,574.079345703125,575.5389404296875,574.2355346679688,569.593994140625,572.454345703125,571.89404296875,577.4503784179688,569.5311279296875,573.353515625,569.683349609375,569.6356811523438,575.0604248046875,566.9161376953125,571.9097900390625,572.3927612304688,568.335205078125,567.6263427734375,568.5746459960938,569.2099609375,568.9041137695312,565.2112426757812,566.5507202148438,565.2484741210938,568.2814331054688,561.8182983398438,567.8231201171875,564.7928466796875,570.7237548828125,564.209228515625,570.3850708007812,565.911865234375,567.714111328125,563.8443603515625,563.73388671875,563.8028564453125,565.4541015625,562.1467895507812,567.442138671875,555.6959228515625,560.4432373046875,553.5845336914062,554.4785766601562,558.6402587890625,559.5726318359375,554.318359375,557.7958984375,552.5825805664062,555.5618896484375,551.2890625,557.1512451171875,556.8524169921875,553.4868774414062,553.9309692382812,564.4556274414062,556.5936889648438,555.7161254882812,555.995361328125,563.44140625,560.085693359375,561.1575927734375,553.8062744140625,553.5128173828125,551.3209228515625,553.4696044921875,546.1876220703125,549.6956787109375,551.4837646484375,548.9742431640625,556.0877075195312,556.015869140625,554.7254638671875,550.6226196289062,554.0836181640625,557.1722412109375,543.1903686523438,555.4913330078125,551.9119262695312,553.4862060546875,551.8515625,546.5205078125,548.452880859375,552.337890625,545.202392578125,551.86572265625,548.9580078125,546.52978515625,555.935791015625,551.5897216796875,552.353515625,548.2410888671875,542.0074462890625,540.8341674804688,546.3836669921875,554.0198364257812,551.4703369140625,545.752685546875,543.525146484375,541.780029296875,556.447509765625,549.4129028320312,546.4410400390625,544.399658203125,535.7800903320312,551.442138671875,538.7723388671875,545.52490234375,538.8375244140625,546.440673828125,540.984619140625,550.45654296875,544.846435546875,544.4224853515625,546.416015625,551.86279296875,548.0228271484375,545.8120727539062,543.1197509765625,542.969482421875,547.6962890625,532.5897827148438,536.628173828125,538.254150390625,539.9032592773438,545.0406494140625,533.064697265625,543.2200927734375,532.3450317382812,537.3677368164062,547.072509765625,539.0194091796875,552.1038208007812,540.00390625,530.224365234375,541.0315551757812,535.5396118164062,546.9716796875,533.3629150390625,527.6340942382812,541.375,543.43115234375,525.062255859375,534.910400390625,534.9442749023438,536.132080078125,538.5291748046875,544.028564453125,533.9675903320312,532.3850708007812,539.6580810546875,531.1424560546875,530.0271606445312,541.0589599609375,540.73583984375,533.887939453125,530.1956176757812,540.2742919921875,528.371337890625,529.5513305664062,542.4425048828125,540.704345703125,532.1749267578125,537.5576171875,539.3612670898438,522.6505126953125,528.6547241210938,531.4938354492188,527.875732421875,536.208740234375,543.6353759765625,530.60546875,538.3643798828125,535.6890258789062,538.0751342773438,538.2868041992188,521.7967529296875,525.3994140625,525.4577026367188,538.52587890625,535.767333984375,541.7808837890625,539.7110595703125,531.6842041015625,541.0521850585938,528.281005859375,539.6033935546875,525.066162109375,536.0892333984375,543.1996459960938,528.1800537109375,533.5594482421875,536.1600341796875,533.7142333984375,535.122802734375,529.8384399414062,531.7487182617188,531.9763793945312,537.4351196289062,534.9935913085938,530.0478515625,521.8007202148438,529.25830078125,528.4205322265625,522.5441284179688,540.86474609375,533.8629150390625,521.610107421875,527.8524169921875,519.4611206054688,534.3035888671875,524.178466796875,541.187744140625,524.8084716796875,528.3179931640625,526.9437255859375,528.5933837890625,528.8768310546875,538.9454345703125,521.6973876953125,525.0406494140625,531.771728515625,516.3974609375,539.0401611328125,510.6236267089844,531.0036010742188,526.9298095703125,517.0767822265625,519.2835693359375,532.4842529296875,512.6014404296875,520.840087890625,513.70166015625,527.318603515625,527.0272216796875,524.0811767578125,522.3207397460938,521.084228515625,520.4822387695312,526.5836791992188,518.5775146484375,540.6730346679688,527.4487915039062,519.77490234375,526.7890625,531.5695190429688,538.2550048828125,514.0714721679688,521.5750732421875,514.0537109375,527.917236328125,529.2896118164062,525.8939208984375,523.7197875976562,536.04736328125,528.182373046875,531.3336181640625,527.1021118164062,525.1577758789062,522.1258544921875,515.3881225585938,516.21337890625,518.965087890625,529.2858276367188,512.1749267578125,524.5889282226562,521.1429443359375,521.2318725585938,510.55584716796875,523.6109008789062,523.724853515625,530.25244140625,525.71875,527.785400390625,517.2976684570312,526.2848510742188,522.9237060546875,513.8247680664062,510.07000732421875,514.2706298828125,510.12860107421875,521.1226806640625,520.6505126953125,510.4940185546875,523.337890625,533.3785400390625,536.1779174804688,525.7288208007812,516.5770874023438,517.1359252929688,521.2857055664062,523.712646484375,518.9847412109375,536.8760986328125,526.8236083984375,513.606201171875,526.70703125,507.9147033691406,526.49462890625,513.3538208007812,519.9010009765625,526.8900146484375,523.4942016601562,527.0164184570312,531.072265625,537.4375610351562,525.255615234375,522.7483520507812,526.6162719726562,517.8259887695312,524.1102294921875,530.217529296875,523.6091918945312,513.068603515625,524.2799682617188,522.354248046875,513.0010375976562,518.2244873046875,516.345703125,515.1838989257812,513.32568359375,514.48193359375,510.30743408203125,500.06494140625,517.8988037109375,524.5411376953125,518.40283203125,518.2222290039062,520.569580078125,526.419189453125,509.5335388183594,506.8264465332031,518.7325439453125,521.2464599609375,524.08935546875,511.3814392089844,509.5557556152344,517.3359375,518.67626953125,512.8021240234375,515.452392578125,523.9312133789062,513.7647094726562,514.4844970703125,501.1637268066406,519.332763671875,524.7610473632812,517.49560546875,517.0543823242188,508.32733154296875,511.578857421875,521.012939453125,520.0926513671875,520.5043334960938,501.0585021972656,507.7392272949219,518.3404541015625,523.7752075195312,535.9908447265625,516.85888671875,513.063232421875,517.4266357421875,529.43310546875,505.390869140625,506.2220458984375,514.2470703125,530.3077392578125,516.1080932617188,519.310791015625,502.54449462890625,508.4617919921875,517.9586791992188,524.662109375,509.62103271484375,492.26751708984375,506.4375,519.15869140625,511.04559326171875,514.6980590820312,512.8306274414062,514.6334228515625,515.6676025390625,512.7488403320312,524.9486694335938,504.04010009765625,519.4070434570312,507.73992919921875,513.6074829101562,519.0718994140625,501.0486145019531,506.78790283203125,517.9132080078125,506.96942138671875,519.2388305664062,504.60443115234375,524.0311279296875,514.386962890625,513.5869140625,533.045166015625,506.2578125,521.0352783203125,507.7430114746094,516.782958984375,505.3212585449219,513.0668334960938,501.86590576171875,520.516845703125,518.9434814453125,524.5621337890625,509.4563903808594,513.948486328125,516.2235107421875,521.821533203125,500.5140075683594,510.9248046875,514.7190551757812,513.9974365234375,513.5572509765625,509.3956604003906,499.0810852050781,513.2200317382812,506.9197692871094,512.9031982421875,506.4794616699219,518.0488891601562,512.3277587890625,486.6378479003906,523.5985107421875,512.55859375,526.472900390625,509.35546875,519.99560546875,514.4149780273438,490.98077392578125,500.2404479980469,519.1973266601562,501.92724609375,520.4273681640625,503.7196350097656,519.8543701171875,512.6038818359375,503.78076171875,504.6864013671875,500.7315979003906,526.9570922851562,513.4449462890625,512.6268310546875,515.6525268554688,513.5352783203125,516.8203735351562,505.9708251953125,511.4526062011719,505.0469970703125,491.47174072265625,510.7156677246094,510.2221374511719,501.7168273925781,504.78802490234375,514.9906005859375,517.1041259765625,503.3341979980469,512.6011352539062,498.971435546875,508.74737548828125,500.5090026855469,509.6289978027344,514.1974487304688,517.8317260742188,518.7698974609375,508.08453369140625,502.9580383300781,505.67529296875,497.4721374511719,508.4276123046875,505.3782043457031,508.21514892578125,513.0579223632812,520.6143798828125,512.3770751953125,501.3651123046875,504.6668701171875,498.40240478515625,507.8056640625,507.92913818359375,504.9234619140625,507.0865173339844,511.92138671875,507.968994140625,504.8740539550781,518.860107421875,492.2283935546875,516.2244873046875,515.0154418945312,521.0880126953125,506.595703125,501.61163330078125,498.3896179199219,511.9931945800781,496.2477722167969,512.9631958007812,497.9525451660156,507.09814453125,501.93902587890625,511.26300048828125,495.8397521972656,491.878173828125,512.604736328125,493.1414794921875,500.0022277832031,514.4297485351562,508.99249267578125,495.6668395996094,509.391845703125,514.0859375,511.3988037109375,511.9716796875,512.6051025390625,501.47503662109375,518.150390625,521.3264770507812,496.72802734375,499.4805908203125,501.8478698730469,502.80206298828125,514.9476318359375,496.83154296875,518.6492919921875,496.3528747558594,529.0769653320312,508.267333984375,508.6786804199219,509.73675537109375,509.11212158203125,499.03875732421875,503.3394775390625,520.7080078125,517.5850830078125,497.6650085449219,504.79736328125,512.9544677734375,500.5675048828125,478.3792724609375,491.33477783203125,513.7839965820312,497.4179992675781,502.93902587890625,501.6685791015625,514.00927734375,496.9237060546875,504.031982421875,502.0489807128906,501.13690185546875,490.78125,481.436279296875,501.53204345703125,498.49493408203125,500.1675720214844,515.802001953125,513.9205932617188,483.5909729003906,505.8740234375,509.2584228515625,500.25567626953125,497.46746826171875,511.4769287109375,495.115234375,503.557861328125,494.3480224609375,489.39813232421875,509.816650390625,498.9015197753906,513.8380126953125,503.97015380859375,505.2333068847656,492.14862060546875,502.121826171875,490.9178466796875,501.484375,497.9940185546875,497.10552978515625,532.86474609375,508.01702880859375,520.3184814453125,495.71697998046875,496.3590393066406,494.25103759765625,489.43994140625,503.5589294433594,508.429443359375,497.0454406738281,519.8172607421875,489.38714599609375,521.0853881835938,494.3008117675781,520.842529296875,487.75518798828125,499.2118835449219,496.2951354980469,506.90411376953125,491.94537353515625,492.0782165527344,504.8002014160156,512.9134521484375,502.685791015625,468.0439147949219,508.4386901855469,498.4412536621094,497.91925048828125,501.1829528808594,511.591064453125,501.0733642578125,498.28057861328125,502.43255615234375,493.5966796875,491.0066833496094,507.3575439453125,490.1168212890625,509.3794860839844,496.18853759765625,505.3936462402344,483.13067626953125,502.54608154296875,505.9208068847656,504.8128356933594,510.0018310546875,509.909423828125,500.3551025390625,502.1605224609375,502.4131774902344,488.8132629394531,509.8346862792969,501.1009521484375,491.88653564453125,498.829833984375,505.4040222167969,502.9031982421875,509.5168151855469,487.5604248046875,501.7138671875,499.35638427734375,509.24859619140625,508.8155822753906,497.34368896484375,505.1199951171875,491.6929931640625,501.9039306640625,502.2264404296875,494.91754150390625,497.8829650878906,489.64129638671875,497.5443115234375,516.2886352539062,489.0364990234375,500.6201477050781,492.45477294921875,509.0402526855469,510.9268493652344,504.5944519042969,515.9742431640625,493.83868408203125,496.9097595214844,490.88458251953125,511.4867858886719,474.7112731933594,504.04522705078125,484.2186279296875,493.97235107421875,492.71160888671875,496.97845458984375,514.3167724609375,501.07171630859375,492.750244140625,482.3272705078125,505.9065856933594,513.4338989257812,499.6722412109375,499.1414794921875,493.586181640625,507.05108642578125,501.76336669921875,492.0472412109375,500.44830322265625,503.1708984375,493.299560546875,498.615966796875,479.74041748046875,490.52984619140625,480.1716613769531,500.06689453125,496.8703308105469,495.19012451171875,494.49603271484375,505.5458679199219,509.74365234375,481.54583740234375,495.75604248046875,500.96600341796875,505.39404296875,508.82403564453125,494.1204528808594,480.12841796875,497.822998046875,488.3650207519531,490.2452392578125,496.224853515625,492.5173645019531,498.4432067871094,495.7303466796875,499.9990234375,499.84344482421875,504.5294494628906,497.328369140625,504.44036865234375,507.7891540527344,497.3033142089844,505.2912292480469,498.8536682128906,521.228271484375,506.2644348144531,485.5300598144531,509.3127746582031,499.3143615722656,501.4467468261719,487.1664733886719,498.93963623046875,499.91912841796875,503.71392822265625,503.3462829589844,503.8294677734375,500.1912841796875,490.47332763671875,500.0030822753906,505.6966247558594,488.54913330078125,494.0047607421875,506.6200256347656,502.1316223144531,518.2039794921875,507.4967956542969,494.26751708984375,497.5967102050781,500.39666748046875,496.93438720703125,481.90264892578125,490.0281982421875,509.21954345703125,493.6600646972656,495.53173828125,496.433349609375,506.9323425292969,493.4834289550781,483.4552001953125,494.06805419921875,499.8695373535156,503.3915100097656,491.7060241699219,486.92041015625,496.9548645019531,507.4905700683594,498.52447509765625,488.999755859375,500.5423583984375,510.81072998046875,488.6402893066406,512.4973754882812,508.5697021484375,492.15765380859375,479.1695556640625,503.03985595703125,484.30224609375,492.25335693359375,497.8245849609375,482.1524658203125,506.16644287109375,503.8523864746094,504.08953857421875,483.7421875,493.66046142578125,508.4949035644531,481.59521484375,499.69293212890625,492.054931640625,490.92901611328125,494.6898193359375,496.77197265625,489.3194274902344,495.88909912109375,501.743896484375,497.00054931640625,496.7286376953125,506.7250671386719,495.4862060546875,493.0095520019531,485.5423889160156,511.4046936035156,484.939208984375,504.62835693359375,496.27197265625,485.7106018066406,485.8168029785156,476.507080078125,497.669677734375,494.586669921875,488.0664978027344,500.52374267578125,461.835205078125,485.84417724609375,503.8688659667969,496.14886474609375,485.5113830566406,496.09735107421875,498.85369873046875,501.42767333984375,495.1575622558594,487.0340881347656,496.248779296875,519.7821044921875,502.6595458984375,507.04058837890625,515.83203125,492.7724914550781,494.9141540527344,491.0574951171875,504.3567810058594,491.0112609863281,502.3005676269531,496.1600036621094,491.1264953613281,485.73193359375,482.89599609375,483.08367919921875,489.1510314941406,517.6318969726562,489.22998046875,506.3656005859375,490.6059265136719,489.439453125,493.5625305175781,494.444091796875,508.824462890625,477.97900390625,498.69891357421875,498.0614013671875,496.33782958984375,495.26629638671875,493.2559814453125,472.41851806640625,490.27386474609375,481.62493896484375,493.92529296875,508.2613220214844,482.565673828125,501.1890563964844,472.43255615234375,502.44183349609375,490.17529296875,504.1366882324219,493.69085693359375,503.7540283203125,500.18701171875,488.7710876464844,488.82476806640625,498.7347106933594,504.23095703125,484.174072265625,505.4768981933594,509.04901123046875,489.0393981933594,491.0517578125,482.18475341796875,495.09271240234375,493.8836975097656,506.0250244140625,491.8026123046875,498.4186096191406,506.4354248046875,509.41363525390625,490.9028015136719,506.38690185546875,504.6829833984375,481.7704162597656,502.4577941894531,492.5857238769531,486.8039245605469,495.59588623046875,491.48663330078125,484.74407958984375,496.11700439453125,495.8425598144531,485.1656494140625,489.007080078125,499.2437744140625,481.33587646484375,515.7510986328125,484.919921875,487.5732727050781,491.4567565917969,506.55853271484375,495.4324951171875,487.7097473144531,487.94659423828125,505.6844482421875,481.8743896484375,498.4471130371094,495.23211669921875,499.2955322265625,490.20843505859375,493.4159851074219,500.60589599609375,488.03106689453125,495.15484619140625,497.9565124511719,490.3375549316406,484.1044921875,477.949951171875,500.5328674316406,520.1716918945312,485.8648681640625,491.79852294921875,487.4081115722656,489.4254455566406,476.57958984375,475.59527587890625,475.046875,496.2991027832031,482.2400817871094,495.5201416015625,497.3094787597656,480.01715087890625,489.7646484375,480.6991882324219,488.5259704589844,496.2333984375,484.9296875,495.775146484375,485.436279296875,484.5334777832031,474.73236083984375,505.32501220703125,493.89312744140625,485.7517395019531,498.73846435546875,486.35986328125,471.9474792480469,498.515625,493.38494873046875,488.6591796875,500.7427673339844,500.1736755371094,491.8586120605469,491.5063781738281,482.72943115234375,478.4229431152344,488.6651916503906,474.6087646484375,495.88873291015625,490.9142150878906,478.0789794921875,491.75872802734375,504.53302001953125,481.16534423828125,489.8651428222656,494.3316650390625,487.4436340332031,482.178466796875,489.834228515625,508.7256774902344,491.61859130859375,498.6475524902344,503.78521728515625,496.1903076171875,500.4569091796875,502.3287353515625,502.066650390625,482.304443359375,468.61785888671875,487.5063781738281,481.04400634765625,481.30865478515625,493.10321044921875,493.398193359375,486.6930847167969,495.2465515136719,487.2444152832031,482.29766845703125,483.306640625,493.93603515625,494.4005126953125,481.288818359375,492.2832336425781,491.63714599609375,494.31951904296875,487.7647399902344,494.37548828125,493.6010437011719,495.6006164550781,509.76123046875,477.0931396484375,494.803955078125,495.0926513671875,496.75616455078125,500.90301513671875,486.9175720214844,485.83355712890625,490.29962158203125,488.430908203125,467.82281494140625,507.75921630859375,492.0678405761719,492.37469482421875,490.5985107421875,497.048583984375,491.49359130859375,496.90313720703125,459.4870300292969,474.8426513671875,487.0478210449219,497.49896240234375,489.17340087890625,488.4753723144531,487.528564453125,482.451416015625,486.8949279785156,488.0762634277344,472.9136962890625,488.20867919921875,496.6458740234375,487.45489501953125,480.4971923828125,498.8763427734375,485.6708679199219,493.5721435546875,490.04913330078125,493.30645751953125,496.3321533203125,471.75714111328125,479.99810791015625,490.65423583984375,473.2373046875,494.3063049316406,492.85162353515625,511.47747802734375,482.30523681640625,481.71832275390625,479.3154602050781,494.3124694824219,481.7254638671875,497.12322998046875,494.3838806152344,499.52020263671875,487.17425537109375,489.848388671875,476.698486328125,477.20330810546875,483.5469055175781,497.2232666015625,502.3607482910156,470.3851318359375,498.0987548828125,487.2353515625,497.0686340332031,485.10211181640625,495.8238220214844,500.8990478515625,494.5362854003906,468.9705810546875,479.811279296875,493.4952087402344,501.9483337402344,467.8324890136719,486.03546142578125,488.58551025390625,458.82989501953125,495.6897277832031,487.7296447753906,483.58477783203125,475.4896240234375,495.93853759765625,490.72125244140625,469.8526306152344,493.0981140136719,480.169189453125,496.7414245605469,498.7019958496094,483.00433349609375,474.86968994140625,483.95941162109375,487.3070983886719,480.62384033203125,489.56591796875,477.7408142089844,497.942138671875,477.7656555175781,482.22100830078125,474.6272277832031,474.2481384277344,484.3909912109375,499.02056884765625,488.3697204589844,498.16094970703125,482.29351806640625,481.6792907714844,494.3033142089844,489.187255859375,489.8401794433594,490.83953857421875,490.977294921875,492.94366455078125,484.81719970703125,495.287353515625,503.0762939453125,483.9067687988281,468.0362854003906,463.3244934082031,472.102783203125,477.63623046875,496.686767578125,484.54095458984375,484.6108093261719,501.1470947265625,491.99346923828125,496.771728515625,496.23016357421875,483.7804870605469,487.6364440917969,492.4456787109375,490.525146484375,491.0942077636719,481.907470703125,505.900390625,484.4814453125,498.28173828125,483.68377685546875,489.09576416015625,474.26824951171875,464.9561767578125,490.18927001953125,498.4065246582031,481.91766357421875,515.97216796875,473.84576416015625,483.92681884765625,490.51092529296875,480.03668212890625,483.1351318359375,451.8557434082031,484.052490234375,475.67657470703125,485.0381164550781,496.40020751953125,468.1700439453125,482.69110107421875,482.97149658203125,494.705078125,479.181640625,471.7596435546875,487.387451171875,484.3409729003906,473.5186767578125,479.3550720214844,483.1966857910156,476.7223205566406,469.4253845214844,489.5805358886719,470.1044921875,494.39923095703125,485.4343566894531,480.63623046875,478.3634033203125,487.3204650878906,492.587158203125,489.0531921386719,496.4965515136719,463.0738525390625,489.1662902832031,499.69866943359375,491.6123046875,474.76806640625,470.75677490234375,493.6029052734375,489.747802734375,483.38165283203125,481.43499755859375,495.27264404296875,497.9010314941406,483.8148498535156,474.0637512207031,494.5896301269531,480.76116943359375,494.17352294921875,471.8519592285156,491.23046875,484.3560791015625,485.806640625,491.03350830078125,485.64080810546875,485.117431640625,488.43572998046875,457.5443115234375,471.9346008300781,482.0981750488281,471.6605224609375,501.2450256347656,491.28985595703125,484.99359130859375,453.6734924316406,504.973388671875,478.6936950683594,522.3505859375,490.4504699707031,475.851806640625,475.2451171875,489.07879638671875,483.243896484375,502.32928466796875,506.55322265625,517.759033203125,490.1174011230469,485.45965576171875,482.4981689453125,498.6505432128906,470.2657470703125,493.8468322753906,469.5884094238281,488.40264892578125,494.8573913574219,487.4377746582031,469.3253173828125,494.5221252441406,482.00567626953125,486.01513671875,482.397216796875,485.2773132324219,468.78094482421875,476.2772216796875,483.3421936035156,492.14337158203125,480.0410461425781,471.2414245605469,494.7054138183594,493.28009033203125,482.34130859375,483.1194763183594,496.0958251953125,488.3954162597656,493.22613525390625,477.247802734375,473.36090087890625,497.6078796386719,466.01202392578125,495.9654541015625,483.496826171875,469.0797119140625,501.86572265625,489.7657165527344,498.0007629394531,502.19256591796875,488.6942138671875,492.0085144042969,496.9293518066406,483.48614501953125,473.8482971191406,475.7894287109375,469.55084228515625,472.7798767089844,475.65289306640625,472.5297546386719,477.02093505859375,484.64703369140625,482.4072265625,480.91156005859375,477.4373779296875,473.7041320800781,465.352783203125,488.9041442871094,478.04058837890625,475.5084228515625,457.6673583984375,484.76812744140625,491.03521728515625,467.718994140625,494.48333740234375,481.86572265625,481.2225341796875,484.55718994140625,488.26556396484375,465.71142578125,471.3249816894531,478.4491882324219,483.0226135253906,473.44122314453125,496.38763427734375,456.93682861328125,487.19097900390625,502.1884765625,483.92572021484375,480.29461669921875,479.7018127441406,477.86065673828125,496.5615539550781,482.98016357421875,475.45953369140625,473.204833984375,479.25079345703125,476.65069580078125,488.11468505859375,495.947998046875,484.0774841308594,471.92333984375,480.7099609375,483.3371887207031,490.2967834472656,486.08148193359375,481.549560546875,481.42431640625,473.17095947265625,484.5860900878906,473.1901550292969,483.65447998046875,492.18951416015625,491.8919982910156,484.21575927734375,467.7180480957031,472.1138610839844,474.16705322265625,484.77685546875,486.0728759765625,482.317138671875,505.4090576171875,488.2827453613281,490.14886474609375,486.1086120605469,479.0074768066406,485.7160339355469,471.97528076171875,494.624755859375,483.837158203125,491.33172607421875,484.3453369140625,466.8558349609375,479.04742431640625,484.2878112792969,474.41326904296875,476.4613037109375,496.406982421875,493.2231140136719,492.3807067871094,477.41119384765625,484.94683837890625,493.27239990234375,493.3763122558594,500.588623046875,479.49700927734375,473.81158447265625,461.6293640136719,476.28082275390625,484.8266906738281,451.92205810546875,518.5040893554688,463.9297790527344,484.66534423828125,484.233642578125,477.2674560546875,481.83392333984375,477.68743896484375,469.8968505859375,505.6242980957031,489.25799560546875,477.3317565917969,474.29205322265625,488.29443359375,455.0302734375,467.6797180175781,462.27655029296875,472.7863464355469,478.3671875,488.685546875,488.04534912109375,505.2645568847656,483.96844482421875,471.67755126953125,483.53143310546875,493.15838623046875,458.91650390625,484.52276611328125,484.92205810546875,518.7601928710938,500.1429748535156,478.24725341796875,447.2114562988281,496.6546936035156,463.60809326171875,476.756591796875,472.63348388671875,493.8849182128906,470.3673095703125,472.208740234375,477.89752197265625,486.9369201660156,493.6050109863281,460.04376220703125,484.3465881347656,491.2002258300781,485.83837890625,488.5098571777344,489.21087646484375,473.1900634765625,462.1016845703125,487.294677734375,479.4859619140625,486.8993225097656,486.34759521484375,474.4573059082031,495.58203125,464.7581787109375,483.57598876953125,480.1626892089844,465.69110107421875,467.5618896484375,480.17999267578125,481.9739990234375,496.3047790527344,488.41851806640625,476.04864501953125,484.4111328125,473.95098876953125,484.4227294921875,476.10498046875,466.61322021484375,479.5125732421875,486.12939453125,490.5223388671875,481.2527770996094,460.2873229980469,487.13812255859375,509.2384338378906,491.0389404296875,478.4815673828125,472.620849609375,458.2853088378906,461.3448486328125,474.66656494140625,488.0586853027344,480.9349365234375,475.75103759765625,460.21832275390625,479.7926025390625,470.4692687988281,474.87225341796875,491.0474548339844,457.4549560546875,483.6431884765625,477.617431640625,497.82232666015625,484.4125671386719,483.03759765625,486.20452880859375,465.2723693847656,453.11798095703125,477.3381652832031,478.28631591796875,486.551025390625,509.33990478515625,467.2356262207031,479.431640625,468.0705261230469,509.9554443359375,484.0098876953125,478.3311462402344,489.9212341308594,481.4755859375,488.1037292480469,475.643310546875,466.53759765625,471.50848388671875,479.24749755859375,487.79730224609375,478.4420166015625,477.1136779785156,464.95587158203125,470.5334777832031,481.68829345703125,486.69940185546875,501.17462158203125,490.68255615234375,476.2669982910156,471.25897216796875,476.1571044921875,469.86370849609375,479.1036376953125,486.9731140136719,473.5460510253906,479.9075927734375,492.1641540527344,487.64532470703125,485.26617431640625,474.8153076171875,473.557861328125,480.912109375,480.49359130859375,467.776611328125,467.90771484375,467.1334228515625,475.39068603515625,469.1541748046875,458.6222229003906,477.3586730957031,475.312744140625,494.32098388671875,475.7953796386719,455.9528503417969,495.6389465332031,487.8295593261719,489.2270812988281,465.9347839355469,478.44781494140625,468.72821044921875,483.21783447265625,465.447998046875,491.95867919921875,459.6472473144531,462.61102294921875,499.7750244140625,482.02276611328125,495.779541015625,477.10302734375,459.15093994140625,486.8433837890625,480.023193359375,469.78509521484375,483.8104248046875,467.19842529296875,491.4751892089844,480.0511169433594,482.7028503417969,475.22601318359375,478.529052734375,471.51971435546875,482.2362365722656,486.2861022949219,471.20416259765625,481.7695617675781,488.1985778808594,500.2803955078125,465.3465576171875,467.0866394042969,480.5413818359375,477.966552734375,489.6287536621094,487.0744934082031,470.4466857910156,481.58807373046875,463.10443115234375,480.8310852050781,465.86346435546875,486.1676025390625,480.9808349609375,484.5079345703125,480.0067138671875,483.541259765625,482.02142333984375,456.86224365234375,471.74517822265625,459.3460693359375,460.128662109375,488.2716369628906,483.4727783203125,470.95001220703125,481.73040771484375,493.75579833984375,474.66925048828125,492.1693115234375,477.4368896484375,459.128173828125,492.9377746582031,483.53778076171875,479.4451904296875,462.2547607421875,492.5170593261719,472.871337890625,486.40673828125,479.33453369140625,471.58819580078125,470.02716064453125,460.1062927246094,495.2715148925781,473.1239013671875,478.171630859375,463.05511474609375,479.26959228515625,454.9202575683594,501.4395751953125,484.650634765625,465.7664489746094,478.5146484375,475.7818908691406,486.799560546875,490.2685241699219,485.8004150390625,471.392578125,460.94036865234375,484.503173828125,459.13824462890625,488.9940185546875,479.4857177734375,492.2549133300781,477.0731201171875,475.6728210449219,461.27996826171875,479.5776062011719,467.2181701660156,482.36309814453125,475.4557189941406,481.835205078125,482.3352355957031,450.6706237792969,492.98468017578125,465.7748107910156,458.66339111328125,473.2103576660156,475.1094970703125,482.5008544921875,487.2727355957031,495.51007080078125,492.21307373046875,483.26385498046875,488.0010986328125,484.4925537109375,461.34588623046875,472.51629638671875,463.67584228515625,471.52081298828125,480.48236083984375,483.3974304199219,467.0069885253906,465.2608642578125,484.704345703125,470.0562744140625,480.39910888671875,503.61578369140625,475.9146728515625,462.40289306640625,484.6490478515625,465.16729736328125,463.7518310546875,489.1269226074219,477.87335205078125,487.7083740234375,476.0239562988281,467.7332458496094,480.83270263671875,479.3658447265625,484.5752258300781,478.37738037109375,473.29302978515625,468.18505859375,472.8809814453125,487.7025146484375,462.5235290527344,477.5909423828125,466.8428039550781,478.2991943359375,485.4850769042969,490.989013671875,483.607421875,450.9622802734375,482.02978515625,483.3972473144531,470.27337646484375,490.063720703125,468.1175537109375,477.8323669433594,468.276611328125,492.9270935058594,471.7464294433594,458.39349365234375,475.7986145019531,492.1136779785156,478.2950439453125,474.8288879394531,475.36224365234375,476.8746337890625,493.677001953125,488.18621826171875,479.2833251953125,482.17913818359375,487.4757080078125,486.3323974609375,485.0960388183594,480.873046875,458.64874267578125,492.8074951171875,481.0541076660156,460.50042724609375,484.8634033203125,484.60076904296875,495.51300048828125,472.4095458984375,458.91375732421875,461.611083984375,470.76202392578125,485.45916748046875,458.7838134765625,476.0341796875,493.2989196777344,479.8258972167969,475.72491455078125,473.2530212402344,471.1644287109375,458.8477783203125,467.423583984375,502.4766540527344,475.0708923339844,492.9149475097656,468.3362121582031,461.244384765625,505.3486328125,495.8775634765625,470.1671447753906,485.310302734375,448.39959716796875,501.78814697265625,492.60125732421875,464.1898498535156,481.32208251953125,469.89349365234375,484.25567626953125,474.13824462890625,471.0923767089844,477.5459289550781,482.9446716308594,463.0143127441406,479.7088317871094,478.73040771484375,464.6691589355469,492.38934326171875,449.9794616699219,479.6276550292969,476.8232116699219,490.34600830078125,476.84136962890625,456.54388427734375,479.73681640625,488.288818359375,486.485595703125,485.7474365234375,495.0418701171875,472.7284851074219,483.36456298828125,471.229248046875,456.01373291015625,472.84320068359375,478.36785888671875,485.656982421875,493.105712890625,468.3291320800781,491.6230773925781,457.6531982421875,465.1920166015625,478.0242004394531,489.33331298828125,454.8283386230469,476.0989990234375,481.46795654296875,484.25177001953125,483.090576171875,480.402587890625,477.20343017578125,471.99969482421875,477.0657653808594,494.978271484375,476.00445556640625,494.43841552734375,469.32659912109375,469.9598388671875,466.33709716796875,479.8626708984375,467.7820129394531,480.4423522949219,485.3359069824219,498.9883117675781,455.20989990234375,452.3797607421875,484.4838562011719,478.3062744140625,490.38238525390625,484.16485595703125,470.17529296875,451.8301086425781,474.2156066894531,461.4766845703125,470.40478515625,480.2613830566406,467.61248779296875,474.3194274902344,472.96978759765625,489.2034606933594,477.0438232421875,496.4806823730469,474.6183166503906,488.67535400390625,464.27117919921875,483.04364013671875,474.61041259765625,485.368408203125,473.548095703125,477.6047058105469,468.35015869140625,472.5772705078125,480.9891357421875,462.69189453125,468.7142028808594,477.06414794921875,484.5566101074219,469.7043151855469,484.9382629394531,488.6172790527344,470.33013916015625,468.98980712890625,487.8836669921875,471.6112365722656,474.21795654296875,460.41680908203125,477.3183898925781,483.66656494140625,466.4607238769531,482.68194580078125,480.5890808105469,466.9327392578125,469.1280517578125,470.4804992675781,464.7138366699219,477.4629211425781,469.5976867675781,459.8681945800781,475.1903076171875,476.36846923828125,464.615478515625,473.86895751953125,483.4913330078125,455.45758056640625,466.73101806640625,469.025390625,489.9090270996094,478.6171875,466.885498046875,464.5384521484375,476.65130615234375,469.0848388671875,486.30267333984375,483.947265625,455.489990234375,483.0733642578125,475.00518798828125,484.0683288574219,480.2889099121094,476.715576171875,477.46533203125,472.9019775390625,477.3420715332031,493.71978759765625,477.4237976074219,484.5872802734375,463.8643798828125,501.5621337890625,478.1893310546875,490.04296875,478.9317626953125,479.30389404296875,481.47442626953125,471.0348815917969,490.34429931640625,475.9649658203125,500.3183898925781,488.3333740234375,473.07000732421875,456.1790771484375,486.01580810546875,481.002685546875,482.0169372558594,491.55712890625,450.21575927734375,462.6654052734375,466.66864013671875,452.14031982421875,461.3839111328125,491.5110168457031,468.7330627441406,486.2907409667969,470.0826721191406,469.7825012207031,454.0468444824219,477.873779296875,470.80096435546875,488.3018493652344,458.0690002441406,475.08349609375,485.9759216308594,483.60382080078125,484.7510070800781,480.8143005371094,484.71417236328125,481.3525390625,467.88995361328125,485.11016845703125,473.7178649902344,465.7071533203125,474.96612548828125,464.5216369628906,482.177490234375,476.3376159667969,485.31610107421875,478.13671875,462.26422119140625,472.5662841796875,459.184814453125,485.63818359375,490.7349548339844,459.79656982421875,472.4296875,490.7001647949219,483.1747741699219,494.480712890625,475.7528381347656,476.44207763671875,469.173828125,491.469970703125,452.00286865234375,464.006591796875,468.67236328125,465.10076904296875,478.02752685546875,486.74493408203125,472.84375,468.20648193359375,496.4356689453125,461.76873779296875,481.88226318359375,502.166259765625,499.73321533203125,475.85992431640625,452.19561767578125,469.234130859375,460.6771240234375,487.555908203125,468.9366455078125,461.55548095703125,459.99847412109375,490.8132629394531,475.2449951171875,484.78082275390625,481.8823547363281,474.61517333984375,464.5946044921875,473.47723388671875,486.60797119140625,471.98895263671875,475.06536865234375,475.9190673828125,494.7416076660156,468.661865234375,489.2410888671875,461.5421142578125,463.1002197265625,470.300537109375,484.248779296875,471.41357421875,468.818603515625,483.5987243652344,488.5426940917969,495.81341552734375,466.4049072265625]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss on time\"};\n",
       "\n",
       "  Plotly.plot('plot-787930328', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mlossSeq\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[32m588.948974609375\u001b[39m,\n",
       "  \u001b[32m587.7423095703125\u001b[39m,\n",
       "  \u001b[32m588.75634765625\u001b[39m,\n",
       "  \u001b[32m588.4892578125\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mplot\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mScatter\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mScatter\u001b[39m(\n",
       "    \u001b[33mSome\u001b[39m(\n",
       "      \u001b[33mDoubles\u001b[39m(\n",
       "        \u001b[33mVector\u001b[39m(\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mres8_4\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-787930328\"\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  var lossSeq: Seq[Double] = Nil \n",
    "  for (_ <- 0 until 2000) {\n",
    "    val trainNDArray = ReadCIFAR10ToNDArray.getSGDTrainNDArray(256)\n",
    "    val loss = lossFunction.train(trainNDArray.head :: makeVectorized(trainNDArray.tail.head) :: HNil)\n",
    "    lossSeq = lossSeq :+ loss.toString.toDouble\n",
    "  }\n",
    "\n",
    "  plotly.JupyterScala.init()\n",
    "  val plot = Seq(\n",
    "    Scatter(\n",
    "      0 until 2000 by 1,\n",
    "      lossSeq\n",
    "    )\n",
    "  )\n",
    "\n",
    "  plot.plot(\n",
    "    title = \"loss on time\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.使用训练后的神经网络判断测试数据的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: [[0.07, 0.11, 0.14, 0.16, 0.07, 0.16, 0.14, 0.03, 0.11, 0.02],\n",
      " [0.06, 0.20, 0.02, 0.01, 0.00, 0.01, 0.01, 0.01, 0.25, 0.44],\n",
      " [0.12, 0.07, 0.03, 0.02, 0.01, 0.02, 0.01, 0.01, 0.55, 0.17],\n",
      " [0.24, 0.09, 0.09, 0.03, 0.03, 0.03, 0.01, 0.05, 0.39, 0.05],\n",
      " [0.04, 0.04, 0.15, 0.13, 0.21, 0.15, 0.14, 0.09, 0.03, 0.02],\n",
      " [0.01, 0.11, 0.06, 0.14, 0.09, 0.13, 0.33, 0.07, 0.01, 0.06],\n",
      " [0.01, 0.05, 0.05, 0.30, 0.03, 0.33, 0.15, 0.03, 0.02, 0.02],\n",
      " [0.03, 0.03, 0.19, 0.09, 0.20, 0.10, 0.19, 0.13, 0.03, 0.02],\n",
      " [0.06, 0.06, 0.18, 0.15, 0.14, 0.19, 0.09, 0.09, 0.04, 0.01],\n",
      " [0.17, 0.26, 0.05, 0.03, 0.01, 0.01, 0.01, 0.03, 0.24, 0.19],\n",
      " [0.24, 0.09, 0.06, 0.08, 0.04, 0.08, 0.03, 0.04, 0.30, 0.05],\n",
      " [0.02, 0.25, 0.02, 0.01, 0.01, 0.01, 0.02, 0.04, 0.07, 0.55],\n",
      " [0.02, 0.11, 0.10, 0.16, 0.13, 0.20, 0.16, 0.06, 0.04, 0.02],\n",
      " [0.08, 0.28, 0.05, 0.07, 0.06, 0.07, 0.11, 0.02, 0.18, 0.07],\n",
      " [0.11, 0.14, 0.08, 0.06, 0.04, 0.04, 0.04, 0.13, 0.17, 0.19],\n",
      " [0.14, 0.04, 0.14, 0.06, 0.09, 0.09, 0.05, 0.08, 0.28, 0.05],\n",
      " [0.05, 0.05, 0.08, 0.10, 0.08, 0.31, 0.15, 0.12, 0.05, 0.01],\n",
      " [0.10, 0.05, 0.11, 0.15, 0.16, 0.11, 0.11, 0.11, 0.02, 0.06],\n",
      " [0.08, 0.09, 0.01, 0.00, 0.00, 0.00, 0.00, 0.01, 0.44, 0.37],\n",
      " [0.01, 0.07, 0.06, 0.12, 0.14, 0.13, 0.33, 0.10, 0.01, 0.03],\n",
      " [0.14, 0.06, 0.11, 0.05, 0.17, 0.09, 0.06, 0.14, 0.05, 0.13],\n",
      " [0.20, 0.00, 0.32, 0.08, 0.13, 0.09, 0.01, 0.15, 0.02, 0.01],\n",
      " [0.43, 0.05, 0.05, 0.03, 0.03, 0.04, 0.01, 0.03, 0.29, 0.05],\n",
      " [0.03, 0.14, 0.04, 0.09, 0.08, 0.10, 0.16, 0.15, 0.02, 0.20],\n",
      " [0.05, 0.03, 0.20, 0.05, 0.27, 0.08, 0.07, 0.20, 0.02, 0.04],\n",
      " [0.08, 0.17, 0.15, 0.03, 0.10, 0.03, 0.15, 0.07, 0.11, 0.10],\n",
      " [0.05, 0.05, 0.11, 0.10, 0.18, 0.11, 0.18, 0.16, 0.01, 0.04],\n",
      " [0.15, 0.06, 0.12, 0.04, 0.07, 0.05, 0.03, 0.13, 0.14, 0.20],\n",
      " [0.10, 0.24, 0.09, 0.06, 0.09, 0.04, 0.05, 0.07, 0.03, 0.25],\n",
      " [0.01, 0.04, 0.10, 0.09, 0.13, 0.10, 0.37, 0.10, 0.01, 0.04],\n",
      " [0.03, 0.10, 0.12, 0.12, 0.13, 0.12, 0.19, 0.14, 0.02, 0.06],\n",
      " [0.04, 0.02, 0.17, 0.13, 0.15, 0.20, 0.11, 0.13, 0.03, 0.02],\n",
      " [0.05, 0.06, 0.13, 0.14, 0.12, 0.19, 0.12, 0.10, 0.06, 0.02],\n",
      " [0.04, 0.18, 0.17, 0.06, 0.12, 0.02, 0.24, 0.12, 0.02, 0.03],\n",
      " [0.14, 0.09, 0.02, 0.02, 0.01, 0.02, 0.01, 0.03, 0.21, 0.45],\n",
      " [0.04, 0.16, 0.12, 0.10, 0.12, 0.14, 0.11, 0.13, 0.05, 0.03],\n",
      " [0.03, 0.05, 0.16, 0.13, 0.13, 0.12, 0.17, 0.11, 0.03, 0.06],\n",
      " [0.04, 0.23, 0.02, 0.03, 0.01, 0.03, 0.03, 0.02, 0.11, 0.48],\n",
      " [0.09, 0.23, 0.06, 0.08, 0.04, 0.04, 0.07, 0.08, 0.10, 0.21],\n",
      " [0.10, 0.02, 0.14, 0.14, 0.11, 0.26, 0.05, 0.08, 0.09, 0.01],\n",
      " [0.31, 0.07, 0.10, 0.04, 0.10, 0.04, 0.04, 0.09, 0.17, 0.05],\n",
      " [0.04, 0.03, 0.15, 0.08, 0.19, 0.13, 0.19, 0.14, 0.03, 0.02],\n",
      " [0.10, 0.08, 0.07, 0.18, 0.04, 0.09, 0.06, 0.08, 0.10, 0.22],\n",
      " [0.04, 0.05, 0.10, 0.13, 0.18, 0.13, 0.21, 0.13, 0.01, 0.03],\n",
      " [0.42, 0.03, 0.01, 0.00, 0.01, 0.00, 0.00, 0.04, 0.39, 0.09],\n",
      " [0.12, 0.13, 0.03, 0.05, 0.02, 0.05, 0.02, 0.05, 0.18, 0.35],\n",
      " [0.02, 0.08, 0.06, 0.17, 0.09, 0.22, 0.20, 0.07, 0.03, 0.06],\n",
      " [0.19, 0.09, 0.03, 0.09, 0.03, 0.06, 0.03, 0.06, 0.32, 0.11],\n",
      " [0.02, 0.03, 0.15, 0.06, 0.27, 0.09, 0.21, 0.14, 0.01, 0.01],\n",
      " [0.02, 0.03, 0.14, 0.07, 0.21, 0.08, 0.28, 0.11, 0.01, 0.04],\n",
      " [0.14, 0.11, 0.02, 0.01, 0.01, 0.00, 0.00, 0.01, 0.36, 0.33],\n",
      " [0.14, 0.13, 0.10, 0.03, 0.07, 0.04, 0.04, 0.09, 0.23, 0.12],\n",
      " [0.02, 0.10, 0.08, 0.12, 0.11, 0.15, 0.24, 0.08, 0.06, 0.04],\n",
      " [0.04, 0.08, 0.06, 0.12, 0.06, 0.15, 0.13, 0.11, 0.10, 0.16],\n",
      " [0.07, 0.06, 0.01, 0.01, 0.00, 0.01, 0.00, 0.00, 0.68, 0.14],\n",
      " [0.16, 0.16, 0.08, 0.04, 0.04, 0.04, 0.03, 0.03, 0.27, 0.15],\n",
      " [0.09, 0.02, 0.13, 0.10, 0.21, 0.20, 0.08, 0.12, 0.04, 0.01],\n",
      " [0.07, 0.10, 0.09, 0.08, 0.09, 0.13, 0.15, 0.05, 0.21, 0.05],\n",
      " [0.07, 0.13, 0.07, 0.10, 0.06, 0.09, 0.08, 0.09, 0.12, 0.17],\n",
      " [0.10, 0.04, 0.17, 0.12, 0.17, 0.15, 0.07, 0.12, 0.04, 0.03],\n",
      " [0.01, 0.01, 0.15, 0.07, 0.30, 0.09, 0.18, 0.19, 0.00, 0.01],\n",
      " [0.01, 0.01, 0.07, 0.21, 0.16, 0.22, 0.20, 0.10, 0.00, 0.01],\n",
      " [0.03, 0.16, 0.07, 0.09, 0.13, 0.09, 0.24, 0.09, 0.06, 0.04],\n",
      " [0.07, 0.09, 0.09, 0.04, 0.07, 0.07, 0.05, 0.07, 0.06, 0.40],\n",
      " [0.05, 0.11, 0.12, 0.14, 0.13, 0.08, 0.16, 0.11, 0.03, 0.09],\n",
      " [0.05, 0.03, 0.14, 0.11, 0.20, 0.13, 0.18, 0.13, 0.03, 0.02],\n",
      " [0.06, 0.48, 0.05, 0.02, 0.05, 0.02, 0.05, 0.04, 0.07, 0.17],\n",
      " [0.46, 0.01, 0.03, 0.01, 0.01, 0.01, 0.00, 0.02, 0.42, 0.02],\n",
      " [0.07, 0.05, 0.05, 0.14, 0.06, 0.20, 0.11, 0.10, 0.09, 0.14],\n",
      " [0.04, 0.11, 0.02, 0.02, 0.01, 0.01, 0.02, 0.03, 0.16, 0.57],\n",
      " [0.07, 0.07, 0.14, 0.11, 0.12, 0.12, 0.11, 0.17, 0.04, 0.05],\n",
      " [0.03, 0.05, 0.10, 0.11, 0.21, 0.17, 0.20, 0.11, 0.01, 0.02],\n",
      " [0.11, 0.19, 0.09, 0.07, 0.06, 0.06, 0.07, 0.05, 0.14, 0.15],\n",
      " [0.24, 0.04, 0.07, 0.04, 0.03, 0.07, 0.01, 0.04, 0.42, 0.04],\n",
      " [0.14, 0.06, 0.04, 0.02, 0.03, 0.02, 0.03, 0.07, 0.13, 0.47],\n",
      " [0.04, 0.05, 0.21, 0.05, 0.25, 0.05, 0.16, 0.11, 0.03, 0.04],\n",
      " [0.08, 0.17, 0.06, 0.07, 0.06, 0.05, 0.08, 0.07, 0.08, 0.30],\n",
      " [0.09, 0.09, 0.06, 0.15, 0.05, 0.23, 0.05, 0.04, 0.20, 0.05],\n",
      " [0.02, 0.12, 0.05, 0.20, 0.06, 0.19, 0.19, 0.06, 0.04, 0.08],\n",
      " [0.11, 0.37, 0.04, 0.03, 0.03, 0.02, 0.03, 0.03, 0.07, 0.27],\n",
      " [0.19, 0.05, 0.10, 0.05, 0.03, 0.06, 0.02, 0.05, 0.31, 0.14],\n",
      " [0.03, 0.20, 0.06, 0.15, 0.06, 0.19, 0.11, 0.05, 0.10, 0.06],\n",
      " [0.05, 0.12, 0.22, 0.09, 0.16, 0.10, 0.17, 0.04, 0.02, 0.04],\n",
      " [0.17, 0.08, 0.13, 0.11, 0.06, 0.12, 0.04, 0.10, 0.11, 0.09],\n",
      " [0.24, 0.07, 0.18, 0.09, 0.06, 0.08, 0.02, 0.08, 0.11, 0.07],\n",
      " [0.30, 0.03, 0.07, 0.02, 0.05, 0.02, 0.01, 0.14, 0.16, 0.21],\n",
      " [0.33, 0.02, 0.08, 0.05, 0.04, 0.09, 0.01, 0.06, 0.28, 0.04],\n",
      " [0.10, 0.11, 0.06, 0.03, 0.02, 0.04, 0.01, 0.03, 0.39, 0.21],\n",
      " [0.19, 0.02, 0.10, 0.07, 0.03, 0.07, 0.03, 0.07, 0.33, 0.08],\n",
      " [0.14, 0.17, 0.04, 0.03, 0.04, 0.04, 0.03, 0.04, 0.17, 0.30],\n",
      " [0.16, 0.17, 0.05, 0.04, 0.04, 0.05, 0.02, 0.04, 0.22, 0.23],\n",
      " [0.02, 0.03, 0.15, 0.11, 0.19, 0.17, 0.18, 0.12, 0.02, 0.02],\n",
      " [0.16, 0.09, 0.03, 0.03, 0.01, 0.03, 0.01, 0.01, 0.52, 0.10],\n",
      " [0.16, 0.04, 0.23, 0.04, 0.12, 0.04, 0.08, 0.08, 0.11, 0.09],\n",
      " [0.12, 0.07, 0.11, 0.07, 0.12, 0.10, 0.09, 0.11, 0.11, 0.08],\n",
      " [0.06, 0.05, 0.11, 0.17, 0.09, 0.14, 0.13, 0.13, 0.07, 0.07],\n",
      " [0.02, 0.05, 0.13, 0.12, 0.17, 0.13, 0.21, 0.12, 0.02, 0.02],\n",
      " [0.25, 0.05, 0.11, 0.10, 0.06, 0.12, 0.03, 0.07, 0.18, 0.04],\n",
      " [0.13, 0.03, 0.24, 0.10, 0.12, 0.07, 0.04, 0.24, 0.02, 0.02],\n",
      " [0.09, 0.06, 0.09, 0.10, 0.07, 0.07, 0.07, 0.19, 0.04, 0.23]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mresult\u001b[39m: \u001b[32mFromTo\u001b[39m.\u001b[32m<refinement>\u001b[39m.this.type.\u001b[32mOutputData\u001b[39m = [[0.07, 0.11, 0.14, 0.16, 0.07, 0.16, 0.14, 0.03, 0.11, 0.02],\n",
       " [0.06, 0.20, 0.02, 0.01, 0.00, 0.01, 0.01, 0.01, 0.25, 0.44],\n",
       " [0.12, 0.07, 0.03, 0.02, 0.01, 0.02, 0.01, 0.01, 0.55, 0.17],\n",
       " [0.24, 0.09, 0.09, 0.03, 0.03, 0.03, 0.01, 0.05, 0.39, 0.05],\n",
       " [0.04, 0.04, 0.15, 0.13, 0.21, 0.15, 0.14, 0.09, 0.03, 0.02],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  val result = myNeuralNetwork.predict(test_data)\n",
    "  println(s\"result: $result\") //输出判断结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.编写工具方法，从一行INDArray中获得值最大的元素所在的列，目的是获得神经网络判断的结果，方便和原始标签比较以得出正确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mfindMaxItemIndex\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  /**\n",
    "    * 从一行INDArray中获得值最大的元素所在的列\n",
    "    * @param iNDArray\n",
    "    * @return\n",
    "    */\n",
    "  def findMaxItemIndex(iNDArray: INDArray): Int = {\n",
    "    val shape = iNDArray.shape()\n",
    "    val col = shape(1)\n",
    "    var maxValue = 0.0\n",
    "    var maxIndex = 0\n",
    "    for (index <- 0 until col) {\n",
    "      val itemValue = iNDArray.getDouble(0, index)\n",
    "      if (itemValue > maxValue) {\n",
    "        maxValue = itemValue\n",
    "        maxIndex = index\n",
    "      }\n",
    "    }\n",
    "    maxIndex\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.判断神经网络对测试数据分类判断的正确率，正确率应该在40%左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result is 39 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mright\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m39\u001b[39m\n",
       "\u001b[36mshape\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m100\u001b[39m, \u001b[32m10\u001b[39m)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  var right = 0\n",
    "\n",
    "  val shape = result.shape()\n",
    "  for (row <- 0 until shape(0)) {\n",
    "    val rowItem = result.getRow(row)\n",
    "    val index = findMaxItemIndex(rowItem)\n",
    "    if (index == test_expect_result.getDouble(row, 0)) {\n",
    "      right += 1\n",
    "    }\n",
    "  }\n",
    "  println(s\"the result is $right %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "13.[完整代码](https://github.com/izhangzhihao/deeplearning-tutorial/blob/master/src/main/scala/com/thoughtworks/deeplearning/tutorial/MiniBatchGradientDescent.scala)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
