{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景\n",
    "\n",
    "这篇文章我们将使用softmax分类器一起来构建一个简单的图像分类神经网络，其准确率可以达到32%。我们将使用[cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)的图像和标签数据来训练这个神经网络。Softmax分类器是二元逻辑回归泛化到多元的情况。Softmax分类器会输出对应类别的概率。我们开始吧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似前一篇教程[GetStarted](https://thoughtworksinc.github.io/DeepLearning.scala/demo/GettingStarted.html)，我们需要引入DeepLearning.scala的各个类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                 \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.{FileInputStream, InputStream}\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableHList._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableDouble._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableAny._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.{\n",
       "  DifferentiableHList,\n",
       "  DifferentiableINDArray,\n",
       "  Layer,\n",
       "  Symbolic\n",
       "}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Layer.Batch\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathFunctions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathMethods./\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathOps\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.cpu.nativecpu.NDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.ops.transforms.Transforms\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4s.Implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mshapeless._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.immutable.IndexedSeq\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $plugin.$ivy.`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:2.0.0`\n",
    "\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableany:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablenothing:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableseq:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiabledouble:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablefloat:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablehlist:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableindarray:1.0.0-RC7`\n",
    "import $ivy.`org.rauschig:jarchivelib:0.5.0`\n",
    "\n",
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.0`\n",
    "\n",
    "import java.io.{FileInputStream, InputStream}\n",
    "\n",
    "\n",
    "import com.thoughtworks.deeplearning\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import com.thoughtworks.deeplearning.DifferentiableHList._\n",
    "import com.thoughtworks.deeplearning.DifferentiableDouble._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray._\n",
    "import com.thoughtworks.deeplearning.DifferentiableAny._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
    "import com.thoughtworks.deeplearning.{\n",
    "  DifferentiableHList,\n",
    "  DifferentiableINDArray,\n",
    "  Layer,\n",
    "  Symbolic\n",
    "}\n",
    "import com.thoughtworks.deeplearning.Layer.Batch\n",
    "import com.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
    "import com.thoughtworks.deeplearning.Symbolic._\n",
    "import com.thoughtworks.deeplearning.Poly.MathFunctions._\n",
    "import com.thoughtworks.deeplearning.Poly.MathMethods./\n",
    "import com.thoughtworks.deeplearning.Poly.MathOps\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.cpu.nativecpu.NDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
    "import org.nd4j.linalg.ops.transforms.Transforms\n",
    "import org.nd4s.Implicits._\n",
    "import shapeless._\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._\n",
    "\n",
    "import scala.collection.immutable.IndexedSeq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了减少`jupyter-scala`输出的行数，避免页面输出太长，需要设置`pprintConfig`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pprintConfig() = pprintConfig().copy(height = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "为了从CIFAR10 database中读取训练数据和测试数据的图片和分类信息。我们需要[`import $file.ReadCIFAR10ToNDArray`](https://github.com/ThoughtWorksInc/DeepLearning.scala-website/blob/master/ipynbs/ReadCIFAR10ToNDArray.sc)。这是一个包含读取和处理CIFAR10 数据的脚本文件，由本教程提供。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                   \n",
       "\n",
       "//加载train数据,我们读取1000条数据作为训练数据\n",
       "\u001b[39m\n",
       "\u001b[36mtrainNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.23, 0.17, 0.20, 0.27, 0.38, 0.46, 0.54, 0.57, 0.58, 0.58, 0.51, 0.49, 0.55, 0.56, 0.54, 0.50, 0.54, 0.52, 0.48, 0.54, 0.54, 0.52, 0.53, 0.54, 0.59, 0.64, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.ReadCIFAR10ToNDArray\n",
    "\n",
    "//加载train数据,我们读取1000条数据作为训练数据\n",
    "val trainNDArray =\n",
    "  ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/data_batch_1.bin\", 1000)\n",
    "\n",
    "//加载测试数据，我们读取100条作为测试数据\n",
    "val testNDArray =\n",
    "  ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/test_batch.bin\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便softmax分类器处理数据，我们先处理标签数据([one hot encoding](https://en.wikipedia.org/wiki/One-hot))：将N行一列的NDArray转换为N行NumberOfClasses列的NDArray，每行对应的正确分类的值为1，其它列的值为0。这里区分训练集和测试集的原因是为了能看出网络是否被过度训练导致[过拟合](https://en.wikipedia.org/wiki/Overfitting)。处理标签数据的时候我们使用了[Utils](https://github.com/ThoughtWorksInc/DeepLearning.scala-website/blob/master/ipynbs/Utils.sc)，也由本教程提供。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Utils.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.23, 0.17, 0.20, 0.27, 0.38, 0.46, 0.54, 0.57, 0.58, 0.58, 0.51, 0.49, 0.55, 0.56, 0.54, 0.50, 0.54, 0.52, 0.48, 0.54, 0.54, 0.52, 0.53, 0.54, 0.59, 0.64, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtrainExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [6.00, 9.00, 9.00, 4.00, 1.00, 1.00, 2.00, 7.00, 8.00, 3.00, 4.00, 7.00, 7.00, 2.00, 9.00, 9.00, 9.00, 3.00, 2.00, 6.00, 4.00, 3.00, 6.00, 6.00, 2.00, 6.00, 3.0\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.0\u001b[33m...\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$    \n",
       "\n",
       "//CIFAR10中的图片共有10个分类(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
       "\u001b[39m\n",
       "\u001b[36mNumberOfClasses\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36mvectorizedTrainExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00],\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mvectorizedTestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainData = trainNDArray.head\n",
    "val testData = testNDArray.head\n",
    "\n",
    "val trainExpectResult = trainNDArray.tail.head\n",
    "val testExpectResult = testNDArray.tail.head\n",
    "\n",
    "import $file.Utils\n",
    "\n",
    "//CIFAR10中的图片共有10个分类(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
    "val NumberOfClasses: Int = 10\n",
    "\n",
    "val vectorizedTrainExpectResult = Utils.makeVectorized(trainExpectResult, NumberOfClasses)\n",
    "val vectorizedTestExpectResult = Utils.makeVectorized(testExpectResult, NumberOfClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使用softmax分类器(softmax分类器是softmax和一个全连接组合起来的神经网络)，我们需要先编写softmax函数,公式：![](https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msoftmax\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(implicit scores: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val expScores = exp(scores)\n",
    "  expScores / expScores.sum(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全连接层需要设置学习率，学习率是Weight变化的快慢的直观描述，学习率设置的过小会导致loss下降的很慢，需要更长时间来训练，学习率设置的过大虽然刚开始下降很快但是会导致在接近最低点的时候在附近徘徊loss下降会非常慢。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36moptimizer\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit def optimizer: Optimizer = new LearningRate {\n",
    "  def currentLearningRate() = 0.00001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个全连接层并初始化[Weight]((https://github.com/ThoughtWorksInc/DeepLearning.scala/wiki/Getting-Started#231--weight-intialization))，Weight应该是一个N*NumberOfClasses的INDArray,每个图片对应每个分类都有一个评分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcreateMyNeuralNetwork\u001b[39m\n",
       "\u001b[36mmyNeuralNetwork\u001b[39m: (\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m[\u001b[32mINDArray\u001b[39m]{type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray;type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray})#\u001b[32m@\u001b[39m = Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum(Exp(Identity()),WrappedArray(1)))),Dot(Identity(),Weight([[-0.00, 0.00, 0.00, 0.00, 0.00, -0.00, -0.00, \u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createMyNeuralNetwork(implicit input: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val initialValueOfWeight = Nd4j.randn(3072, NumberOfClasses) * 0.001\n",
    "  val weight: INDArray @Symbolic = initialValueOfWeight.toWeight\n",
    "  val result: INDArray @Symbolic = input dot weight\n",
    "  softmax.compose(result) //对结果调用softmax方法，压缩结果值在0到1之间方便处理\n",
    "}\n",
    "val myNeuralNetwork = createMyNeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了判断神经网络判断的结果好坏，我们需要编写损失函数Loss Function，这里我们使用cross-entropy loss将此次判断的结果和真实结果进行对比并返回评分，公式：\n",
    "![](https://zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlossFunction\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lossFunction(implicit pair: (INDArray :: INDArray :: HNil) @Symbolic): Double @Symbolic = {\n",
    "  val input = pair.head\n",
    "  val expectedOutput = pair.tail.head\n",
    "  val probabilities = myNeuralNetwork.compose(input)\n",
    "\n",
    "  -(expectedOutput * log(probabilities)).mean //此处上面的交叉熵损失公式对应\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了观察神经网络训练的过程，我们需要输出`loss`，在训练神经网络时，loss的变化趋势应该是越来越低的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at iteration 0 loss is 0.230167041015625\n",
      "at iteration 100 loss is 0.19087379150390624\n",
      "at iteration 200 loss is 0.17819180908203125\n",
      "at iteration 300 loss is 0.17023980712890624\n",
      "at iteration 400 loss is 0.164288232421875\n",
      "at iteration 500 loss is 0.15945126953125\n",
      "at iteration 600 loss is 0.15533076171875\n",
      "at iteration 700 loss is 0.15171451416015624\n",
      "at iteration 800 loss is 0.14847626953125\n",
      "at iteration 900 loss is 0.1455343505859375\n",
      "at iteration 1000 loss is 0.14283216552734376\n",
      "at iteration 1100 loss is 0.1403289794921875\n",
      "at iteration 1200 loss is 0.1379939208984375\n",
      "at iteration 1300 loss is 0.13580316162109374\n",
      "at iteration 1400 loss is 0.13373787841796875\n",
      "at iteration 1500 loss is 0.1317827880859375\n",
      "at iteration 1600 loss is 0.1299253173828125\n",
      "at iteration 1700 loss is 0.1281551025390625\n",
      "at iteration 1800 loss is 0.126463330078125\n",
      "at iteration 1900 loss is 0.1248426513671875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-117118284\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0,1989.0,1990.0,1991.0,1992.0,1993.0,1994.0,1995.0,1996.0,1997.0,1998.0,1999.0],\"y\":[0.230167041015625,0.228539892578125,0.22757255859375,0.226677734375,0.2258137451171875,0.2249757080078125,0.22416220703125,0.2233719970703125,0.222604248046875,0.22185791015625,0.2211322265625,0.22042626953125,0.219739208984375,0.2190703125,0.21841884765625,0.217784033203125,0.217165283203125,0.216561962890625,0.215973388671875,0.2153989990234375,0.2148383056640625,0.21429072265625,0.2137556884765625,0.2132328857421875,0.212721728515625,0.212221826171875,0.2117328125,0.211254150390625,0.2107856201171875,0.2103267578125,0.209877294921875,0.209436865234375,0.209005078125,0.2085818359375,0.208166650390625,0.2077593994140625,0.20735966796875,0.206967333984375,0.2065821533203125,0.20620380859375,0.2058321533203125,0.2054669921875,0.2051080078125,0.20475517578125,0.204408203125,0.2040669189453125,0.2037311767578125,0.2034008056640625,0.2030757080078125,0.20275556640625,0.2024404541015625,0.20213011474609374,0.2018243896484375,0.2015231689453125,0.2012263671875,0.200933837890625,0.20064549560546874,0.20036121826171874,0.2000807861328125,0.1998042724609375,0.19953153076171876,0.19926240234375,0.19899683837890625,0.19873472900390626,0.19847603759765625,0.198220556640625,0.197968359375,0.1977192626953125,0.1974733154296875,0.1972302978515625,0.1969902099609375,0.19675291748046875,0.1965184814453125,0.196286767578125,0.19605771484375,0.1958312744140625,0.195607373046875,0.195385986328125,0.1951669921875,0.19495040283203124,0.1947361328125,0.194524169921875,0.1943143798828125,0.1941068603515625,0.19390140380859375,0.193698095703125,0.193496826171875,0.193297607421875,0.1931003173828125,0.192904931640625,0.1927115234375,0.192519970703125,0.19233017578125,0.1921422119140625,0.1919560302734375,0.1917715087890625,0.19158868408203125,0.191407568359375,0.1912280517578125,0.191050146484375,0.19087379150390624,0.19069893798828125,0.19052562255859376,0.1903538330078125,0.19018350830078126,0.19001455078125,0.1898470703125,0.18968094482421874,0.1895161865234375,0.1893527099609375,0.189190673828125,0.189029833984375,0.18887032470703125,0.18871202392578126,0.18855499267578124,0.18839912109375,0.1882444580078125,0.18809097900390626,0.18793868408203124,0.1877874755859375,0.1876374267578125,0.18748846435546876,0.1873406005859375,0.18719373779296875,0.187047998046875,0.1869032958984375,0.1867595703125,0.18661685791015625,0.186475146484375,0.1863344482421875,0.186194677734375,0.186055810546875,0.18591790771484376,0.18578095703125,0.18564490966796876,0.18550972900390625,0.185375439453125,0.1852420166015625,0.1851094970703125,0.18497779541015624,0.184846875,0.18471688232421876,0.18458759765625,0.1844591796875,0.18433154296875,0.1842046875,0.18407857666015626,0.18395322265625,0.183828662109375,0.18370478515625,0.183581689453125,0.1834593017578125,0.18333759765625,0.18321666259765626,0.183096337890625,0.18297677001953125,0.182857861328125,0.1827396240234375,0.1826220947265625,0.182505126953125,0.1823888671875,0.182273193359375,0.18215819091796875,0.182043798828125,0.181930029296875,0.18181683349609376,0.181704296875,0.1815923583984375,0.18148095703125,0.1813701416015625,0.181259912109375,0.18115030517578126,0.18104114990234374,0.180932666015625,0.180824658203125,0.1807172119140625,0.1806103271484375,0.1805039306640625,0.18039803466796875,0.1802927734375,0.180187939453125,0.180083642578125,0.179979833984375,0.179876513671875,0.17977371826171876,0.17967138671875,0.1795695556640625,0.17946817626953124,0.17936729736328125,0.17926685791015626,0.17916688232421876,0.17906741943359375,0.1789683837890625,0.17886976318359374,0.1787716552734375,0.1786738525390625,0.178576611328125,0.1784798095703125,0.178383349609375,0.17828739013671874,0.17819180908203125,0.17809664306640624,0.1780019287109375,0.17790755615234374,0.1778136474609375,0.1777201171875,0.17762694091796874,0.1775342041015625,0.17744188232421876,0.17734990234375,0.1772583251953125,0.17716708984375,0.177076220703125,0.1769857666015625,0.176895654296875,0.17680595703125,0.1767165771484375,0.1766275390625,0.1765388916015625,0.17645059814453126,0.176362646484375,0.176275,0.1761877197265625,0.17610081787109375,0.17601422119140625,0.1759279541015625,0.1758419921875,0.17575638427734375,0.17567110595703125,0.17558614501953124,0.17550152587890624,0.1754172119140625,0.175333251953125,0.1752495361328125,0.17516611328125,0.17508306884765626,0.17500030517578125,0.17491781005859375,0.17483564453125,0.174753759765625,0.174672216796875,0.1745908935546875,0.174509912109375,0.17442916259765626,0.17434876708984376,0.1742685791015625,0.1741887451171875,0.1741091796875,0.1740299072265625,0.173950830078125,0.17387205810546874,0.17379356689453124,0.17371533203125,0.1736373779296875,0.17355966796875,0.17348223876953126,0.1734051025390625,0.1733281982421875,0.173251513671875,0.17317510986328125,0.173098974609375,0.1730230224609375,0.17294735107421874,0.1728719482421875,0.17279677734375,0.17272183837890626,0.172647216796875,0.1725727294921875,0.17249853515625,0.172424560546875,0.172350830078125,0.17227733154296876,0.17220401611328126,0.1721309814453125,0.17205819091796876,0.1719855712890625,0.17191318359375,0.1718410888671875,0.17176915283203126,0.1716974365234375,0.1716259765625,0.1715546875,0.1714836181640625,0.17141279296875,0.1713421630859375,0.1712717041015625,0.171201513671875,0.17113153076171875,0.17106173095703125,0.1709921142578125,0.1709227294921875,0.1708535400390625,0.17078458251953124,0.17071578369140625,0.17064722900390625,0.17057884521484376,0.1705106201171875,0.1704426025390625,0.1703748046875,0.1703072021484375,0.17023980712890624,0.17017254638671875,0.17010546875,0.170038623046875,0.1699719482421875,0.1699054443359375,0.16983916015625,0.169773046875,0.169707080078125,0.1696412841796875,0.16957574462890626,0.1695103271484375,0.16944512939453124,0.169380029296875,0.1693151611328125,0.1692504638671875,0.16918592529296875,0.1691215087890625,0.16905732421875,0.168993310546875,0.16892945556640626,0.1688657470703125,0.16880224609375,0.16873887939453125,0.16867572021484376,0.168612646484375,0.1685498046875,0.16848707275390626,0.16842452392578125,0.16836212158203126,0.16829989013671875,0.1682378173828125,0.1681759033203125,0.168114111328125,0.16805250244140624,0.167991064453125,0.16792974853515624,0.16786861572265624,0.1678076171875,0.1677467529296875,0.16768603515625,0.1676255126953125,0.16756510009765624,0.1675048095703125,0.1674447265625,0.167384716796875,0.167324951171875,0.167265234375,0.1672056884765625,0.1671463134765625,0.16708704833984375,0.16702794189453124,0.16696893310546876,0.1669100830078125,0.166851416015625,0.16679285888671874,0.166734423828125,0.16667611083984374,0.166618017578125,0.1665599365234375,0.1665020751953125,0.166444287109375,0.166386669921875,0.166329150390625,0.16627181396484375,0.1662145751953125,0.1661574951171875,0.16610050048828126,0.16604365234375,0.1659869384765625,0.1659303466796875,0.165873876953125,0.1658175537109375,0.165761328125,0.16570521240234376,0.1656492431640625,0.1655933837890625,0.16553768310546876,0.16548209228515626,0.1654265625,0.16537119140625,0.165315966796875,0.16526083984375,0.165205859375,0.16515096435546875,0.16509617919921876,0.1650415283203125,0.164986962890625,0.1649325439453125,0.1648782470703125,0.16482401123046875,0.164769970703125,0.1647159912109375,0.1646621337890625,0.1646083740234375,0.1645547119140625,0.16450120849609376,0.1644477783203125,0.1643945068359375,0.16434130859375,0.164288232421875,0.1642352294921875,0.1641823486328125,0.1641295654296875,0.164076953125,0.1640243896484375,0.16397197265625,0.163919580078125,0.1638673095703125,0.16381522216796876,0.16376318359375,0.16371124267578124,0.163659423828125,0.16360772705078125,0.163556103515625,0.1635046142578125,0.1634531494140625,0.16340185546875,0.1633506103515625,0.1632994873046875,0.16324849853515624,0.1631975830078125,0.163146728515625,0.1630960205078125,0.1630453857421875,0.16299483642578125,0.16294437255859376,0.16289405517578126,0.16284381103515624,0.16279365234375,0.16274359130859375,0.162693603515625,0.1626437744140625,0.1625939697265625,0.16254429931640624,0.16249471435546875,0.16244517822265625,0.1623957763671875,0.16234647216796874,0.1622972412109375,0.16224808349609374,0.1621990234375,0.16215009765625,0.1621011962890625,0.1620524658203125,0.1620037841796875,0.161955126953125,0.1619066162109375,0.161858203125,0.16180986328125,0.16176158447265626,0.16171341552734375,0.1616653564453125,0.16161734619140625,0.161569384765625,0.16152159423828125,0.16147384033203124,0.1614261474609375,0.16137862548828125,0.16133111572265624,0.16128367919921874,0.16123636474609376,0.16118912353515624,0.1611419677734375,0.1610948486328125,0.1610478759765625,0.161000927734375,0.1609540771484375,0.16090731201171876,0.16086064453125,0.16081405029296875,0.16076754150390626,0.1607210693359375,0.16067469482421876,0.1606283935546875,0.1605822265625,0.160536083984375,0.1604900390625,0.1604440185546875,0.1603981201171875,0.16035228271484375,0.16030655517578124,0.1602608642578125,0.160215283203125,0.1601697509765625,0.16012427978515625,0.1600789306640625,0.16003359375,0.15998839111328125,0.1599432373046875,0.15989815673828126,0.159853173828125,0.159808203125,0.1597633544921875,0.1597185546875,0.15967384033203125,0.15962921142578124,0.1595846435546875,0.1595401123046875,0.1594956787109375,0.15945126953125,0.15940703125,0.1593627685546875,0.15931865234375,0.15927457275390625,0.159230517578125,0.1591865966796875,0.15914273681640626,0.15909891357421874,0.15905518798828125,0.15901148681640626,0.158967919921875,0.158924365234375,0.1588808837890625,0.15883748779296875,0.158794140625,0.15875086669921876,0.158707666015625,0.158664501953125,0.158621435546875,0.15857843017578124,0.158535498046875,0.1584926513671875,0.15844984130859374,0.158407080078125,0.1583643798828125,0.15832176513671875,0.1582792236328125,0.15823671875,0.15819427490234375,0.1581519287109375,0.1581095703125,0.15806737060546874,0.15802515869140624,0.15798306884765626,0.1579409912109375,0.1578990234375,0.1578570556640625,0.157815234375,0.1577734130859375,0.1577316650390625,0.15769000244140624,0.15764835205078126,0.15760679931640625,0.15756527099609374,0.157523828125,0.15748243408203125,0.1574410888671875,0.15739984130859375,0.157358642578125,0.15731749267578124,0.157276416015625,0.1572353515625,0.157194384765625,0.157153466796875,0.1571126220703125,0.1570718017578125,0.1570310302734375,0.1569903564453125,0.15694970703125,0.15690919189453126,0.1568686279296875,0.15682816162109375,0.15678780517578125,0.15674742431640626,0.1567071533203125,0.15666689453125,0.1566267333984375,0.1565865966796875,0.1565465576171875,0.1565065185546875,0.15646651611328125,0.15642666015625,0.15638681640625,0.15634703369140626,0.156307275390625,0.1562675537109375,0.1562279052734375,0.1561883544921875,0.1561488525390625,0.156109326171875,0.156069921875,0.15603052978515625,0.155991259765625,0.15595196533203126,0.15591279296875,0.1558736328125,0.1558344970703125,0.15579541015625,0.1557564453125,0.1557175048828125,0.155678564453125,0.15563974609375,0.1556009033203125,0.155562158203125,0.15552349853515626,0.155484814453125,0.1554462158203125,0.1554076904296875,0.1553692138671875,0.15533076171875,0.1552923583984375,0.1552539794921875,0.15521572265625,0.1551774658203125,0.1551392578125,0.15510111083984374,0.1550630126953125,0.155024951171875,0.1549869384765625,0.1549490478515625,0.1549110595703125,0.15487322998046876,0.154835400390625,0.1547976806640625,0.1547599365234375,0.154722265625,0.1546846435546875,0.15464708251953124,0.1546095458984375,0.1545720703125,0.15453466796875,0.1544972412109375,0.154459912109375,0.1544226318359375,0.15438536376953124,0.15434814453125,0.1543110107421875,0.1542739013671875,0.1542368408203125,0.154199853515625,0.15416287841796875,0.1541259033203125,0.1540890380859375,0.1540522216796875,0.1540154052734375,0.153978662109375,0.1539419921875,0.15390531005859376,0.153868701171875,0.15383212890625,0.1537956298828125,0.153759130859375,0.153722705078125,0.1536863037109375,0.1536499755859375,0.153613623046875,0.153577392578125,0.153541162109375,0.15350498046875,0.15346884765625,0.15343280029296874,0.1533967529296875,0.153360693359375,0.15332476806640624,0.1532888427734375,0.1532529541015625,0.153217138671875,0.1531813232421875,0.15314556884765626,0.1531098876953125,0.15307421875,0.1530385498046875,0.1530030029296875,0.1529674560546875,0.1529319580078125,0.1528964599609375,0.152861083984375,0.1528256591796875,0.1527903564453125,0.1527550537109375,0.152719775390625,0.1526845947265625,0.15264937744140625,0.1526142578125,0.152579150390625,0.152544091796875,0.1525091064453125,0.15247410888671875,0.15243917236328125,0.15240428466796874,0.15236943359375,0.15233460693359374,0.1522998291015625,0.152265087890625,0.1522303955078125,0.152195751953125,0.15216112060546874,0.15212652587890624,0.1520919921875,0.1520574462890625,0.152022998046875,0.15198857421875,0.15195419921875,0.1519198486328125,0.15188553466796875,0.1518512451171875,0.1518170166015625,0.1517828125,0.15174864501953125,0.15171451416015624,0.151680419921875,0.151646337890625,0.15161240234375,0.1515783935546875,0.1515444091796875,0.1515105224609375,0.1514766845703125,0.15144283447265625,0.1514090576171875,0.151375341796875,0.15134158935546874,0.1513078857421875,0.151274267578125,0.1512406494140625,0.15120704345703126,0.15117352294921876,0.15114002685546876,0.1511065673828125,0.1510731201171875,0.15103973388671876,0.151006396484375,0.15097308349609376,0.1509397705078125,0.150906494140625,0.150873291015625,0.1508401123046875,0.150806982421875,0.1507738037109375,0.15074075927734376,0.15070770263671876,0.1506746826171875,0.1506417236328125,0.1506087890625,0.15057589111328126,0.150543017578125,0.15051019287109374,0.1504773681640625,0.15044462890625,0.1504118896484375,0.15037919921875,0.1503465087890625,0.1503138671875,0.15028125,0.1502487060546875,0.1502161865234375,0.15018365478515625,0.150151220703125,0.150118798828125,0.1500863525390625,0.15005404052734375,0.1500216796875,0.149989404296875,0.14995714111328126,0.14992489013671875,0.1498927001953125,0.149860498046875,0.149828369140625,0.1497962646484375,0.149764208984375,0.149732177734375,0.1497001708984375,0.14966817626953124,0.14963623046875,0.14960435791015625,0.14957242431640624,0.149540625,0.1495088134765625,0.1494770263671875,0.149445263671875,0.14941353759765624,0.1493818603515625,0.149350244140625,0.14931861572265626,0.1492869873046875,0.149255419921875,0.14922388916015625,0.1491923828125,0.1491609130859375,0.1491294921875,0.14909805908203125,0.14906668701171874,0.14903536376953125,0.1490040283203125,0.14897275390625,0.14894150390625,0.1489102783203125,0.1488790771484375,0.1488479248046875,0.1488167724609375,0.148785693359375,0.14875465087890624,0.14872359619140624,0.14869256591796876,0.14866158447265626,0.14863062744140626,0.1485997314453125,0.1485688232421875,0.148537939453125,0.14850709228515624,0.14847626953125,0.1484455322265625,0.1484147705078125,0.1483840576171875,0.14835338134765624,0.14832271728515625,0.14829208984375,0.14826148681640625,0.148230908203125,0.14820035400390624,0.14816988525390626,0.14813936767578126,0.1481089111328125,0.1480785400390625,0.1480480712890625,0.14801771240234374,0.14798740234375,0.14795703125,0.147926806640625,0.147896533203125,0.14786629638671875,0.1478361083984375,0.14780592041015625,0.1477758056640625,0.147745654296875,0.1477156005859375,0.1476855224609375,0.14765546875,0.1476254638671875,0.1475955078125,0.147565576171875,0.147535595703125,0.1475057373046875,0.14747589111328124,0.147446044921875,0.1474162109375,0.14738641357421875,0.1473566650390625,0.14732694091796875,0.147297265625,0.147267578125,0.147237890625,0.14720828857421875,0.14717869873046874,0.1471491455078125,0.14711961669921875,0.14709005126953126,0.1470605712890625,0.14703111572265626,0.1470016845703125,0.146972265625,0.1469428955078125,0.1469135009765625,0.1468841796875,0.1468549072265625,0.14682559814453125,0.14679635009765624,0.1467671142578125,0.146737939453125,0.1467087646484375,0.14667960205078126,0.14665047607421874,0.1466214111328125,0.14659232177734374,0.1465632568359375,0.14653426513671874,0.1465052734375,0.146476318359375,0.14644735107421875,0.14641844482421876,0.14638955078125,0.14636070556640626,0.1463318603515625,0.14630302734375,0.14627421875,0.146245458984375,0.1462167236328125,0.1461880126953125,0.146159326171875,0.1461306396484375,0.1461020263671875,0.146073388671875,0.1460447998046875,0.1460162109375,0.1459877197265625,0.1459591796875,0.1459306884765625,0.1459022216796875,0.145873779296875,0.1458453369140625,0.1458169677734375,0.1457885986328125,0.14576024169921875,0.145731884765625,0.14570360107421876,0.14567535400390624,0.1456470947265625,0.14561888427734376,0.145590625,0.14556251220703126,0.1455343505859375,0.1455062255859375,0.145478076171875,0.14545,0.145421923828125,0.1453939453125,0.14536593017578125,0.14533792724609376,0.14530992431640624,0.14528203125,0.1452541015625,0.14522618408203125,0.1451983642578125,0.14517047119140625,0.14514267578125,0.14511485595703125,0.14508707275390625,0.145059326171875,0.14503160400390624,0.14500390625,0.14497618408203125,0.14494853515625,0.14492093505859374,0.144893310546875,0.14486571044921875,0.1448381103515625,0.1448106201171875,0.1447830810546875,0.14475555419921876,0.14472806396484375,0.1447005859375,0.1446731689453125,0.14464576416015626,0.144618359375,0.144591015625,0.1445636474609375,0.1445363525390625,0.1445090087890625,0.14448177490234376,0.1444544921875,0.14442728271484376,0.1444000244140625,0.1443728759765625,0.144345703125,0.1443185546875,0.144291455078125,0.14426431884765625,0.144237255859375,0.14421019287109374,0.144183154296875,0.1441561279296875,0.1441291259765625,0.1441021728515625,0.1440751953125,0.1440482666015625,0.14402137451171876,0.1439945068359375,0.14396763916015626,0.14394075927734376,0.1439139892578125,0.143887158203125,0.143860400390625,0.1438336181640625,0.143806884765625,0.1437801513671875,0.1437534912109375,0.143726806640625,0.143700146484375,0.14367353515625,0.1436468994140625,0.1436203369140625,0.14359375,0.1435672119140625,0.143540673828125,0.1435141845703125,0.14348768310546875,0.14346121826171876,0.14343477783203126,0.1434083740234375,0.1433819580078125,0.1433555908203125,0.14332918701171876,0.14330286865234376,0.1432765380859375,0.14325025634765626,0.1432239501953125,0.143197705078125,0.14317149658203124,0.1431452392578125,0.1431190673828125,0.1430928466796875,0.14306669921875,0.1430406005859375,0.143014453125,0.1429883544921875,0.142962255859375,0.14293623046875,0.1429101806640625,0.14288416748046875,0.14285816650390626,0.14283216552734376,0.1428062255859375,0.14278028564453124,0.14275439453125,0.14272850341796875,0.1427026123046875,0.14267677001953125,0.14265087890625,0.14262509765625,0.142599267578125,0.14257349853515625,0.1425477294921875,0.14252197265625,0.1424963134765625,0.142470556640625,0.14244486083984376,0.1424192138671875,0.1423935546875,0.142367919921875,0.1423423095703125,0.1423167236328125,0.142291162109375,0.1422656005859375,0.14224007568359376,0.1422145751953125,0.1421890625,0.142163623046875,0.142138134765625,0.14211268310546876,0.14208726806640626,0.1420618896484375,0.1420365234375,0.1420111572265625,0.1419858154296875,0.1419604736328125,0.14193514404296875,0.141909912109375,0.14188465576171874,0.14185933837890624,0.14183416748046876,0.14180894775390626,0.141783740234375,0.141758544921875,0.14173341064453124,0.141708251953125,0.141683154296875,0.14165804443359375,0.1416329833984375,0.14160789794921874,0.141582861328125,0.1415577880859375,0.1415328125,0.1415078125,0.14148282470703125,0.14145791015625,0.14143294677734375,0.14140804443359375,0.1413831298828125,0.14135823974609374,0.1413333984375,0.141308544921875,0.14128372802734376,0.14125888671875,0.14123409423828126,0.141209326171875,0.1411845458984375,0.14115982666015625,0.1411350830078125,0.1411104248046875,0.141085693359375,0.14106103515625,0.141036376953125,0.1410117431640625,0.14098714599609374,0.1409625244140625,0.14093795166015624,0.14091337890625,0.14088880615234375,0.140864306640625,0.1408397705078125,0.1408152587890625,0.14079083251953126,0.140766357421875,0.14074188232421875,0.14071746826171874,0.14069305419921874,0.14066866455078125,0.140644287109375,0.14061995849609374,0.1405955810546875,0.14057127685546875,0.14054697265625,0.1405226806640625,0.1404984130859375,0.1404741455078125,0.1404498779296875,0.14042567138671874,0.14040146484375,0.1403772705078125,0.140353125,0.1403289794921875,0.14030482177734374,0.1402806884765625,0.14025660400390624,0.14023253173828126,0.140208447265625,0.14018436279296875,0.1401603759765625,0.1401363037109375,0.1401123046875,0.140088330078125,0.1400643798828125,0.1400404052734375,0.140016455078125,0.139992529296875,0.13996864013671875,0.13994471435546876,0.1399208740234375,0.1398970458984375,0.13987315673828124,0.13984931640625,0.139825537109375,0.139801708984375,0.13977796630859374,0.1397542236328125,0.1397304443359375,0.1397067138671875,0.1396830322265625,0.13965933837890626,0.13963564453125,0.1396119873046875,0.13958831787109374,0.139564697265625,0.1395410888671875,0.13951748046875,0.139493896484375,0.1394703125,0.13944678955078124,0.1394232666015625,0.139399755859375,0.139376220703125,0.13935272216796876,0.1393292724609375,0.13930579833984374,0.139282373046875,0.13925892333984374,0.139235498046875,0.1392121337890625,0.13918876953125,0.139165380859375,0.139142041015625,0.13911871337890624,0.13909534912109375,0.1390720703125,0.1390488037109375,0.139025537109375,0.13900225830078125,0.13897901611328126,0.1389557861328125,0.138932568359375,0.13890938720703125,0.13888623046875,0.138863037109375,0.138839892578125,0.13881678466796876,0.13879366455078124,0.13877054443359374,0.1387474853515625,0.1387244140625,0.138701318359375,0.1386782958984375,0.13865523681640626,0.13863226318359376,0.1386092529296875,0.138586279296875,0.13856328125,0.1385403564453125,0.1385174072265625,0.138494482421875,0.13847159423828126,0.138448681640625,0.1384258056640625,0.13840296630859375,0.1383801025390625,0.138357275390625,0.138334423828125,0.13831163330078125,0.1382888427734375,0.1382660888671875,0.13824332275390624,0.138220556640625,0.1381978271484375,0.138175146484375,0.1381524169921875,0.13812974853515625,0.1381070556640625,0.1380843994140625,0.138061767578125,0.1380391357421875,0.13801649169921876,0.1379939208984375,0.1379713134765625,0.1379487548828125,0.13792618408203125,0.13790364990234374,0.137881103515625,0.13785859375,0.137836083984375,0.1378135986328125,0.1377911865234375,0.13776865234375,0.137746240234375,0.137723828125,0.13770137939453125,0.1376790283203125,0.13765660400390625,0.1376342041015625,0.13761187744140624,0.13758948974609375,0.13756717529296875,0.137544873046875,0.13752259521484375,0.13750029296875,0.137477978515625,0.13745576171875,0.1374334716796875,0.1374112548828125,0.1373890380859375,0.1373668212890625,0.1373446533203125,0.1373224609375,0.13730028076171874,0.137278173828125,0.1372560302734375,0.1372339111328125,0.1372117919921875,0.137189697265625,0.1371676025390625,0.1371455322265625,0.137123486328125,0.13710146484375,0.13707939453125,0.1370573974609375,0.1370354248046875,0.1370134033203125,0.1369914306640625,0.13696947021484376,0.13694755859375,0.1369256103515625,0.1369036865234375,0.1368817626953125,0.13685986328125,0.1368380126953125,0.1368161376953125,0.13679427490234375,0.1367724609375,0.1367506103515625,0.13672880859375,0.1367070068359375,0.13668521728515626,0.1366634033203125,0.136641650390625,0.13661990966796875,0.13659820556640626,0.13657647705078124,0.13655478515625,0.1365330810546875,0.13651141357421875,0.1364897216796875,0.13646806640625,0.13644642333984375,0.1364248046875,0.13640316162109375,0.136381591796875,0.1363600341796875,0.1363384521484375,0.136316845703125,0.1362953125,0.1362737548828125,0.13625224609375,0.1362307373046875,0.136209228515625,0.136187744140625,0.1361662353515625,0.13614483642578126,0.1361233642578125,0.1361019287109375,0.13608052978515625,0.13605911865234374,0.1360376953125,0.1360163330078125,0.135994921875,0.1359736083984375,0.1359522216796875,0.1359309326171875,0.1359095947265625,0.13588826904296875,0.1358669677734375,0.135845703125,0.1358244384765625,0.13580316162109374,0.1357819580078125,0.135760693359375,0.1357394775390625,0.13571827392578126,0.1356970458984375,0.1356759033203125,0.1356547119140625,0.135633544921875,0.1356124267578125,0.13559127197265625,0.1355701416015625,0.13554906005859374,0.1355279541015625,0.135506884765625,0.1354858154296875,0.13546474609375,0.135443701171875,0.13542265625,0.13540159912109376,0.135380615234375,0.135359619140625,0.135338623046875,0.1353176513671875,0.1352967041015625,0.13527574462890626,0.13525478515625,0.13523388671875,0.13521298828125,0.1351920654296875,0.13517119140625,0.13515030517578125,0.1351294189453125,0.1351085693359375,0.135087744140625,0.1350669189453125,0.13504608154296874,0.135025244140625,0.1350044921875,0.1349836669921875,0.13496292724609374,0.1349421630859375,0.1349214111328125,0.13490069580078126,0.1348799560546875,0.13485924072265626,0.13483857421875,0.1348178955078125,0.13479715576171875,0.1347765380859375,0.1347558837890625,0.13473521728515625,0.13471461181640626,0.1346939697265625,0.1346733642578125,0.13465279541015626,0.13463221435546874,0.1346116455078125,0.134591064453125,0.1345705322265625,0.13454998779296876,0.134529443359375,0.1345089599609375,0.134488427734375,0.1344679443359375,0.134447509765625,0.1344270263671875,0.1344065673828125,0.134386083984375,0.1343656982421875,0.13434530029296876,0.134324853515625,0.13430445556640624,0.1342840576171875,0.13426365966796874,0.1342433349609375,0.13422298583984374,0.1342026123046875,0.1341823486328125,0.1341619873046875,0.13414169921875,0.134121435546875,0.13410107421875,0.13408084716796875,0.13406053466796874,0.1340403076171875,0.134020068359375,0.1339998291015625,0.13397960205078124,0.13395941162109376,0.1339392333984375,0.13391904296875,0.133898876953125,0.1338787109375,0.1338585693359375,0.13383839111328125,0.1338182861328125,0.13379814453125,0.133778076171875,0.133757958984375,0.13373787841796875,0.13371781005859376,0.1336977294921875,0.1336777099609375,0.1336576904296875,0.13363765869140626,0.1336176025390625,0.133597607421875,0.1335776123046875,0.1335576171875,0.13353763427734375,0.13351768798828126,0.13349774169921874,0.133477783203125,0.133457861328125,0.133437939453125,0.1334180419921875,0.133398095703125,0.1333781982421875,0.133358349609375,0.1333384765625,0.133318603515625,0.1332987548828125,0.13327894287109374,0.13325908203125,0.133239306640625,0.1332195068359375,0.1331997314453125,0.133179931640625,0.13316015625,0.1331404052734375,0.13312066650390625,0.133100927734375,0.1330811767578125,0.1330614501953125,0.133041748046875,0.1330220458984375,0.13300235595703125,0.1329826904296875,0.1329630126953125,0.13294337158203126,0.13292374267578125,0.13290408935546874,0.13288447265625,0.13286483154296874,0.13284525146484374,0.13282568359375,0.1328060791015625,0.1327864990234375,0.13276695556640625,0.13274739990234374,0.1327278564453125,0.1327083251953125,0.132688818359375,0.132669287109375,0.13264981689453126,0.132630322265625,0.13261083984375,0.1325913330078125,0.13257191162109375,0.1325524169921875,0.1325330078125,0.13251357421875,0.13249415283203125,0.13247471923828125,0.132455322265625,0.1324359619140625,0.13241661376953126,0.1323972412109375,0.1323778564453125,0.13235853271484374,0.1323391845703125,0.132319873046875,0.13230057373046875,0.1322812255859375,0.13226197509765625,0.13224267578125,0.1322233642578125,0.13220411376953126,0.13218486328125,0.1321656005859375,0.13214638671875,0.1321271484375,0.13210791015625,0.1320887451171875,0.13206951904296876,0.132050341796875,0.1320311767578125,0.13201199951171874,0.131992822265625,0.13197369384765625,0.13195455322265626,0.13193543701171875,0.13191630859375,0.1318971923828125,0.13187808837890624,0.13185899658203126,0.13183995361328124,0.131820849609375,0.13180184326171876,0.1317827880859375,0.1317637451171875,0.1317447021484375,0.13172564697265626,0.13170667724609375,0.13168770751953124,0.1316686767578125,0.13164971923828125,0.13163076171875,0.13161177978515626,0.1315928466796875,0.131573876953125,0.13155494384765626,0.13153603515625,0.13151712646484376,0.13149820556640626,0.131479345703125,0.1314604248046875,0.1314415771484375,0.1314226806640625,0.13140382080078125,0.1313849853515625,0.131366162109375,0.131347314453125,0.131328515625,0.13130970458984376,0.13129088134765626,0.131272119140625,0.1312533203125,0.13123455810546875,0.1312157958984375,0.1311970458984375,0.131178271484375,0.13115955810546875,0.1311408447265625,0.13112213134765624,0.1311034423828125,0.131084716796875,0.1310660400390625,0.13104736328125,0.1310286865234375,0.1310100341796875,0.1309913818359375,0.1309727294921875,0.13095411376953126,0.1309354736328125,0.1309169189453125,0.1308982666015625,0.1308796875,0.13086109619140626,0.1308425048828125,0.130823974609375,0.13080540771484375,0.13078685302734375,0.13076834716796876,0.1307497802734375,0.13073123779296875,0.13071279296875,0.13069425048828126,0.13067579345703126,0.13065723876953125,0.1306387939453125,0.1306203125,0.1306018798828125,0.1305834716796875,0.130564990234375,0.13054656982421875,0.130528173828125,0.130509765625,0.13049136962890626,0.13047294921875,0.1304545654296875,0.130436181640625,0.13041781005859376,0.1303994873046875,0.1303811279296875,0.1303627685546875,0.1303444580078125,0.13032613525390624,0.1303078125,0.1302895263671875,0.13027122802734376,0.1302529541015625,0.1302346435546875,0.13021640625,0.13019813232421876,0.13017989501953126,0.13016163330078126,0.130143408203125,0.13012515869140626,0.13010697021484374,0.1300887451171875,0.1300705810546875,0.13005240478515626,0.130034228515625,0.1300160400390625,0.1299978759765625,0.1299797119140625,0.1299615234375,0.129943408203125,0.1299253173828125,0.1299072021484375,0.1298890625,0.12987099609375,0.12985289306640624,0.1298347900390625,0.129816748046875,0.129798681640625,0.1297805908203125,0.12976256103515624,0.12974451904296874,0.12972646484375,0.129708447265625,0.12969046630859374,0.1296724365234375,0.129654443359375,0.12963643798828126,0.12961845703125,0.12960050048828126,0.12958253173828124,0.129564599609375,0.1295466552734375,0.1295287353515625,0.1295107666015625,0.12949283447265625,0.12947493896484374,0.12945703125,0.1294391357421875,0.1294212158203125,0.129403369140625,0.129385498046875,0.1293676513671875,0.1293498046875,0.12933194580078125,0.129314111328125,0.1292962890625,0.129278466796875,0.12926064453125,0.129242822265625,0.12922506103515624,0.129207275390625,0.129189501953125,0.1291717529296875,0.12915396728515624,0.12913623046875,0.1291184814453125,0.129100732421875,0.12908302001953126,0.1290653076171875,0.12904755859375,0.12902982177734376,0.1290121826171875,0.128994482421875,0.12897684326171874,0.12895914306640624,0.12894146728515626,0.128923828125,0.1289061767578125,0.12888853759765626,0.1288709228515625,0.1288532958984375,0.128835693359375,0.12881805419921874,0.1288004638671875,0.12878291015625,0.1287653076171875,0.12874779052734375,0.12873018798828126,0.128712646484375,0.1286950927734375,0.1286775390625,0.1286599853515625,0.1286424560546875,0.12862496337890625,0.128607470703125,0.12858994140625,0.12857249755859376,0.1285550048828125,0.12853748779296875,0.1285200439453125,0.128502587890625,0.12848515625,0.1284677001953125,0.1284502685546875,0.128432861328125,0.1284154296875,0.1283980224609375,0.1283805908203125,0.128363232421875,0.12834583740234376,0.12832841796875,0.12831104736328125,0.12829371337890624,0.12827635498046874,0.1282590087890625,0.12824168701171876,0.12822432861328126,0.12820701904296875,0.128189697265625,0.12817237548828125,0.1281551025390625,0.12813779296875,0.128120458984375,0.12810323486328126,0.12808597412109374,0.1280686767578125,0.12805146484375,0.12803419189453125,0.12801695556640624,0.1279997314453125,0.12798251953125,0.12796529541015625,0.1279481201171875,0.127930908203125,0.1279136962890625,0.12789652099609375,0.127879345703125,0.127862158203125,0.12784501953125,0.127827880859375,0.1278107177734375,0.12779354248046876,0.12777642822265625,0.127759326171875,0.12774222412109376,0.127725146484375,0.12770804443359374,0.12769091796875,0.12767381591796875,0.12765675048828126,0.127639697265625,0.12762261962890625,0.12760555419921876,0.1275885009765625,0.1275714599609375,0.1275544677734375,0.1275374267578125,0.12752042236328126,0.1275033935546875,0.12748642578125,0.12746939697265625,0.12745242919921876,0.1274354736328125,0.12741844482421874,0.12740150146484375,0.1273845458984375,0.1273676025390625,0.1273506591796875,0.127333740234375,0.127316796875,0.12729990234375,0.1272829833984375,0.12726610107421876,0.1272491943359375,0.12723228759765626,0.12721541748046875,0.12719853515625,0.1271816650390625,0.1271648193359375,0.12714794921875,0.1271311279296875,0.12711431884765625,0.1270974609375,0.127080615234375,0.1270638427734375,0.12704700927734375,0.12703021240234375,0.127013427734375,0.12699664306640626,0.1269798583984375,0.1269630859375,0.126946337890625,0.12692955322265626,0.126912841796875,0.12689609375,0.12687933349609376,0.1268626220703125,0.1268458984375,0.12682916259765625,0.1268124755859375,0.12679580078125,0.1267791015625,0.12676240234375,0.1267457275390625,0.126729052734375,0.1267124267578125,0.126695751953125,0.12667908935546876,0.1266624755859375,0.1266458251953125,0.12662921142578126,0.12661259765625,0.12659600830078124,0.1265793701171875,0.12656278076171876,0.1265461669921875,0.12652958984375,0.12651300048828126,0.12649647216796875,0.1264798828125,0.126463330078125,0.12644676513671874,0.12643023681640625,0.126413720703125,0.12639716796875,0.12638065185546876,0.12636412353515625,0.12634765625,0.12633115234375,0.1263146484375,0.12629818115234376,0.1262817138671875,0.12626527099609375,0.1262487548828125,0.12623233642578124,0.126215869140625,0.12619947509765625,0.12618299560546875,0.1261665771484375,0.126150146484375,0.1261337158203125,0.126117333984375,0.126100927734375,0.1260845458984375,0.12606812744140625,0.12605177001953125,0.12603538818359375,0.12601904296875,0.12600267333984375,0.125986328125,0.12596998291015624,0.12595364990234376,0.1259373291015625,0.12592100830078126,0.125904638671875,0.12588837890625,0.1258720458984375,0.12585576171875,0.1258394775390625,0.12582318115234375,0.1258069091796875,0.125790625,0.1257744140625,0.12575811767578124,0.1257418701171875,0.1257256591796875,0.1257093994140625,0.12569315185546875,0.12567696533203124,0.1256607421875,0.12564454345703124,0.1256283203125,0.1256121337890625,0.12559595947265625,0.12557977294921874,0.1255635986328125,0.1255474365234375,0.12553126220703126,0.12551513671875,0.1254989501953125,0.1254828369140625,0.12546669921875,0.12545057373046875,0.12543446044921874,0.1254183349609375,0.12540220947265626,0.1253861328125,0.1253700439453125,0.125353955078125,0.125337841796875,0.12532177734375,0.1253057373046875,0.1252896728515625,0.12527359619140624,0.1252575439453125,0.125241552734375,0.12522548828125,0.1252094970703125,0.12519345703125,0.12517740478515624,0.12516143798828125,0.125145458984375,0.125129443359375,0.125113427734375,0.1250974365234375,0.125081494140625,0.1250655029296875,0.125049560546875,0.1250335693359375,0.1250176513671875,0.125001708984375,0.1249857666015625,0.12496986083984375,0.1249539306640625,0.1249380126953125,0.12492210693359375,0.12490621337890626,0.12489029541015625,0.1248743896484375,0.12485853271484375,0.1248426513671875,0.12482679443359375,0.1248109130859375,0.12479508056640624,0.1247791748046875,0.12476339111328125,0.1247475341796875,0.124731689453125,0.124715869140625,0.124700048828125,0.124684228515625,0.12466844482421875,0.12465262451171875,0.12463682861328125,0.124621044921875,0.1246052734375,0.124589501953125,0.12457373046875,0.124557958984375,0.1245421875,0.1245264404296875,0.124510693359375,0.12449498291015625,0.12447926025390625,0.124463525390625,0.12444779052734375,0.1244321044921875,0.12441640625,0.12440069580078125,0.12438499755859375,0.1243693359375,0.1243536376953125,0.1243379638671875,0.12432230224609375,0.124306640625,0.12429095458984375,0.12427530517578125,0.1242596923828125,0.1242440673828125,0.12422843017578125,0.12421279296875,0.12419720458984375,0.12418154296875,0.1241659912109375,0.1241503662109375,0.12413477783203125,0.12411917724609375,0.12410361328125,0.1240880615234375,0.1240724609375,0.1240569091796875,0.124041357421875,0.124025830078125,0.12401025390625,0.1239947265625,0.1239792236328125,0.1239636474609375,0.12394814453125,0.12393265380859375,0.12391712646484375,0.12390166015625,0.12388614501953125,0.123870703125,0.1238552001953125,0.1238397216796875,0.12382423095703125,0.1238087646484375,0.12379332275390625,0.123777880859375,0.1237624267578125,0.1237469970703125,0.12373153076171875,0.12371612548828125,0.123700732421875,0.1236852783203125,0.123669873046875,0.1236544677734375,0.12363907470703125,0.123623681640625,0.12360830078125,0.1235929443359375,0.1235775634765625,0.12356220703125,0.12354683837890625,0.1235314697265625,0.123516162109375,0.12350078125,0.12348546142578125,0.1234701171875,0.1234548095703125,0.12343946533203125,0.12342415771484375,0.12340887451171875,0.1233935302734375,0.123378271484375,0.12336297607421876,0.12334769287109375,0.12333243408203125,0.12331715087890625,0.1233018798828125]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss by time\"};\n",
       "\n",
       "  Plotly.plot('plot-117118284', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mlossSeq\u001b[39m: \u001b[32mIndexedSeq\u001b[39m[\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m.\u001b[32m<refinement>\u001b[39m.this.type.\u001b[32mOutputData\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m0.230167041015625\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mplot\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mScatter\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mScatter\u001b[39m(\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mres8_2\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-117118284\"\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lossSeq = for (iteration <- 0 until 2000) yield {\n",
    "  val loss = lossFunction.train(trainData :: vectorizedTrainExpectResult :: HNil)\n",
    "  if(iteration % 100 == 0){\n",
    "    println(s\"at iteration $iteration loss is $loss\")\n",
    "  }\n",
    "  loss\n",
    "}\n",
    "\n",
    "val plot = Seq(\n",
    "  Scatter(lossSeq.indices, lossSeq)\n",
    ")\n",
    "\n",
    "plot.plot(\n",
    "  title = \"loss by time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用已经处理好的测试数据来验证神经网络的准确率。准确率应该在32%左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result is 32.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mright\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m32.0\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val right = Utils.getAccuracy(myNeuralNetwork.predict(testData), testExpectResult)\n",
    "println(s\"the result is $right %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[完整代码](https://github.com/izhangzhihao/deeplearning-tutorial/blob/master/src/main/scala/com/thoughtworks/deeplearning/tutorial/SoftmaxLinearClassifier.scala)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
