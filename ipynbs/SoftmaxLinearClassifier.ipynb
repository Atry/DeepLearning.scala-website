{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景\n",
    "\n",
    "这篇文章我们将使用softmax分类器一起来构建一个简单的图像分类神经网络，其准确率可以达到32%。我们将使用[cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)的图像和标签数据来训练这个神经网络。Softmax分类器是二元逻辑回归泛化到多元的情况。Softmax分类器的输出不是得分，而是对应类别的概率。我们开始吧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似前一篇教程[GetStarted](https://thoughtworksinc.github.io/DeepLearning.scala/demo/GettingStarted.html)，我们需要引入DeepLearning.scala的各个类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                 \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.{FileInputStream, InputStream}\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableHList._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableDouble._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableAny._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.{\n",
       "  DifferentiableHList,\n",
       "  DifferentiableINDArray,\n",
       "  Layer,\n",
       "  Symbolic\n",
       "}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Layer.Batch\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathFunctions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathMethods./\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathOps\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.cpu.nativecpu.NDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.ops.transforms.Transforms\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4s.Implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mshapeless._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.immutable.IndexedSeq\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $plugin.$ivy.`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:2.0.0`\n",
    "\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableany:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablenothing:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableseq:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiabledouble:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablefloat:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablehlist:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableindarray:1.0.0-RC7`\n",
    "import $ivy.`org.rauschig:jarchivelib:0.5.0`\n",
    "\n",
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.0`\n",
    "\n",
    "import java.io.{FileInputStream, InputStream}\n",
    "\n",
    "\n",
    "import com.thoughtworks.deeplearning\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import com.thoughtworks.deeplearning.DifferentiableHList._\n",
    "import com.thoughtworks.deeplearning.DifferentiableDouble._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray._\n",
    "import com.thoughtworks.deeplearning.DifferentiableAny._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
    "import com.thoughtworks.deeplearning.{\n",
    "  DifferentiableHList,\n",
    "  DifferentiableINDArray,\n",
    "  Layer,\n",
    "  Symbolic\n",
    "}\n",
    "import com.thoughtworks.deeplearning.Layer.Batch\n",
    "import com.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
    "import com.thoughtworks.deeplearning.Symbolic._\n",
    "import com.thoughtworks.deeplearning.Poly.MathFunctions._\n",
    "import com.thoughtworks.deeplearning.Poly.MathMethods./\n",
    "import com.thoughtworks.deeplearning.Poly.MathOps\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.cpu.nativecpu.NDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
    "import org.nd4j.linalg.ops.transforms.Transforms\n",
    "import org.nd4s.Implicits._\n",
    "import shapeless._\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._\n",
    "\n",
    "import scala.collection.immutable.IndexedSeq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了减少`jupyter-scala`输出的行数，避免页面输出太长，需要设置`pprintConfig`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pprintConfig() = pprintConfig().copy(height = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "为了从CIFAR10 database中读取训练数据和测试数据的图片和分类信息。我们需要`import $file.ReadCIFAR10ToNDArray`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling ReadCIFAR10ToNDArray.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                   \n",
       "\n",
       "//加载train数据,我们读取1000条数据作为训练数据\n",
       "\u001b[39m\n",
       "\u001b[36mtrainNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.23, 0.17, 0.20, 0.27, 0.38, 0.46, 0.54, 0.57, 0.58, 0.58, 0.51, 0.49, 0.55, 0.56, 0.54, 0.50, 0.54, 0.52, 0.48, 0.54, 0.54, 0.52, 0.53, 0.54, 0.59, 0.64, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.ReadCIFAR10ToNDArray\n",
    "\n",
    "//加载train数据,我们读取1000条数据作为训练数据\n",
    "val trainNDArray =\n",
    "  ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/data_batch_1.bin\", 1000)\n",
    "\n",
    "//加载测试数据，我们读取100条作为测试数据\n",
    "val testNDArray =\n",
    "  ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/test_batch.bin\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便softmax分类器处理数据，我们先处理标签数据([one hot encoding](https://en.wikipedia.org/wiki/One-hot))：将N行一列的NDArray转换为N行NumberOfClasses列的NDArray，每行对应的正确分类的值为1，其它列的值为0。这里区分训练集和测试集的原因是为了能看出网络是否被过度训练导致[过拟合](https://en.wikipedia.org/wiki/Overfitting)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Utils.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.23, 0.17, 0.20, 0.27, 0.38, 0.46, 0.54, 0.57, 0.58, 0.58, 0.51, 0.49, 0.55, 0.56, 0.54, 0.50, 0.54, 0.52, 0.48, 0.54, 0.54, 0.52, 0.53, 0.54, 0.59, 0.64, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtrainExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [6.00, 9.00, 9.00, 4.00, 1.00, 1.00, 2.00, 7.00, 8.00, 3.00, 4.00, 7.00, 7.00, 2.00, 9.00, 9.00, 9.00, 3.00, 2.00, 6.00, 4.00, 3.00, 6.00, 6.00, 2.00, 6.00, 3.0\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.0\u001b[33m...\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$    \n",
       "\n",
       "//CIFAR10中的图片共有10个分类(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
       "\u001b[39m\n",
       "\u001b[36mNumberOfClasses\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36mvectorizedTrainExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00],\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mvectorizedTestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainData = trainNDArray.head\n",
    "val testData = testNDArray.head\n",
    "\n",
    "val trainExpectResult = trainNDArray.tail.head\n",
    "val testExpectResult = testNDArray.tail.head\n",
    "\n",
    "import $file.Utils\n",
    "\n",
    "//CIFAR10中的图片共有10个分类(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
    "val NumberOfClasses: Int = 10\n",
    "\n",
    "val vectorizedTrainExpectResult = Utils.makeVectorized(trainExpectResult, NumberOfClasses)\n",
    "val vectorizedTestExpectResult = Utils.makeVectorized(testExpectResult, NumberOfClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使用softmax分类器(softmax分类器是softmax和一个全连接组合起来的神经网络)，我们需要先编写softmax函数,公式：![](https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msoftmax\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(implicit scores: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val expScores = exp(scores)\n",
    "  expScores / expScores.sum(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全连接层需要设置学习率，学习率是Weight变化的快慢的直观描述，学习率设置的过小会导致loss下降的很慢，需要更长时间来训练，学习率设置的过大虽然刚开始下降很快但是会导致在接近最低点的时候在附近徘徊loss下降会非常慢。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36moptimizer\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit def optimizer: Optimizer = new LearningRate {\n",
    "  def currentLearningRate() = 0.00001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个全连接层并初始化Weight，Weight应该是一个N*NumberOfClasses的INDArray,每个图片对应每个分类都有一个评分。[什么是Weight](https://github.com/ThoughtWorksInc/DeepLearning.scala/wiki/Getting-Started#231--weight-intialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcreateMyNeuralNetwork\u001b[39m\n",
       "\u001b[36mmyNeuralNetwork\u001b[39m: (\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m[\u001b[32mINDArray\u001b[39m]{type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray;type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray})#\u001b[32m@\u001b[39m = Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum(Exp(Identity()),WrappedArray(1)))),Dot(Identity(),Weight([[-0.00, 0.00, -0.00, 0.00, 0.00, -0.00, 0.00, \u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createMyNeuralNetwork(implicit input: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val initialValueOfWeight = Nd4j.randn(3072, NumberOfClasses) * 0.001\n",
    "  val weight: INDArray @Symbolic = initialValueOfWeight.toWeight\n",
    "  val result: INDArray @Symbolic = input dot weight\n",
    "  softmax.compose(result) //对结果调用softmax方法，压缩结果值在0到1之间方便处理\n",
    "}\n",
    "val myNeuralNetwork = createMyNeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了判断神经网络判断的结果好坏，我们需要编写损失函数Loss Function，这里我们使用cross-entropy loss将此次判断的结果和真实结果进行对比并返回评分，公式：\n",
    "![](https://zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlossFunction\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lossFunction(implicit pair: (INDArray :: INDArray :: HNil) @Symbolic): Double @Symbolic = {\n",
    "  val input = pair.head\n",
    "  val expectedOutput = pair.tail.head\n",
    "  val probabilities = myNeuralNetwork.compose(input)\n",
    "\n",
    "  -(expectedOutput * log(probabilities)).mean //此处和准备一节中的交叉熵损失对应\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了观察神经网络训练的过程，我们需要输出`loss`，在训练神经网络时，loss的变化趋势应该是越来越低的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is :0.2303436767578125\n",
      "loss is :0.1908893310546875\n",
      "loss is :0.17819090576171875\n",
      "loss is :0.17023267822265625\n",
      "loss is :0.1642774658203125\n",
      "loss is :0.15943809814453125\n",
      "loss is :0.1553156982421875\n",
      "loss is :0.1516980224609375\n",
      "loss is :0.14845870361328126\n",
      "loss is :0.14551583251953126\n",
      "loss is :0.1428130615234375\n",
      "loss is :0.1403093017578125\n",
      "loss is :0.13797386474609374\n",
      "loss is :0.13578282470703126\n",
      "loss is :0.13371734619140624\n",
      "loss is :0.13176209716796874\n",
      "loss is :0.1299045654296875\n",
      "loss is :0.12813433837890625\n",
      "loss is :0.126442578125\n",
      "loss is :0.124821923828125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-1291879632\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0,1989.0,1990.0,1991.0,1992.0,1993.0,1994.0,1995.0,1996.0,1997.0,1998.0,1999.0],\"y\":[0.2303436767578125,0.22865458984375,0.2276819091796875,0.2267847412109375,0.22591845703125,0.225078173828125,0.224262353515625,0.2234701171875,0.222700244140625,0.221951904296875,0.2212241943359375,0.22051630859375,0.21982734375,0.219156591796875,0.2185033203125,0.21786669921875,0.217246240234375,0.2166412353515625,0.216051025390625,0.2154750244140625,0.214912744140625,0.2143635986328125,0.2138271484375,0.2133028564453125,0.212790283203125,0.21228896484375,0.21179853515625,0.211318603515625,0.21084873046875,0.2103885986328125,0.209937890625,0.2094962158203125,0.209063330078125,0.2086388427734375,0.20822255859375,0.20781416015625,0.20741337890625,0.2070199462890625,0.20663369140625,0.206254345703125,0.2058817138671875,0.20551552734375,0.2051556640625,0.2048018798828125,0.20445396728515625,0.20411181640625,0.2037751953125,0.203443994140625,0.203118017578125,0.20279713134765626,0.20248114013671875,0.20217000732421875,0.20186348876953125,0.2015615234375,0.2012639892578125,0.20097078857421874,0.20068170166015625,0.20039671630859374,0.20011566162109376,0.19983843994140624,0.19956502685546876,0.19929522705078126,0.19902906494140624,0.19876636962890626,0.19850704345703124,0.19825103759765625,0.19799820556640624,0.19774859619140625,0.1975020751953125,0.19725849609375,0.1970178466796875,0.196780078125,0.1965451416015625,0.196312939453125,0.1960833740234375,0.1958564697265625,0.19563203125,0.19541019287109376,0.1951907470703125,0.19497374267578124,0.194758984375,0.1945466064453125,0.194336376953125,0.1941284423828125,0.1939226318359375,0.19371885986328125,0.193517236328125,0.19331754150390626,0.19311990966796874,0.1929241943359375,0.19273038330078124,0.1925385009765625,0.19234833984375,0.19216004638671874,0.191973486328125,0.1917886474609375,0.191605517578125,0.1914240478515625,0.19124422607421876,0.19106600341796875,0.1908893310546875,0.190714208984375,0.19054061279296874,0.19036846923828124,0.1901978759765625,0.19002864990234375,0.18986090087890625,0.1896945068359375,0.189529443359375,0.189365771484375,0.1892033935546875,0.18904234619140625,0.1888825439453125,0.1887239501953125,0.1885667236328125,0.188410595703125,0.188255712890625,0.1881020263671875,0.18794947509765625,0.1877980224609375,0.1876477294921875,0.1874985595703125,0.187350439453125,0.1872034423828125,0.1870574951171875,0.18691251220703126,0.1867686279296875,0.18662569580078125,0.1864837890625,0.1863428466796875,0.186202880859375,0.18606387939453126,0.1859257568359375,0.1857886474609375,0.1856523681640625,0.185517041015625,0.1853825439453125,0.1852490234375,0.18511624755859374,0.18498438720703125,0.1848533203125,0.184723095703125,0.184593701171875,0.18446507568359374,0.1843373046875,0.184210302734375,0.1840840087890625,0.18395850830078125,0.1838337646484375,0.1837097900390625,0.18358653564453126,0.18346396484375,0.18334215087890626,0.183221044921875,0.183100634765625,0.1829808837890625,0.1828618408203125,0.18274349365234374,0.18262576904296876,0.18250867919921876,0.18239227294921875,0.18227650146484375,0.18216134033203124,0.1820468505859375,0.18193294677734376,0.18181962890625,0.181706982421875,0.181594921875,0.1814833740234375,0.1813724853515625,0.1812621337890625,0.1811523681640625,0.1810431640625,0.1809344970703125,0.180826416015625,0.18071883544921874,0.1806117919921875,0.180505322265625,0.180399365234375,0.18029393310546876,0.18018900146484376,0.1800845947265625,0.1799806884765625,0.179877294921875,0.17977435302734374,0.179671923828125,0.17957001953125,0.1794685302734375,0.17936754150390624,0.17926702880859374,0.1791669677734375,0.1790673828125,0.178968212890625,0.17886954345703124,0.1787712890625,0.178673486328125,0.17857607421875,0.178479150390625,0.178382666015625,0.17828662109375,0.17819090576171875,0.178095703125,0.17800086669921875,0.17790645751953124,0.1778124267578125,0.1777188232421875,0.1776256103515625,0.1775327880859375,0.17744031982421876,0.1773482421875,0.1772565673828125,0.177165283203125,0.17707437744140625,0.17698382568359375,0.17689361572265624,0.1768038330078125,0.17671435546875,0.1766253173828125,0.1765365478515625,0.1764481689453125,0.17636016845703126,0.17627244873046874,0.1761850830078125,0.176098095703125,0.17601142578125,0.17592509765625,0.1758390869140625,0.17575340576171875,0.1756680419921875,0.175583056640625,0.17549833984375,0.17541396484375,0.1753298828125,0.1752460693359375,0.175162646484375,0.1750795166015625,0.1749967041015625,0.17491414794921875,0.17483193359375,0.174749951171875,0.174668310546875,0.1745869873046875,0.1745058837890625,0.1744251220703125,0.17434462890625,0.17426444091796875,0.1741844970703125,0.17410487060546875,0.17402548828125,0.1739464111328125,0.1738676025390625,0.173789013671875,0.1737107177734375,0.17363271484375,0.17355498046875,0.1734774658203125,0.17340023193359375,0.1733232666015625,0.17324654541015624,0.1731700439453125,0.17309384765625,0.17301790771484374,0.17294217529296874,0.1728667236328125,0.17279150390625,0.17271650390625,0.172641796875,0.172567236328125,0.172493017578125,0.1724189697265625,0.17234517822265624,0.17227164306640624,0.1721983154296875,0.17212523193359375,0.17205234375,0.1719796875,0.1719072509765625,0.17183505859375,0.1717630859375,0.1716913330078125,0.17161978759765625,0.17154852294921874,0.17147734375,0.1714064697265625,0.1713358154296875,0.17126533203125,0.17119510498046875,0.1711250244140625,0.1710552001953125,0.1709855712890625,0.17091610107421876,0.1708468994140625,0.17077783203125,0.17070902099609375,0.17064039306640624,0.1705718994140625,0.1705037109375,0.17043564453125,0.1703677978515625,0.170300146484375,0.17023267822265625,0.1701654296875,0.170098291015625,0.17003140869140626,0.16996473388671876,0.16989814453125,0.16983177490234375,0.1697656494140625,0.169699658203125,0.169633837890625,0.16956822509765626,0.169502734375,0.1694374755859375,0.16937236328125,0.1693074462890625,0.169242724609375,0.1691781494140625,0.1691137451171875,0.16904947509765625,0.1689854248046875,0.16892152099609375,0.1688577880859375,0.1687942138671875,0.16873082275390625,0.1686676025390625,0.1686045166015625,0.168541552734375,0.1684788330078125,0.1684162109375,0.16835380859375,0.168291552734375,0.168229443359375,0.1681674560546875,0.1681056396484375,0.168043994140625,0.16798251953125,0.16792117919921876,0.1678599609375,0.1677989501953125,0.1677380615234375,0.16767733154296874,0.16761671142578125,0.1675562744140625,0.16749598388671874,0.1674358154296875,0.1673758056640625,0.16731597900390624,0.1672562255859375,0.1671966796875,0.1671372314453125,0.16707791748046874,0.167018798828125,0.166959765625,0.16690089111328124,0.1668421630859375,0.16678353271484375,0.16672510986328126,0.1666668212890625,0.1666085693359375,0.1665505615234375,0.166492626953125,0.1664348388671875,0.16637711181640624,0.16631962890625,0.16626220703125,0.16620498046875,0.16614781494140626,0.16609083251953125,0.1660339599609375,0.1659772216796875,0.165920556640625,0.1658640625,0.1658076904296875,0.165751416015625,0.16569527587890626,0.16563929443359374,0.16558343505859374,0.16552764892578126,0.16547203369140626,0.1654165283203125,0.1653611083984375,0.16530582275390626,0.1652506591796875,0.165195654296875,0.16514068603515625,0.1650859130859375,0.16503125,0.1649766845703125,0.16492218017578125,0.16486785888671876,0.1648136474609375,0.16475948486328126,0.1647054931640625,0.164651611328125,0.1645978271484375,0.1645441650390625,0.16449061279296875,0.16443717041015626,0.1643838623046875,0.1643306396484375,0.1642774658203125,0.16422449951171875,0.16417156982421874,0.16411875,0.1640660888671875,0.16401346435546876,0.16396107177734376,0.1639087158203125,0.16385643310546874,0.163804248046875,0.1637521484375,0.1637002197265625,0.16364837646484376,0.1635966552734375,0.16354495849609374,0.163493408203125,0.1634419677734375,0.1633906494140625,0.16333935546875,0.16328824462890626,0.16323714599609376,0.1631862060546875,0.1631354248046875,0.1630846435546875,0.16303397216796875,0.16298343505859375,0.1629329833984375,0.16288258056640625,0.16283233642578124,0.16278211669921874,0.16273203125,0.16268203125,0.16263212890625,0.1625823486328125,0.1625326416015625,0.162483056640625,0.16243348388671874,0.1623840576171875,0.16233470458984375,0.16228548583984376,0.1622363037109375,0.16218721923828125,0.16213824462890625,0.16208934326171875,0.16204051513671874,0.16199183349609375,0.16194320068359375,0.16189462890625,0.16184622802734375,0.161797802734375,0.1617495361328125,0.161701318359375,0.16165322265625,0.16160518798828125,0.1615572509765625,0.161509423828125,0.16146165771484375,0.1614139404296875,0.16136636962890624,0.16131884765625,0.1612713623046875,0.16122403564453125,0.1611767578125,0.1611295654296875,0.161082470703125,0.1610354248046875,0.1609885009765625,0.1609416259765625,0.1608948486328125,0.1608481201171875,0.16080150146484376,0.1607549560546875,0.1607084716796875,0.16066212158203125,0.160615771484375,0.1605695556640625,0.1605234130859375,0.160477294921875,0.1604313232421875,0.1603853759765625,0.16033953857421876,0.1602937744140625,0.1602480712890625,0.16020242919921876,0.1601569091796875,0.16011142578125,0.160066015625,0.1600206787109375,0.15997545166015625,0.1599302490234375,0.1598851318359375,0.15984013671875,0.1597951416015625,0.15975029296875,0.15970546875,0.15966072998046876,0.15961607666015626,0.159571435546875,0.159526904296875,0.1594824951171875,0.15943809814453125,0.1593937744140625,0.15934954833984374,0.15930535888671876,0.1592612548828125,0.15921719970703124,0.15917325439453126,0.1591293701171875,0.1590855224609375,0.159041748046875,0.15899808349609376,0.15895445556640625,0.15891090087890625,0.15886739501953126,0.1588239990234375,0.1587806396484375,0.1587373291015625,0.158694091796875,0.158650927734375,0.15860784912109374,0.15856484375,0.1585218505859375,0.1584789794921875,0.1584361083984375,0.158393359375,0.15835062255859375,0.1583080078125,0.1582654296875,0.158222900390625,0.15818045654296875,0.1581380859375,0.158095751953125,0.158053515625,0.1580113037109375,0.1579691650390625,0.157927099609375,0.157885107421875,0.1578431396484375,0.15780123291015624,0.157759423828125,0.15771767578125,0.1576759521484375,0.15763428955078124,0.1575927001953125,0.157551171875,0.157509716796875,0.15746832275390624,0.1574269775390625,0.157385693359375,0.15734443359375,0.157303271484375,0.1572621826171875,0.1572211181640625,0.1571801025390625,0.1571391845703125,0.15709830322265625,0.157057470703125,0.1570167236328125,0.1569760498046875,0.15693536376953124,0.1568947509765625,0.15685423583984376,0.1568137939453125,0.15677333984375,0.1567329833984375,0.15669263916015624,0.1566524169921875,0.156612255859375,0.15657208251953125,0.156531982421875,0.156491943359375,0.1564519775390625,0.1564120361328125,0.1563721435546875,0.156332373046875,0.1562926025390625,0.15625286865234375,0.1562132568359375,0.156173681640625,0.156134130859375,0.156094580078125,0.15605518798828125,0.1560157958984375,0.1559764404296875,0.1559371826171875,0.15589794921875,0.1558587646484375,0.15581964111328125,0.1557805419921875,0.155741552734375,0.1557025634765625,0.155663623046875,0.1556247802734375,0.1555859619140625,0.1555471923828125,0.15550850830078125,0.1554698486328125,0.1554312255859375,0.15539263916015625,0.155354150390625,0.1553156982421875,0.15527728271484376,0.1552388671875,0.15520059814453124,0.1551623291015625,0.1551240966796875,0.1550859375,0.1550478515625,0.1550097412109375,0.15497174072265624,0.1549337890625,0.154895849609375,0.154857958984375,0.15482015380859376,0.1547823974609375,0.15474461669921874,0.15470694580078126,0.1546693115234375,0.15463175048828126,0.1545941650390625,0.15455667724609376,0.1545192626953125,0.1544818115234375,0.154444482421875,0.1544071533203125,0.15436993408203126,0.15433272705078124,0.154295556640625,0.15425836181640626,0.15422132568359376,0.1541843017578125,0.15414732666015624,0.15411038818359374,0.154073486328125,0.15403663330078124,0.15399981689453124,0.15396307373046875,0.1539263671875,0.1538896728515625,0.15385302734375,0.153816455078125,0.15377991943359376,0.153743408203125,0.15370697021484375,0.15367054443359374,0.1536342041015625,0.1535978759765625,0.15356160888671874,0.1535253662109375,0.15348919677734374,0.15345303955078124,0.15341695556640625,0.153380859375,0.1533448486328125,0.153308837890625,0.1532729248046875,0.1532370361328125,0.1532011962890625,0.153165380859375,0.15312962646484374,0.1530939208984375,0.153058203125,0.15302259521484374,0.15298697509765624,0.15295146484375,0.15291591796875,0.1528803955078125,0.1528450439453125,0.1528096435546875,0.15277427978515626,0.1527389404296875,0.1527036865234375,0.15266845703125,0.15263328857421876,0.1525980712890625,0.15256300048828125,0.1525279296875,0.1524928955078125,0.1524578857421875,0.1524229736328125,0.152388037109375,0.152353173828125,0.152318359375,0.152283544921875,0.1522488037109375,0.1522140625,0.1521794189453125,0.15214482421875,0.1521101806640625,0.152075634765625,0.15204112548828125,0.1520066162109375,0.1519721923828125,0.1519377685546875,0.15190340576171876,0.151869091796875,0.15183482666015624,0.15180054931640624,0.1517663330078125,0.1517321533203125,0.1516980224609375,0.1516639404296875,0.1516298828125,0.1515958251953125,0.1515618408203125,0.151527880859375,0.15149398193359376,0.1514600830078125,0.15142626953125,0.151392431640625,0.1513586669921875,0.15132496337890625,0.15129124755859374,0.151257568359375,0.1512239501953125,0.151190380859375,0.1511568115234375,0.1511233154296875,0.15108983154296876,0.151056396484375,0.1510229736328125,0.1509896484375,0.15095628662109375,0.1509229736328125,0.1508897705078125,0.150856494140625,0.15082327880859375,0.15079013671875,0.15075703125,0.15072392578125,0.150690869140625,0.1506578125,0.150624853515625,0.15059190673828124,0.1505590087890625,0.150526123046875,0.15049326171875,0.1504604736328125,0.150427685546875,0.150394921875,0.1503621826171875,0.1503295166015625,0.150296875,0.1502642822265625,0.15023170166015626,0.1501991455078125,0.1501666748046875,0.1501342041015625,0.15010174560546874,0.1500693359375,0.150036962890625,0.15000460205078125,0.149972314453125,0.149939990234375,0.14990775146484375,0.149875537109375,0.149843359375,0.14981124267578125,0.1497791259765625,0.149747021484375,0.149714990234375,0.14968294677734376,0.1496510009765625,0.14961904296875,0.1495870849609375,0.149555224609375,0.14952337646484376,0.14949154052734376,0.14945972900390625,0.1494280029296875,0.14939625244140625,0.14936455078125,0.14933291015625,0.14930126953125,0.149269677734375,0.1492380615234375,0.1492065673828125,0.149175,0.1491435546875,0.14911209716796875,0.149080712890625,0.14904927978515625,0.1490178955078125,0.14898658447265625,0.148955322265625,0.148924072265625,0.148892822265625,0.14886162109375,0.1488304443359375,0.1487992919921875,0.14876817626953126,0.1487371337890625,0.14870609130859375,0.14867503662109374,0.14864404296875,0.1486130859375,0.148582177734375,0.14855123291015626,0.14852037353515626,0.1484895263671875,0.14845870361328126,0.14842794189453126,0.14839716796875,0.148366455078125,0.1483357421875,0.14830509033203124,0.14827442626953125,0.148243798828125,0.148213232421875,0.1481826904296875,0.1481521728515625,0.1481217041015625,0.1480912109375,0.148060791015625,0.14803035888671875,0.14799996337890625,0.14796962890625,0.14793929443359374,0.147908984375,0.147878759765625,0.14784849853515625,0.1478182861328125,0.147788134765625,0.147757958984375,0.14772786865234375,0.14769775390625,0.14766767578125,0.1476376220703125,0.1476076171875,0.14757764892578126,0.147547705078125,0.14751773681640626,0.1474878173828125,0.14745797119140625,0.147428125,0.147398291015625,0.147368505859375,0.1473387451171875,0.147308984375,0.147279248046875,0.14724957275390624,0.147219921875,0.1471903076171875,0.1471607177734375,0.1471311279296875,0.14710155029296876,0.14707205810546875,0.1470425537109375,0.14701309814453126,0.14698365478515624,0.14695421142578124,0.1469248291015625,0.1468954833984375,0.14686611328125,0.1468367919921875,0.14680751953125,0.14677822265625,0.1467490234375,0.1467197998046875,0.1466906494140625,0.1466614990234375,0.14663232421875,0.1466032470703125,0.146574169921875,0.1465451171875,0.14651610107421875,0.1464870849609375,0.1464580810546875,0.146429150390625,0.146400244140625,0.14637132568359376,0.1463424560546875,0.1463135986328125,0.14628477783203125,0.14625595703125,0.14622716064453126,0.14619842529296875,0.14616973876953124,0.1461410400390625,0.14611236572265626,0.1460837158203125,0.146055078125,0.14602647705078126,0.145997900390625,0.14596932373046875,0.14594083251953124,0.1459123046875,0.145883837890625,0.1458553955078125,0.1458269775390625,0.145798583984375,0.14577021484375,0.14574183349609374,0.145713525390625,0.14568519287109374,0.1456569091796875,0.1456286865234375,0.14560042724609376,0.1455722412109375,0.1455440185546875,0.14551583251953126,0.14548770751953125,0.1454596435546875,0.14543153076171875,0.145403466796875,0.14537542724609376,0.145347412109375,0.1453194091796875,0.145291455078125,0.14526348876953124,0.1452355712890625,0.145207666015625,0.14517979736328124,0.1451519287109375,0.14512406005859374,0.1450963134765625,0.14506851806640625,0.1450407958984375,0.1450130126953125,0.144985302734375,0.1449576171875,0.14492996826171875,0.14490228271484376,0.14487467041015625,0.1448470703125,0.14481951904296875,0.1447919189453125,0.144764404296875,0.144736865234375,0.1447093994140625,0.14468193359375,0.14465450439453126,0.144627099609375,0.1445996826171875,0.14457227783203125,0.1445449462890625,0.14451763916015625,0.14449033203125,0.144463037109375,0.1444357177734375,0.14440850830078125,0.14438131103515625,0.1443541015625,0.1443269287109375,0.14429979248046876,0.14427264404296875,0.1442455322265625,0.1442184814453125,0.1441913818359375,0.1441643310546875,0.1441373291015625,0.14411031494140625,0.1440833251953125,0.1440563720703125,0.1440294189453125,0.1440025146484375,0.143975634765625,0.1439487548828125,0.14392191162109375,0.1438950927734375,0.14386827392578125,0.1438414794921875,0.1438147216796875,0.14378797607421875,0.143761279296875,0.1437345458984375,0.1437078857421875,0.1436811767578125,0.1436545654296875,0.1436279541015625,0.1436013916015625,0.143574755859375,0.1435482666015625,0.1435217041015625,0.14349521484375,0.143468701171875,0.14344222412109375,0.143415771484375,0.1433893798828125,0.1433629150390625,0.14333658447265624,0.1433102294921875,0.1432838623046875,0.14325750732421874,0.143231201171875,0.14320491943359376,0.14317862548828125,0.143152392578125,0.143126171875,0.1430999755859375,0.14307379150390626,0.14304764404296874,0.14302149658203125,0.1429953369140625,0.14296923828125,0.1429431884765625,0.14291710205078126,0.142891064453125,0.1428650634765625,0.14283907470703125,0.1428130615234375,0.142787060546875,0.1427611572265625,0.14273525390625,0.14270928955078124,0.1426834228515625,0.14265758056640626,0.1426317138671875,0.14260584716796876,0.1425801025390625,0.14255428466796874,0.142528515625,0.142502783203125,0.14247705078125,0.14245135498046874,0.14242564697265625,0.14239998779296875,0.1423743408203125,0.14234869384765625,0.14232308349609374,0.14229747314453126,0.142271923828125,0.142246337890625,0.1422208251953125,0.1421953125,0.1421697998046875,0.14214429931640624,0.1421188232421875,0.1420934326171875,0.1420679931640625,0.14204259033203126,0.1420172119140625,0.141991845703125,0.1419664794921875,0.1419411376953125,0.14191585693359374,0.14189053955078124,0.141865283203125,0.1418400390625,0.141814794921875,0.14178956298828124,0.1417643798828125,0.14173919677734376,0.1417140625,0.14168887939453126,0.1416637451171875,0.141638671875,0.1416135986328125,0.14158848876953126,0.14156346435546874,0.141538427734375,0.1415134033203125,0.14148839111328124,0.14146343994140625,0.1414384521484375,0.14141353759765626,0.1413885986328125,0.141363671875,0.141338818359375,0.1413139404296875,0.14128907470703125,0.1412642578125,0.141239404296875,0.141214599609375,0.141189794921875,0.141165087890625,0.14114033203125,0.141115576171875,0.1410908935546875,0.1410662109375,0.14104151611328125,0.141016845703125,0.14099219970703125,0.1409676025390625,0.14094296875,0.140918408203125,0.14089384765625,0.1408693115234375,0.14084473876953124,0.14082022705078126,0.14079573974609375,0.140771240234375,0.14074677734375,0.1407223388671875,0.140697900390625,0.14067344970703125,0.1406490966796875,0.14062467041015625,0.1406003173828125,0.14057598876953126,0.14055166015625,0.14052735595703125,0.14050306396484374,0.1404787841796875,0.1404544677734375,0.14043026123046876,0.1404060546875,0.1403818603515625,0.1403576171875,0.14033348388671876,0.1403093017578125,0.14028514404296874,0.1402610595703125,0.1402369384765625,0.1402128173828125,0.14018873291015624,0.140164697265625,0.14014066162109376,0.140116650390625,0.14009261474609375,0.1400686279296875,0.140044677734375,0.1400206787109375,0.13999678955078124,0.1399728271484375,0.13994892578125,0.139925048828125,0.1399011474609375,0.139877294921875,0.1398534423828125,0.13982960205078124,0.13980579833984375,0.13978197021484376,0.13975823974609375,0.1397344482421875,0.1397107177734375,0.1396869873046875,0.1396632568359375,0.13963955078125,0.13961585693359374,0.13959219970703124,0.139568603515625,0.13954490966796876,0.1395212890625,0.139497705078125,0.13947413330078126,0.13945050048828125,0.13942698974609374,0.139403466796875,0.13937991943359376,0.1393564208984375,0.1393328857421875,0.139309423828125,0.13928597412109375,0.13926253662109375,0.139239111328125,0.13921571044921874,0.13919227294921874,0.139168896484375,0.1391455078125,0.13912220458984376,0.139098828125,0.1390755126953125,0.13905224609375,0.13902891845703125,0.1390056396484375,0.13898238525390624,0.1389591552734375,0.13893592529296875,0.1389127197265625,0.138889501953125,0.1388663330078125,0.1388431640625,0.13882000732421876,0.1387968505859375,0.13877373046875,0.13875064697265624,0.1387275146484375,0.138704443359375,0.138681396484375,0.13865836181640626,0.13863529052734375,0.1386123046875,0.1385892822265625,0.1385663330078125,0.1385433349609375,0.1385203857421875,0.1384974609375,0.13847452392578125,0.138451611328125,0.13842872314453125,0.138405810546875,0.13838294677734375,0.1383601318359375,0.138337255859375,0.1383144287109375,0.13829163818359375,0.1382688720703125,0.1382460693359375,0.1382233154296875,0.138200537109375,0.13817783203125,0.138155078125,0.138132373046875,0.1381096923828125,0.13808701171875,0.1380643310546875,0.13804171142578125,0.1380190673828125,0.137996484375,0.13797386474609374,0.1379512451171875,0.1379287109375,0.13790611572265624,0.13788355712890624,0.13786104736328125,0.137838525390625,0.13781605224609375,0.137793505859375,0.1377710693359375,0.1377486328125,0.137726171875,0.1377037109375,0.13768128662109375,0.13765888671875,0.13763648681640625,0.1376141357421875,0.13759176025390624,0.13756943359375,0.1375470703125,0.13752479248046875,0.13750247802734375,0.1374801513671875,0.13745789794921875,0.1374356201171875,0.13741337890625,0.13739112548828125,0.13736888427734376,0.137346728515625,0.13732449951171874,0.13730233154296875,0.137280126953125,0.1372580078125,0.13723585205078126,0.137213720703125,0.13719163818359376,0.13716953125,0.1371474365234375,0.13712535400390624,0.13710328369140626,0.13708128662109376,0.13705924072265624,0.1370371826171875,0.1370152099609375,0.136993212890625,0.1369712890625,0.13694930419921875,0.1369273193359375,0.1369053955078125,0.136883447265625,0.13686156005859376,0.1368396728515625,0.1368177978515625,0.13679593505859375,0.1367740478515625,0.13675224609375,0.1367303955078125,0.13670855712890626,0.1366867919921875,0.136664990234375,0.136643212890625,0.13662144775390625,0.13659970703125,0.13657796630859376,0.13655625,0.136534521484375,0.1365128173828125,0.136491162109375,0.13646947021484376,0.136447802734375,0.13642618408203125,0.136404541015625,0.1363829345703125,0.136361328125,0.13633970947265625,0.13631815185546875,0.1362966064453125,0.1362750244140625,0.136253466796875,0.13623199462890626,0.13621043701171875,0.13618895263671876,0.13616744384765625,0.1361459716796875,0.1361245361328125,0.136103076171875,0.1360816162109375,0.136060205078125,0.13603878173828124,0.13601739501953125,0.13599598388671874,0.13597464599609374,0.135953271484375,0.13593193359375,0.135910595703125,0.13588927001953124,0.135867919921875,0.1358466552734375,0.1358253662109375,0.1358041015625,0.13578282470703126,0.1357615966796875,0.1357403564453125,0.135719140625,0.1356979248046875,0.1356767333984375,0.13565556640625,0.135634375,0.1356132080078125,0.13559205322265624,0.13557093505859374,0.1355498291015625,0.1355286376953125,0.1355075927734375,0.1354864990234375,0.1354654296875,0.1354443603515625,0.13542330322265625,0.13540224609375,0.13538125,0.13536024169921876,0.1353392333984375,0.1353182373046875,0.135297265625,0.1352763427734375,0.1352553466796875,0.1352343994140625,0.13521346435546874,0.13519256591796874,0.1351716552734375,0.1351507568359375,0.13512987060546874,0.1351090087890625,0.13508817138671875,0.135067333984375,0.13504647216796875,0.1350256591796875,0.1350048828125,0.1349840576171875,0.13496329345703126,0.13494248046875,0.1349217529296875,0.13490098876953124,0.134880224609375,0.13485955810546876,0.13483880615234375,0.1348181396484375,0.13479742431640626,0.13477674560546876,0.1347560791015625,0.1347354248046875,0.134714794921875,0.13469415283203126,0.13467354736328124,0.13465294189453125,0.13463236083984376,0.1346117431640625,0.134591162109375,0.13457059326171875,0.1345500732421875,0.134529541015625,0.134508984375,0.13448848876953126,0.13446798095703125,0.134447509765625,0.134427001953125,0.13440654296875,0.1343860595703125,0.134365625,0.13434520263671876,0.1343247802734375,0.13430439453125,0.134283984375,0.1342635986328125,0.13424317626953125,0.1342228271484375,0.13420250244140625,0.13418212890625,0.13416180419921875,0.13414150390625,0.13412119140625,0.13410087890625,0.13408057861328124,0.1340603271484375,0.13404005126953125,0.13401978759765626,0.133999560546875,0.133979345703125,0.13395911865234375,0.13393887939453125,0.1339186767578125,0.13389854736328125,0.13387838134765626,0.133858154296875,0.13383804931640625,0.1338178955078125,0.13379776611328126,0.13377764892578126,0.13375751953125,0.133737451171875,0.13371734619140624,0.1336972412109375,0.133677197265625,0.1336571533203125,0.1336371337890625,0.13361707763671876,0.13359708251953126,0.13357705078125,0.1335570556640625,0.1335370849609375,0.1335170654296875,0.133497119140625,0.1334771484375,0.13345723876953125,0.1334372802734375,0.1334173583984375,0.13339744873046874,0.13337757568359376,0.133357666015625,0.13333782958984375,0.13331790771484375,0.13329805908203124,0.1332781982421875,0.1332583740234375,0.13323851318359375,0.13321871337890626,0.13319892578125,0.1331791259765625,0.133159326171875,0.1331395751953125,0.1331197998046875,0.13310003662109374,0.13308028564453125,0.13306058349609376,0.133040869140625,0.1330211669921875,0.13300146484375,0.132981787109375,0.132962109375,0.1329424072265625,0.1329227783203125,0.1329031005859375,0.13288349609375,0.1328638916015625,0.13284423828125,0.132824609375,0.132805029296875,0.13278548583984376,0.1327658935546875,0.13274630126953124,0.13272677001953126,0.1327072265625,0.1326876953125,0.132668212890625,0.1326487060546875,0.13262919921875,0.13260966796875,0.1325902099609375,0.132570751953125,0.13255128173828126,0.1325318359375,0.13251240234375,0.13249295654296875,0.13247352294921874,0.1324541259765625,0.132434716796875,0.1324153564453125,0.13239595947265625,0.132376611328125,0.13235726318359375,0.132337890625,0.13231856689453125,0.1322991943359375,0.132279931640625,0.13226060791015626,0.1322412841796875,0.1322220458984375,0.1322027099609375,0.132183447265625,0.132164208984375,0.1321449462890625,0.132125732421875,0.13210648193359376,0.13208726806640625,0.13206806640625,0.132048876953125,0.13202967529296875,0.13201051025390625,0.13199134521484376,0.1319721923828125,0.1319530517578125,0.1319339111328125,0.1319147705078125,0.13189564208984375,0.13187655029296874,0.1318574951171875,0.13183834228515626,0.13181925048828125,0.13180020751953125,0.13178115234375,0.13176209716796874,0.13174307861328124,0.1317240234375,0.1317050048828125,0.13168597412109376,0.1316669921875,0.13164803466796876,0.13162900390625,0.13161005859375,0.1315910888671875,0.1315721435546875,0.131553173828125,0.1315342529296875,0.1315153076171875,0.13149642333984374,0.131477490234375,0.131458642578125,0.1314397216796875,0.131420849609375,0.1314019775390625,0.131383154296875,0.131364306640625,0.1313454345703125,0.13132662353515626,0.1313078125,0.1312889892578125,0.13127021484375,0.13125140380859374,0.1312326416015625,0.1312138671875,0.1311950927734375,0.131176318359375,0.131157568359375,0.1311388427734375,0.13112012939453124,0.13110142822265625,0.13108271484375,0.131064013671875,0.1310453369140625,0.1310266845703125,0.13100797119140625,0.1309893310546875,0.1309706787109375,0.1309520263671875,0.1309334228515625,0.130914794921875,0.13089619140625,0.1308775634765625,0.1308589599609375,0.1308404296875,0.130821826171875,0.13080323486328124,0.1307846923828125,0.13076612548828126,0.1307475830078125,0.13072908935546876,0.1307105712890625,0.13069202880859376,0.13067353515625,0.13065504150390625,0.13063656005859375,0.13061806640625,0.130599609375,0.13058114013671876,0.1305627197265625,0.130544287109375,0.1305258544921875,0.1305074462890625,0.130489013671875,0.13047064208984374,0.1304522216796875,0.1304337890625,0.1304154541015625,0.13039710693359374,0.1303787353515625,0.130360400390625,0.1303420654296875,0.130323681640625,0.13030540771484375,0.1302870849609375,0.13026878662109376,0.1302504638671875,0.1302322021484375,0.1302139404296875,0.13019566650390624,0.13017740478515624,0.1301591552734375,0.13014090576171874,0.13012266845703124,0.130104443359375,0.13008624267578126,0.130068017578125,0.1300498291015625,0.13003162841796875,0.1300134765625,0.1299953125,0.1299771240234375,0.1299589599609375,0.1299408203125,0.12992265625,0.1299045654296875,0.12988643798828126,0.129868359375,0.1298502197265625,0.12983212890625,0.12981405029296875,0.1297959716796875,0.1297779052734375,0.1297598388671875,0.129741796875,0.12972376708984376,0.1297057373046875,0.1296877197265625,0.1296696533203125,0.129651708984375,0.1296337158203125,0.1296156982421875,0.1295977294921875,0.12957974853515625,0.12956177978515626,0.12954384765625,0.1295258544921875,0.129507958984375,0.1294900146484375,0.1294720947265625,0.12945416259765624,0.12943629150390626,0.129418359375,0.12940048828125,0.1293826171875,0.12936474609375,0.1293468994140625,0.1293290283203125,0.12931119384765624,0.1292933349609375,0.129275537109375,0.1292576904296875,0.1292399169921875,0.129222119140625,0.129204296875,0.12918648681640624,0.1291687255859375,0.12915098876953124,0.129133203125,0.1291154296875,0.12909771728515626,0.12907996826171875,0.129062255859375,0.12904451904296876,0.12902679443359374,0.1290091064453125,0.128991455078125,0.12897373046875,0.1289560546875,0.12893837890625,0.12892071533203125,0.128903076171875,0.128885400390625,0.1288677978515625,0.12885018310546875,0.12883251953125,0.1288149169921875,0.1287972900390625,0.12877972412109376,0.12876212158203126,0.1287445556640625,0.1287269775390625,0.12870941162109376,0.12869188232421874,0.12867431640625,0.12865677490234376,0.1286392578125,0.1286217041015625,0.12860419921875,0.12858668212890625,0.128569189453125,0.12855172119140626,0.12853421630859374,0.128516748046875,0.128499267578125,0.1284818359375,0.12846434326171874,0.1284468994140625,0.1284294921875,0.128412060546875,0.12839466552734374,0.12837724609375,0.12835980224609375,0.12834239501953126,0.128325048828125,0.128307666015625,0.128290283203125,0.12827294921875,0.1282555908203125,0.128238232421875,0.1282209228515625,0.12820355224609375,0.12818623046875,0.12816890869140626,0.128151611328125,0.12813433837890625,0.1281170166015625,0.12809970703125,0.128082470703125,0.128065185546875,0.12804793701171874,0.1280306884765625,0.128013427734375,0.12799619140625,0.1279789306640625,0.12796173095703126,0.1279445556640625,0.1279273193359375,0.12791015625,0.1278929443359375,0.12787574462890625,0.127858544921875,0.1278413818359375,0.1278242431640625,0.12780709228515624,0.1277899169921875,0.1277727783203125,0.1277556396484375,0.1277385498046875,0.127721435546875,0.1277043212890625,0.12768720703125,0.12767012939453126,0.12765306396484374,0.1276359619140625,0.127618896484375,0.12760185546875,0.1275847900390625,0.1275677490234375,0.1275507080078125,0.1275336669921875,0.12751666259765626,0.1274996337890625,0.1274826416015625,0.1274656494140625,0.1274486328125,0.1274316650390625,0.127414697265625,0.1273976806640625,0.12738076171875,0.12736376953125,0.1273468505859375,0.1273299072265625,0.12731295166015624,0.1272960693359375,0.1272791015625,0.12726221923828124,0.1272452880859375,0.1272284423828125,0.1272115234375,0.12719461669921875,0.12717774658203124,0.1271609130859375,0.12714405517578126,0.1271271728515625,0.12711036376953125,0.1270935302734375,0.1270766845703125,0.12705987548828124,0.1270430419921875,0.12702623291015624,0.1270094482421875,0.12699267578125,0.126975830078125,0.1269591064453125,0.126942333984375,0.12692552490234374,0.1269087890625,0.12689205322265626,0.12687530517578124,0.12685859375,0.12684183349609374,0.1268251220703125,0.1268084228515625,0.12679168701171875,0.126775048828125,0.126758349609375,0.126741650390625,0.126724951171875,0.12670833740234375,0.126691650390625,0.12667498779296876,0.12665831298828126,0.1266416748046875,0.1266250732421875,0.1266084228515625,0.126591845703125,0.1265752197265625,0.12655859375,0.12654200439453125,0.126525390625,0.1265087890625,0.126492236328125,0.12647572021484374,0.1264591552734375,0.126442578125,0.1264260498046875,0.12640947265625,0.1263929443359375,0.1263764404296875,0.126359912109375,0.1263434326171875,0.1263268798828125,0.1263103759765625,0.1262939208984375,0.12627742919921875,0.1262609619140625,0.1262444580078125,0.12622803955078124,0.1262115478515625,0.1261951171875,0.12617867431640625,0.1261622314453125,0.12614578857421874,0.12612938232421875,0.1261129638671875,0.1260965576171875,0.1260802001953125,0.12606378173828126,0.1260473876953125,0.1260310302734375,0.1260146484375,0.1259982666015625,0.1259819091796875,0.125965576171875,0.12594921875,0.12593291015625,0.12591656494140624,0.12590023193359376,0.12588394775390624,0.1258676025390625,0.125851318359375,0.1258349853515625,0.12581873779296876,0.12580244140625,0.1257861572265625,0.125769873046875,0.1257536376953125,0.12573740234375,0.125721142578125,0.1257049072265625,0.1256886474609375,0.1256724365234375,0.125656201171875,0.125639990234375,0.1256238037109375,0.1256075927734375,0.12559140625,0.1255752197265625,0.125559033203125,0.12554285888671876,0.12552666015625,0.1255105224609375,0.1254943603515625,0.12547821044921875,0.12546207275390625,0.1254459228515625,0.12542984619140626,0.12541370849609376,0.125397607421875,0.1253815185546875,0.125365380859375,0.1253492919921875,0.1253332275390625,0.1253171142578125,0.1253010498046875,0.125285009765625,0.12526890869140625,0.1252529052734375,0.12523682861328125,0.12522078857421876,0.12520477294921875,0.1251887451171875,0.125172705078125,0.125156689453125,0.125140673828125,0.1251246337890625,0.12510870361328125,0.125092724609375,0.1250766845703125,0.1250607421875,0.125044775390625,0.1250287841796875,0.1250128662109375,0.124996923828125,0.12498096923828125,0.1249650390625,0.12494910888671874,0.12493321533203125,0.12491728515625,0.1249013671875,0.12488546142578125,0.1248695556640625,0.12485367431640625,0.12483779296875,0.124821923828125,0.12480606689453125,0.124790185546875,0.12477435302734376,0.12475849609375,0.124742626953125,0.1247267822265625,0.1247109619140625,0.1246951416015625,0.12467933349609375,0.1246635009765625,0.12464771728515625,0.12463192138671875,0.1246160888671875,0.124600341796875,0.124584521484375,0.12456878662109375,0.1245530029296875,0.1245372314453125,0.124521484375,0.12450576171875,0.1244899658203125,0.124474267578125,0.1244585205078125,0.12444281005859376,0.12442711181640626,0.124411376953125,0.1243956787109375,0.12437999267578125,0.12436429443359374,0.12434862060546875,0.12433291015625,0.12431728515625,0.12430159912109374,0.1242859375,0.12427027587890625,0.1242546142578125,0.1242389892578125,0.1242233642578125,0.12420772705078124,0.1241920654296875,0.1241764892578125,0.1241608642578125,0.12414527587890625,0.1241296875,0.12411407470703124,0.1240985107421875,0.12408292236328125,0.124067333984375,0.12405179443359375,0.12403623046875,0.12402066650390625,0.1240051025390625,0.12398958740234375,0.1239740478515625,0.123958544921875,0.12394295654296875,0.123927490234375,0.12391197509765625,0.123896484375,0.1238809326171875,0.12386549072265625,0.12385,0.1238345458984375,0.1238190673828125,0.12380357666015625,0.1237880859375,0.12377265625,0.12375718994140625,0.12374173583984376,0.12372633056640625,0.123710888671875,0.12369542236328125,0.12368001708984375,0.123664599609375,0.12364921875,0.1236337890625,0.123618408203125,0.12360301513671874,0.123587646484375,0.12357227783203124,0.12355689697265625,0.12354150390625,0.123526171875,0.1235108154296875,0.1234954833984375,0.123480126953125,0.12346480712890626,0.1234494384765625,0.12343414306640625,0.12341881103515626,0.12340350341796875,0.12338818359375,0.12337288818359375,0.1233576171875,0.1233423095703125,0.12332703857421876,0.1233117431640625,0.123296484375,0.12328123779296875]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss by time\"};\n",
       "\n",
       "  Plotly.plot('plot-1291879632', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mlossSeq\u001b[39m: \u001b[32mIndexedSeq\u001b[39m[\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m.\u001b[32m<refinement>\u001b[39m.this.type.\u001b[32mOutputData\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m0.2303436767578125\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mplot\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mScatter\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mScatter\u001b[39m(\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mres13_2\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-1291879632\"\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lossSeq = for (i <- 0 until 2000) yield {\n",
    "  val loss = lossFunction.train(trainData :: vectorizedTrainExpectResult :: HNil)\n",
    "  if(i % 100 == 0){\n",
    "    println(\"loss is :\" + loss)\n",
    "  }\n",
    "  loss\n",
    "}\n",
    "\n",
    "val plot = Seq(\n",
    "  Scatter(lossSeq.indices, lossSeq)\n",
    ")\n",
    "\n",
    "plot.plot(\n",
    "  title = \"loss by time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用已经处理好的测试数据来验证神经网络的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: [[0.03, 0.05, 0.17, 0.13, 0.01, 0.13, 0.43, 0.00, 0.04, 0.00],\n",
      " [0.03, 0.17, 0.00, 0.05, 0.00, 0.01, 0.00, 0.00, 0.18, 0.55],\n",
      " [0.08, 0.09, 0.01, 0.03, 0.01, 0.00, 0.00, 0.01, 0.71, 0.05],\n",
      " [0.50, 0.04, 0.07, 0.04, 0.01, 0.02, 0.00, 0.03, 0.21, 0.08],\n",
      " [0.02, 0.03, 0.09, 0.05, 0.36, 0.10, 0.25, 0.08, 0.01, 0.00],\n",
      " [0.01, 0.07, 0.02, 0.63, 0.02, 0.04, 0.13, 0.05, 0.00, 0.03],\n",
      " [0.00, 0.00, 0.00, 0.68, 0.02, 0.05, 0.24, 0.00, 0.00, 0.00],\n",
      " [0.04, 0.01, 0.24, 0.04, 0.26, 0.10, 0.17, 0.11, 0.01, 0.01],\n",
      " [0.05, 0.17, 0.22, 0.09, 0.17, 0.16, 0.09, 0.03, 0.01, 0.01],\n",
      " [0.08, 0.44, 0.02, 0.03, 0.00, 0.06, 0.01, 0.01, 0.11, 0.24],\n",
      " [0.33, 0.09, 0.04, 0.09, 0.02, 0.07, 0.04, 0.02, 0.28, 0.03],\n",
      " [0.02, 0.49, 0.02, 0.05, 0.01, 0.01, 0.01, 0.03, 0.04, 0.33],\n",
      " [0.02, 0.31, 0.02, 0.24, 0.12, 0.07, 0.15, 0.07, 0.01, 0.01],\n",
      " [0.04, 0.21, 0.03, 0.07, 0.05, 0.01, 0.15, 0.01, 0.29, 0.15],\n",
      " [0.24, 0.18, 0.05, 0.17, 0.02, 0.04, 0.03, 0.13, 0.08, 0.06],\n",
      " [0.19, 0.03, 0.19, 0.02, 0.09, 0.06, 0.03, 0.03, 0.31, 0.04],\n",
      " [0.12, 0.04, 0.00, 0.07, 0.01, 0.21, 0.44, 0.06, 0.04, 0.01],\n",
      " [0.19, 0.03, 0.15, 0.21, 0.06, 0.02, 0.11, 0.19, 0.02, 0.02],\n",
      " [0.05, 0.12, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.46, 0.34],\n",
      " [0.01, 0.11, 0.06, 0.22, 0.06, 0.03, 0.35, 0.16, 0.00, 0.01],\n",
      " [0.18, 0.07, 0.08, 0.03, 0.19, 0.11, 0.06, 0.10, 0.13, 0.05],\n",
      " [0.18, 0.00, 0.47, 0.01, 0.21, 0.09, 0.00, 0.03, 0.01, 0.00],\n",
      " [0.48, 0.03, 0.04, 0.02, 0.01, 0.01, 0.00, 0.03, 0.35, 0.03],\n",
      " [0.01, 0.35, 0.02, 0.02, 0.06, 0.15, 0.06, 0.06, 0.01, 0.25],\n",
      " [0.02, 0.01, 0.23, 0.01, 0.41, 0.03, 0.01, 0.26, 0.00, 0.02],\n",
      " [0.05, 0.08, 0.23, 0.01, 0.06, 0.04, 0.30, 0.05, 0.12, 0.07],\n",
      " [0.10, 0.06, 0.07, 0.05, 0.14, 0.06, 0.40, 0.05, 0.04, 0.03],\n",
      " [0.31, 0.07, 0.14, 0.04, 0.09, 0.01, 0.02, 0.14, 0.09, 0.10],\n",
      " [0.10, 0.24, 0.22, 0.04, 0.15, 0.02, 0.06, 0.05, 0.02, 0.09],\n",
      " [0.00, 0.03, 0.09, 0.06, 0.18, 0.09, 0.47, 0.06, 0.00, 0.02],\n",
      " [0.02, 0.23, 0.13, 0.11, 0.10, 0.16, 0.16, 0.05, 0.01, 0.04],\n",
      " [0.03, 0.01, 0.17, 0.11, 0.12, 0.39, 0.09, 0.06, 0.01, 0.01],\n",
      " [0.08, 0.03, 0.10, 0.25, 0.10, 0.17, 0.16, 0.07, 0.04, 0.01],\n",
      " [0.07, 0.08, 0.13, 0.10, 0.03, 0.15, 0.21, 0.14, 0.02, 0.07],\n",
      " [0.06, 0.03, 0.00, 0.01, 0.01, 0.00, 0.01, 0.02, 0.69, 0.17],\n",
      " [0.03, 0.13, 0.11, 0.06, 0.11, 0.12, 0.17, 0.19, 0.03, 0.04],\n",
      " [0.02, 0.07, 0.11, 0.09, 0.39, 0.08, 0.14, 0.05, 0.02, 0.04],\n",
      " [0.02, 0.12, 0.00, 0.06, 0.00, 0.01, 0.02, 0.01, 0.05, 0.71],\n",
      " [0.05, 0.27, 0.01, 0.13, 0.02, 0.09, 0.03, 0.02, 0.06, 0.33],\n",
      " [0.19, 0.01, 0.10, 0.13, 0.05, 0.26, 0.05, 0.18, 0.03, 0.00],\n",
      " [0.69, 0.00, 0.03, 0.03, 0.04, 0.01, 0.02, 0.12, 0.03, 0.04],\n",
      " [0.05, 0.01, 0.16, 0.03, 0.30, 0.04, 0.22, 0.15, 0.03, 0.01],\n",
      " [0.07, 0.04, 0.01, 0.43, 0.05, 0.11, 0.02, 0.19, 0.01, 0.05],\n",
      " [0.03, 0.04, 0.19, 0.07, 0.19, 0.04, 0.32, 0.08, 0.02, 0.01],\n",
      " [0.55, 0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.08, 0.28, 0.08],\n",
      " [0.06, 0.16, 0.00, 0.10, 0.02, 0.01, 0.01, 0.14, 0.11, 0.39],\n",
      " [0.00, 0.09, 0.04, 0.17, 0.13, 0.13, 0.41, 0.01, 0.01, 0.01],\n",
      " [0.17, 0.04, 0.00, 0.24, 0.02, 0.06, 0.02, 0.12, 0.28, 0.04],\n",
      " [0.01, 0.01, 0.37, 0.02, 0.22, 0.04, 0.22, 0.09, 0.00, 0.01],\n",
      " [0.01, 0.02, 0.17, 0.02, 0.36, 0.03, 0.32, 0.06, 0.00, 0.01],\n",
      " [0.14, 0.04, 0.01, 0.00, 0.00, 0.00, 0.00, 0.01, 0.40, 0.39],\n",
      " [0.14, 0.08, 0.03, 0.01, 0.08, 0.01, 0.01, 0.29, 0.15, 0.20],\n",
      " [0.01, 0.02, 0.02, 0.26, 0.05, 0.07, 0.48, 0.04, 0.03, 0.03],\n",
      " [0.02, 0.05, 0.02, 0.23, 0.05, 0.06, 0.31, 0.04, 0.08, 0.14],\n",
      " [0.02, 0.03, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.93, 0.01],\n",
      " [0.13, 0.17, 0.05, 0.06, 0.03, 0.02, 0.03, 0.03, 0.32, 0.17],\n",
      " [0.12, 0.00, 0.07, 0.06, 0.20, 0.04, 0.08, 0.39, 0.04, 0.00],\n",
      " [0.05, 0.06, 0.14, 0.26, 0.02, 0.05, 0.11, 0.02, 0.23, 0.05],\n",
      " [0.06, 0.22, 0.08, 0.16, 0.09, 0.04, 0.11, 0.06, 0.07, 0.11],\n",
      " [0.08, 0.01, 0.42, 0.04, 0.22, 0.04, 0.02, 0.11, 0.04, 0.02],\n",
      " [0.01, 0.00, 0.16, 0.01, 0.34, 0.14, 0.22, 0.11, 0.00, 0.00],\n",
      " [0.00, 0.01, 0.09, 0.12, 0.20, 0.35, 0.21, 0.02, 0.00, 0.00],\n",
      " [0.02, 0.15, 0.01, 0.14, 0.09, 0.02, 0.38, 0.16, 0.02, 0.02],\n",
      " [0.03, 0.18, 0.37, 0.01, 0.13, 0.02, 0.02, 0.02, 0.07, 0.16],\n",
      " [0.03, 0.16, 0.09, 0.26, 0.06, 0.03, 0.22, 0.09, 0.01, 0.06],\n",
      " [0.05, 0.00, 0.20, 0.06, 0.18, 0.13, 0.22, 0.13, 0.01, 0.01],\n",
      " [0.04, 0.66, 0.03, 0.01, 0.02, 0.00, 0.02, 0.04, 0.01, 0.17],\n",
      " [0.15, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.01, 0.82, 0.00],\n",
      " [0.03, 0.02, 0.07, 0.37, 0.05, 0.06, 0.12, 0.06, 0.09, 0.12],\n",
      " [0.05, 0.12, 0.01, 0.02, 0.01, 0.00, 0.02, 0.08, 0.17, 0.52],\n",
      " [0.09, 0.05, 0.25, 0.12, 0.06, 0.05, 0.10, 0.22, 0.01, 0.05],\n",
      " [0.04, 0.05, 0.10, 0.16, 0.17, 0.12, 0.13, 0.22, 0.01, 0.01],\n",
      " [0.14, 0.18, 0.05, 0.16, 0.07, 0.05, 0.05, 0.05, 0.12, 0.14],\n",
      " [0.27, 0.01, 0.02, 0.02, 0.02, 0.02, 0.00, 0.02, 0.60, 0.02],\n",
      " [0.17, 0.02, 0.05, 0.03, 0.01, 0.01, 0.02, 0.07, 0.05, 0.55],\n",
      " [0.01, 0.01, 0.28, 0.01, 0.28, 0.01, 0.30, 0.08, 0.01, 0.01],\n",
      " [0.09, 0.24, 0.04, 0.11, 0.04, 0.06, 0.03, 0.11, 0.08, 0.20],\n",
      " [0.04, 0.13, 0.02, 0.28, 0.06, 0.22, 0.04, 0.04, 0.14, 0.04],\n",
      " [0.01, 0.15, 0.04, 0.34, 0.12, 0.09, 0.17, 0.01, 0.02, 0.04],\n",
      " [0.06, 0.37, 0.03, 0.06, 0.01, 0.01, 0.04, 0.03, 0.02, 0.37],\n",
      " [0.19, 0.05, 0.09, 0.07, 0.03, 0.03, 0.03, 0.02, 0.39, 0.10],\n",
      " [0.01, 0.24, 0.01, 0.39, 0.07, 0.08, 0.02, 0.03, 0.11, 0.04],\n",
      " [0.01, 0.04, 0.45, 0.09, 0.12, 0.05, 0.22, 0.01, 0.01, 0.00],\n",
      " [0.14, 0.02, 0.29, 0.08, 0.07, 0.05, 0.04, 0.05, 0.19, 0.07],\n",
      " [0.37, 0.11, 0.15, 0.15, 0.03, 0.01, 0.01, 0.04, 0.11, 0.03],\n",
      " [0.43, 0.00, 0.02, 0.00, 0.08, 0.00, 0.00, 0.25, 0.15, 0.07],\n",
      " [0.36, 0.02, 0.25, 0.04, 0.02, 0.15, 0.01, 0.07, 0.08, 0.01],\n",
      " [0.04, 0.13, 0.06, 0.06, 0.05, 0.04, 0.01, 0.03, 0.32, 0.27],\n",
      " [0.20, 0.02, 0.14, 0.26, 0.01, 0.06, 0.03, 0.03, 0.21, 0.04],\n",
      " [0.06, 0.16, 0.02, 0.01, 0.02, 0.02, 0.01, 0.01, 0.15, 0.54],\n",
      " [0.13, 0.16, 0.01, 0.02, 0.09, 0.01, 0.00, 0.04, 0.33, 0.20],\n",
      " [0.01, 0.02, 0.38, 0.04, 0.18, 0.09, 0.19, 0.08, 0.00, 0.01],\n",
      " [0.09, 0.15, 0.01, 0.04, 0.01, 0.01, 0.00, 0.01, 0.63, 0.06],\n",
      " [0.15, 0.01, 0.29, 0.03, 0.10, 0.03, 0.10, 0.07, 0.17, 0.05],\n",
      " [0.12, 0.05, 0.11, 0.04, 0.20, 0.05, 0.11, 0.17, 0.10, 0.06],\n",
      " [0.07, 0.02, 0.13, 0.20, 0.07, 0.15, 0.19, 0.07, 0.03, 0.09],\n",
      " [0.01, 0.04, 0.07, 0.12, 0.10, 0.21, 0.39, 0.06, 0.00, 0.01],\n",
      " [0.31, 0.02, 0.07, 0.16, 0.03, 0.11, 0.02, 0.05, 0.20, 0.02],\n",
      " [0.11, 0.00, 0.55, 0.01, 0.02, 0.09, 0.00, 0.21, 0.00, 0.00],\n",
      " [0.03, 0.02, 0.03, 0.34, 0.03, 0.01, 0.03, 0.26, 0.01, 0.25]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mresult\u001b[39m: \u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m.\u001b[32m<refinement>\u001b[39m.this.type.\u001b[32mOutputData\u001b[39m = [[0.03, 0.05, 0.17, 0.13, 0.01, 0.13, 0.43, 0.00, 0.04, 0.00],\n",
       " [0.03, 0.17, 0.00, 0.05, 0.00, 0.01, 0.00, 0.00, 0.18, 0.55],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = myNeuralNetwork.predict(testData)\n",
    "println(s\"result: $result\") //输出判断结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算并神经网络对测试数据分类判断的正确率，正确率应该在32%左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result is 32.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mright\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m32.0\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val right = Utils.getAccuracy(result, testExpectResult)\n",
    "println(s\"the result is $right %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[完整代码](https://github.com/izhangzhihao/deeplearning-tutorial/blob/master/src/main/scala/com/thoughtworks/deeplearning/tutorial/SoftmaxLinearClassifier.scala)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
