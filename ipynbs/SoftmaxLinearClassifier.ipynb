{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "In this article, we will use [softmax](https://en.wikipedia.org/wiki/Softmax_function) classifier to build a simple image classification neural network with an accuracy of 32%. In a Softmax classifier, binary logic is generalized and regressed to multiple logic. Softmax classifier will output the probability of the corresponding category.\n",
    "\n",
    "We will first define a softmax classifier, then use the training set of [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) to train the neural network, and finally use the test set to verify the accuracy of the neural network.\n",
    "\n",
    "Letâ€™s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the previous course [GettingStarted](https://thoughtworksinc.github.io/DeepLearning.scala/demo/GettingStarted.html), we need to introduce each class of DeepLearning.scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                            \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                            \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                 \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.{FileInputStream, InputStream}\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableHList._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableDouble._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableAny._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.{\n",
       "  DifferentiableHList,\n",
       "  DifferentiableINDArray,\n",
       "  Layer,\n",
       "  Symbolic\n",
       "}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Layer.Tape\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathFunctions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathMethods./\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathOps\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.ops.transforms.Transforms\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4s.Implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mshapeless._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.immutable.IndexedSeq\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $plugin.$ivy.`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:2.0.0`\n",
    "\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableany:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablenothing:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableseq:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiabledouble:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablefloat:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablehlist:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableindarray:1.0.0`\n",
    "import $ivy.`org.nd4j:nd4j-native-platform:0.7.2`\n",
    "import $ivy.`org.rauschig:jarchivelib:0.5.0`\n",
    "\n",
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.0`\n",
    "\n",
    "import java.io.{FileInputStream, InputStream}\n",
    "\n",
    "\n",
    "import com.thoughtworks.deeplearning\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import com.thoughtworks.deeplearning.DifferentiableHList._\n",
    "import com.thoughtworks.deeplearning.DifferentiableDouble._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray._\n",
    "import com.thoughtworks.deeplearning.DifferentiableAny._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
    "import com.thoughtworks.deeplearning.{\n",
    "  DifferentiableHList,\n",
    "  DifferentiableINDArray,\n",
    "  Layer,\n",
    "  Symbolic\n",
    "}\n",
    "import com.thoughtworks.deeplearning.Layer.Tape\n",
    "import com.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
    "import com.thoughtworks.deeplearning.Symbolic._\n",
    "import com.thoughtworks.deeplearning.Poly.MathFunctions._\n",
    "import com.thoughtworks.deeplearning.Poly.MathMethods./\n",
    "import com.thoughtworks.deeplearning.Poly.MathOps\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
    "import org.nd4j.linalg.ops.transforms.Transforms\n",
    "import org.nd4s.Implicits._\n",
    "import shapeless._\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._\n",
    "\n",
    "import scala.collection.immutable.IndexedSeq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the line numbers outputted by `jupyter-scala` and to make sure that the page output will not be too long, we need to set `pprintConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pprintConfig() = pprintConfig().copy(height = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `softmax` classifier (softmax classifier is a neural network combined by `softmax` and a full connection), we first need to write softmax function, formula: ![](https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msoftmax\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(implicit scores: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val expScores = exp(scores)\n",
    "  expScores / expScores.sum(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate need to be set for the full connection layer. Learning rate visually describes the change rate of `weight`. A too-low learning rate will result in slow decrease of `loss`, which will require longer time for training; A too-high learning rate will result in rapid decrease of `loss` at first while fluctuation around the lowest point afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36moptimizer\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit def optimizer: Optimizer = new LearningRate {\n",
    "  def currentLearningRate() = 0.00001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a full connection layer and [initialize Weight](https://github.com/ThoughtWorksInc/DeepLearning.scala/wiki/Getting-Started#231--weight-intialization), `Weight` shall be a two-dimension `INDArray` of `NumberOfPixels Ã— NumberOfClasses`. `scores` is the score of each image corresponding to each category, representing the feasible probability of each category corresponding to each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mNumberOfClasses\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36mNumberOfPixels\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m3072\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcreateMyNeuralNetwork\u001b[39m\n",
       "\u001b[36mmyNeuralNetwork\u001b[39m: (\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m[\u001b[32mINDArray\u001b[39m]{type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray;type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray})#\u001b[32m@\u001b[39m = Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum(Exp(Identity()),WrappedArray(1)))),Dot(Identity(),Weight([[0.00, -0.00, -0.00, -0.00, -0.00, -0.00, 0.00\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//10 label of CIFAR10 images(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
    "val NumberOfClasses: Int = 10\n",
    "val NumberOfPixels: Int = 3072\n",
    "\n",
    "def createMyNeuralNetwork(implicit input: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val initialValueOfWeight = Nd4j.randn(NumberOfPixels, NumberOfClasses) * 0.001\n",
    "  val weight: INDArray @Symbolic = initialValueOfWeight.toWeight\n",
    "  val scores: INDArray @Symbolic = input dot weight\n",
    "  softmax.compose(scores)\n",
    "}\n",
    "val myNeuralNetwork = createMyNeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine LossFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn about the prediction result of the neural network, we need to write the loss function `lossFunction`. We use [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy) to make comparison between this result and the actual result before return the score. Formula:\n",
    "![](https://zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlossFunction\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lossFunction(implicit pair: (INDArray :: INDArray :: HNil) @Symbolic): Double @Symbolic = {\n",
    "  val input = pair.head\n",
    "  val expectedOutput = pair.tail.head\n",
    "  val probabilities = myNeuralNetwork.compose(input)\n",
    "\n",
    "  -(expectedOutput * log(probabilities)).mean\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To read the images and corresponding label information for test data from CIFAR10 database and process them, we need [`import $file.ReadCIFAR10ToNDArray`](https://github.com/ThoughtWorksInc/DeepLearning.scala-website/blob/master/ipynbs/ReadCIFAR10ToNDArray.sc). This is a script file containing the read and processed CIFAR10 data, provided in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling ReadCIFAR10ToNDArray.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                   \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mtrainNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.23, 0.17, 0.20, 0.27, 0.38, 0.46, 0.54, 0.57, 0.58, 0.58, 0.51, 0.49, 0.55, 0.56, 0.54, 0.50, 0.54, 0.52, 0.48, 0.54, 0.54, 0.52, 0.53, 0.54, 0.59, 0.64, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.ReadCIFAR10ToNDArray\n",
    "\n",
    "val trainNDArray =\n",
    "  ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/data_batch_1.bin\", 1000)\n",
    "\n",
    "val testNDArray =\n",
    "  ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/test_batch.bin\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before passing data to the softmax classifier, we first process label data with ([one hot encoding](https://en.wikipedia.org/wiki/One-hot)): transform INDArray of `NumberOfPixels Ã— 1` into INDArray of `NumberOfPixels Ã— NumberOfClasses`. The value of correct classification corresponding to each line is 1, and the values of other columns are 0. The reason for differentiating the training set and test set is to make it clear that whether the network is over trained which leads to [overfitting](https://en.wikipedia.org/wiki/Overfitting). While processing label data, we used [Utils](https://github.com/ThoughtWorksInc/DeepLearning.scala-website/blob/master/ipynbs/Utils.sc), which is also provided in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Utils.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.23, 0.17, 0.20, 0.27, 0.38, 0.46, 0.54, 0.57, 0.58, 0.58, 0.51, 0.49, 0.55, 0.56, 0.54, 0.50, 0.54, 0.52, 0.48, 0.54, 0.54, 0.52, 0.53, 0.54, 0.59, 0.64, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtrainExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [6.00, 9.00, 9.00, 4.00, 1.00, 1.00, 2.00, 7.00, 8.00, 3.00, 4.00, 7.00, 7.00, 2.00, 9.00, 9.00, 9.00, 3.00, 2.00, 6.00, 4.00, 3.00, 6.00, 6.00, 2.00, 6.00, 3.0\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.0\u001b[33m...\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$    \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mvectorizedTrainExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00],\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mvectorizedTestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainData = trainNDArray.head\n",
    "val testData = testNDArray.head\n",
    "\n",
    "val trainExpectResult = trainNDArray.tail.head\n",
    "val testExpectResult = testNDArray.tail.head\n",
    "\n",
    "import $file.Utils\n",
    "\n",
    "val vectorizedTrainExpectResult = Utils.makeVectorized(trainExpectResult, NumberOfClasses)\n",
    "val vectorizedTestExpectResult = Utils.makeVectorized(testExpectResult, NumberOfClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe the training process of the neural network, we need to output `loss`; while training the neural network, the `loss` shall be deceasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at iteration 0 loss is 0.230375927734375\n",
      "at iteration 100 loss is 0.1908932861328125\n",
      "at iteration 200 loss is 0.17819312744140625\n",
      "at iteration 300 loss is 0.17023385009765624\n",
      "at iteration 400 loss is 0.164277880859375\n",
      "at iteration 500 loss is 0.159437890625\n",
      "at iteration 600 loss is 0.15531514892578124\n",
      "at iteration 700 loss is 0.15169727783203124\n",
      "at iteration 800 loss is 0.148457763671875\n",
      "at iteration 900 loss is 0.1455148681640625\n",
      "at iteration 1000 loss is 0.1428119140625\n",
      "at iteration 1100 loss is 0.14030811767578125\n",
      "at iteration 1200 loss is 0.13797265625\n",
      "at iteration 1300 loss is 0.135781591796875\n",
      "at iteration 1400 loss is 0.133716064453125\n",
      "at iteration 1500 loss is 0.13176082763671876\n",
      "at iteration 1600 loss is 0.12990330810546874\n",
      "at iteration 1700 loss is 0.12813306884765624\n",
      "at iteration 1800 loss is 0.12644134521484374\n",
      "at iteration 1900 loss is 0.12482071533203125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-1486598411\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0,1989.0,1990.0,1991.0,1992.0,1993.0,1994.0,1995.0,1996.0,1997.0,1998.0,1999.0],\"y\":[0.23026494140625,0.2285526123046875,0.22758212890625,0.2266884033203125,0.22582578125,0.224989013671875,0.2241767578125,0.223387744140625,0.22262099609375,0.221875634765625,0.221150830078125,0.2204455810546875,0.2197592529296875,0.2190909423828125,0.2184399658203125,0.2178056640625,0.2171872802734375,0.21658427734375,0.21599599609375,0.215421875,0.2148613525390625,0.2143138671875,0.213778955078125,0.213256201171875,0.212745068359375,0.2122451416015625,0.211756005859375,0.211277294921875,0.21080859375,0.2103496337890625,0.209899951171875,0.2094593017578125,0.209027392578125,0.2086038330078125,0.2081884033203125,0.207780810546875,0.207380908203125,0.20698818359375,0.2066026611328125,0.2062240234375,0.20585205078125,0.205486474609375,0.205127197265625,0.2047739501953125,0.20442659912109376,0.2040849365234375,0.2037488525390625,0.20341806640625,0.2030925537109375,0.2027720703125,0.20245648193359375,0.2021457275390625,0.2018396240234375,0.20153797607421875,0.201240771484375,0.2009478271484375,0.20065908203125,0.20037431640625,0.2000935546875,0.19981661376953125,0.1995434326171875,0.1992739013671875,0.1990079345703125,0.19874544677734374,0.198486279296875,0.1982304443359375,0.19797779541015625,0.1977283447265625,0.1974819580078125,0.19723853759765625,0.196998046875,0.19676043701171875,0.19652562255859374,0.19629346923828125,0.1960640380859375,0.19583724365234376,0.19561292724609375,0.195391162109375,0.19517177734375,0.1949548095703125,0.19474017333984375,0.19452783203125,0.1943177001953125,0.194109814453125,0.19390406494140625,0.1937003662109375,0.1934987548828125,0.1932991943359375,0.19310155029296874,0.192905859375,0.1927120849609375,0.1925201904296875,0.19233004150390626,0.192141748046875,0.19195526123046874,0.1917705078125,0.19158734130859376,0.191405859375,0.19122601318359375,0.1910477783203125,0.19087119140625,0.190696044921875,0.1905224609375,0.1903503662109375,0.1901796875,0.19001053466796874,0.1898427001953125,0.1896762939453125,0.18951126708984375,0.18934755859375,0.18918521728515625,0.18902412109375,0.18886429443359376,0.188705712890625,0.188548486328125,0.1883923583984375,0.188237451171875,0.18808370361328125,0.18793116455078124,0.18777972412109376,0.18762939453125,0.18748018798828125,0.1873321044921875,0.1871849853515625,0.1870390625,0.186894091796875,0.1867501220703125,0.1866072021484375,0.1864653076171875,0.1863243408203125,0.186184326171875,0.18604532470703125,0.1859072509765625,0.1857699951171875,0.1856337646484375,0.1854983642578125,0.18536387939453125,0.1852302978515625,0.185097509765625,0.1849656005859375,0.184834521484375,0.184704296875,0.184574853515625,0.18444622802734376,0.18431842041015625,0.18419136962890625,0.18406507568359376,0.18393953857421874,0.18381478271484375,0.1836907958984375,0.18356751708984376,0.1834448974609375,0.1833230712890625,0.18320194091796874,0.1830814453125,0.182961767578125,0.1828426513671875,0.1827242431640625,0.1826065185546875,0.182489404296875,0.1823729736328125,0.18225718994140624,0.18214202880859376,0.1820274658203125,0.18191353759765624,0.181800244140625,0.18168756103515624,0.181575439453125,0.1814638916015625,0.18135296630859374,0.18124259033203124,0.18113280029296874,0.1810235595703125,0.1809148681640625,0.180806787109375,0.18069920654296875,0.1805921630859375,0.18048560791015625,0.18037967529296875,0.18027415771484376,0.1801692626953125,0.18006484375,0.179960888671875,0.1798574951171875,0.1797545166015625,0.17965206298828126,0.1795501220703125,0.1794486572265625,0.17934761962890625,0.1792470703125,0.17914703369140625,0.17904737548828126,0.1789482177734375,0.17884951171875,0.17875125732421876,0.17865340576171876,0.178556103515625,0.1784591064453125,0.17836259765625,0.17826650390625,0.17817081298828125,0.17807552490234374,0.17798067626953126,0.1778862548828125,0.1777922119140625,0.17769857177734374,0.17760535888671874,0.17751251220703124,0.17742003173828125,0.17732796630859374,0.177236279296875,0.1771449951171875,0.1770541015625,0.1769634765625,0.17687327880859374,0.17678350830078124,0.176693994140625,0.17660489501953125,0.1765161376953125,0.176427783203125,0.1763397216796875,0.17625198974609374,0.1761646240234375,0.17607764892578126,0.17599093017578124,0.175904638671875,0.175818603515625,0.17573291015625,0.17564754638671876,0.1755625244140625,0.1754778076171875,0.175393408203125,0.17530931396484375,0.17522552490234375,0.17514205322265625,0.1750589111328125,0.17497608642578125,0.1748935302734375,0.1748113037109375,0.174729345703125,0.17464764404296876,0.17456632080078124,0.17448524169921875,0.1744044677734375,0.17432396240234374,0.17424375,0.1741638671875,0.1740841796875,0.1740048095703125,0.17392568359375,0.1738468505859375,0.17376832275390625,0.173689990234375,0.1736119873046875,0.1735342041015625,0.173456689453125,0.173379443359375,0.173302490234375,0.1732257568359375,0.1731492919921875,0.17307310791015626,0.17299708251953125,0.1729214111328125,0.172845947265625,0.172770703125,0.1726957275390625,0.1726209716796875,0.1725464599609375,0.17247218017578125,0.1723981689453125,0.172324365234375,0.17225078125,0.172177490234375,0.17210439453125,0.1720315185546875,0.17195882568359375,0.171886474609375,0.1718142578125,0.1717422607421875,0.17167049560546874,0.17159893798828124,0.17152763671875,0.1714565185546875,0.1713856201171875,0.17131494140625,0.1712444580078125,0.1711742431640625,0.17110416259765626,0.17103433837890625,0.17096466064453125,0.1708952880859375,0.1708260009765625,0.170756982421875,0.1706881591796875,0.1706195068359375,0.17055106201171874,0.1704828369140625,0.1704147705078125,0.17034691162109375,0.170279248046875,0.17021180419921875,0.17014449462890624,0.17007740478515626,0.1700105224609375,0.16994375,0.16987725830078124,0.1698109130859375,0.1697447265625,0.1696787841796875,0.169612939453125,0.16954736328125,0.1694818603515625,0.1694166015625,0.16935146484375,0.1692865966796875,0.16922181396484376,0.1691572509765625,0.16909283447265624,0.1690285888671875,0.1689645751953125,0.1689006591796875,0.1688369140625,0.16877335205078126,0.168709912109375,0.1686467041015625,0.1685836181640625,0.16852073974609375,0.1684580078125,0.16839539794921876,0.1683329345703125,0.168270703125,0.1682085693359375,0.168146630859375,0.168084814453125,0.1680231689453125,0.16796168212890625,0.16790032958984374,0.16783916015625,0.1677781005859375,0.16771724853515624,0.167656494140625,0.1675959228515625,0.16753546142578124,0.1674751708984375,0.16741502685546875,0.1673550048828125,0.16729515380859375,0.16723543701171875,0.167175830078125,0.16711644287109376,0.16705714111328124,0.166997998046875,0.16693897705078126,0.1668801025390625,0.1668214599609375,0.1667628173828125,0.16670435791015625,0.16664599609375,0.16658785400390624,0.16652978515625,0.16647183837890625,0.1664140869140625,0.16635643310546874,0.166298876953125,0.1662415283203125,0.16618424072265625,0.16612708740234375,0.1660701171875,0.1660132080078125,0.1659565185546875,0.1658998779296875,0.165843359375,0.16578697509765625,0.16573074951171876,0.16567462158203125,0.1656186279296875,0.165562744140625,0.165506982421875,0.1654513671875,0.16539583740234376,0.1653404296875,0.1652851806640625,0.165230078125,0.165175,0.16512008056640626,0.16506527099609375,0.165010595703125,0.16495601806640625,0.16490155029296874,0.1648472412109375,0.16479296875,0.16473892822265626,0.1646848876953125,0.1646309814453125,0.1645772705078125,0.16452359619140625,0.16447003173828126,0.1644165771484375,0.16436329345703124,0.16431004638671876,0.16425693359375,0.164203955078125,0.1641510498046875,0.16409825439453124,0.1640455810546875,0.163993017578125,0.16394052734375,0.16388818359375,0.163835888671875,0.1637837158203125,0.16373167724609375,0.1636797119140625,0.16362789306640624,0.1635761474609375,0.16352451171875,0.16347296142578124,0.1634215087890625,0.1633701904296875,0.16331895751953124,0.16326778564453126,0.16321676025390625,0.1631658203125,0.1631149658203125,0.1630642333984375,0.16301357421875,0.162963037109375,0.162912548828125,0.16286219482421874,0.1628119384765625,0.1627617431640625,0.16271162109375,0.1626616943359375,0.16261181640625,0.16256202392578126,0.16251229248046875,0.16246265869140625,0.16241318359375,0.162363720703125,0.162314404296875,0.16226512451171876,0.1622159912109375,0.16216690673828124,0.1621179443359375,0.16206904296875,0.162020263671875,0.16197152099609374,0.161922900390625,0.1618744140625,0.16182593994140626,0.161777587890625,0.16172930908203126,0.16168111572265625,0.1616329833984375,0.1615849853515625,0.161537060546875,0.16148917236328125,0.1614414794921875,0.1613937744140625,0.1613461669921875,0.16129866943359375,0.16125125732421874,0.16120390625,0.161156640625,0.16110943603515626,0.161062353515625,0.16101534423828126,0.1609683837890625,0.1609215087890625,0.1608747314453125,0.16082803955078126,0.160781396484375,0.1607348876953125,0.16068843994140625,0.16064200439453125,0.16059576416015625,0.16054947509765624,0.16050338134765624,0.16045728759765626,0.1604113037109375,0.1603654052734375,0.1603195556640625,0.1602737548828125,0.160228076171875,0.160182421875,0.1601368896484375,0.1600914306640625,0.160046044921875,0.16000072021484374,0.1599554931640625,0.159910302734375,0.1598652099609375,0.15982021484375,0.159775244140625,0.15973037109375,0.15968558349609374,0.15964080810546874,0.1595961669921875,0.1595515625,0.15950704345703126,0.1594626220703125,0.1594182373046875,0.15937392578125,0.1593296875,0.1592855224609375,0.1592414306640625,0.1591973876953125,0.15915341796875,0.159109521484375,0.1590657470703125,0.15902196044921876,0.158978271484375,0.15893468017578125,0.15889111328125,0.1588475830078125,0.15880418701171875,0.15876085205078125,0.1587175537109375,0.15867435302734376,0.158631201171875,0.158588134765625,0.1585450927734375,0.15850213623046874,0.15845926513671876,0.15841644287109374,0.1583736572265625,0.15833099365234374,0.15828831787109374,0.15824580078125,0.1582032470703125,0.15816080322265624,0.15811844482421875,0.158076123046875,0.15803387451171874,0.1579916748046875,0.157949560546875,0.1579074462890625,0.157865478515625,0.15782353515625,0.157781640625,0.15773983154296875,0.15769803466796875,0.1576563720703125,0.15761474609375,0.15757315673828126,0.1575316650390625,0.157490185546875,0.157448779296875,0.157407470703125,0.157366162109375,0.1573249267578125,0.1572837646484375,0.15724268798828125,0.15720164794921876,0.15716064453125,0.1571197509765625,0.15707886962890624,0.157038037109375,0.1569972900390625,0.15695655517578125,0.15691591796875,0.1568753662109375,0.15683482666015625,0.15679437255859374,0.156753955078125,0.15671358642578126,0.1566732666015625,0.156633056640625,0.15659287109375,0.1565527099609375,0.156512646484375,0.156472607421875,0.1564326416015625,0.156392724609375,0.15635283203125,0.1563130615234375,0.156273291015625,0.15623359375,0.15619395751953125,0.156154345703125,0.15611484375,0.156075341796875,0.15603590087890626,0.15599652099609376,0.155957177734375,0.155917919921875,0.1558787109375,0.15583956298828125,0.1558004150390625,0.1557613525390625,0.1557223388671875,0.15568338623046876,0.15564449462890625,0.155605615234375,0.1555668212890625,0.1555280517578125,0.15548935546875,0.15545067138671875,0.155412109375,0.15537354736328124,0.1553350341796875,0.1552965576171875,0.1552581787109375,0.155219775390625,0.155181494140625,0.15514324951171876,0.1551050537109375,0.1550668701171875,0.155028759765625,0.1549906982421875,0.15495272216796874,0.15491470947265626,0.15487681884765625,0.15483896484375,0.15480111083984374,0.1547633544921875,0.15472564697265626,0.1546879150390625,0.15465032958984376,0.15461275634765625,0.15457523193359374,0.15453770751953125,0.15450029296875,0.15446290283203126,0.15442557373046875,0.154388232421875,0.1543510009765625,0.15431376953125,0.1542766357421875,0.1542395263671875,0.1542024169921875,0.1541654052734375,0.15412843017578126,0.15409146728515624,0.15405460205078125,0.15401776123046876,0.15398094482421876,0.15394422607421876,0.153907470703125,0.1538708251953125,0.1538342041015625,0.15379761962890626,0.153761083984375,0.15372459716796874,0.15368818359375,0.1536517578125,0.1536154296875,0.1535791015625,0.1535427978515625,0.1535066162109375,0.15347041015625,0.15343426513671876,0.1533981689453125,0.153362158203125,0.153326123046875,0.15329013671875,0.15325421142578124,0.15321832275390626,0.15318251953125,0.1531467041015625,0.15311092529296874,0.1530752197265625,0.1530395263671875,0.15300391845703126,0.1529683349609375,0.15293275146484375,0.152897265625,0.1528617919921875,0.1528263671875,0.1527909912109375,0.1527556396484375,0.15272032470703126,0.1526850830078125,0.1526498779296875,0.152614697265625,0.15257952880859374,0.1525444091796875,0.152509326171875,0.15247431640625,0.1524393310546875,0.15240445556640625,0.152369482421875,0.152334619140625,0.1522998291015625,0.1522650390625,0.1522302978515625,0.15219559326171875,0.1521609130859375,0.15212628173828124,0.15209169921875,0.1520571533203125,0.15202264404296875,0.151988134765625,0.1519537353515625,0.15191934814453126,0.1518849853515625,0.15185064697265624,0.151816357421875,0.15178212890625,0.15174793701171874,0.1517137451171875,0.151679638671875,0.1516455322265625,0.1516114501953125,0.15157742919921874,0.15154346923828124,0.151509521484375,0.151475634765625,0.15144176025390624,0.1514078857421875,0.151374072265625,0.151340380859375,0.15130660400390625,0.15127293701171876,0.15123927001953125,0.1512056640625,0.1511720703125,0.151138525390625,0.15110501708984375,0.15107154541015624,0.1510381103515625,0.1510047119140625,0.15097139892578126,0.1509380126953125,0.150904736328125,0.1508714599609375,0.1508382568359375,0.1508050537109375,0.150771875,0.15073876953125,0.1507056640625,0.15067264404296876,0.15063963623046875,0.1506066650390625,0.1505737060546875,0.1505408203125,0.15050791015625,0.1504750732421875,0.15044228515625,0.1504094970703125,0.15037679443359375,0.15034404296875,0.1503114013671875,0.150278759765625,0.15024615478515624,0.15021358642578125,0.1501810302734375,0.15014853515625,0.15011610107421874,0.1500836669921875,0.1500512451171875,0.15001885986328126,0.1499865478515625,0.149954248046875,0.14992193603515624,0.1498897216796875,0.1498575439453125,0.149825341796875,0.14979317626953126,0.149761083984375,0.1497290283203125,0.1496969970703125,0.14966494140625,0.1496329833984375,0.1496010498046875,0.149569091796875,0.1495372314453125,0.1495053955078125,0.1494735595703125,0.1494418212890625,0.1494100341796875,0.14937828369140624,0.1493466064453125,0.1493149658203125,0.14928333740234376,0.1492517333984375,0.149220166015625,0.14918861083984375,0.1491571044921875,0.14912567138671876,0.14909423828125,0.14906279296875,0.1490314208984375,0.1490000732421875,0.14896878662109375,0.148937451171875,0.1489061767578125,0.148874951171875,0.14884376220703124,0.14881260986328124,0.148781494140625,0.148750341796875,0.1487192626953125,0.1486882568359375,0.1486572509765625,0.14862626953125,0.1485953125,0.1485643798828125,0.14853343505859376,0.14850257568359376,0.14847177734375,0.1484409423828125,0.1484101806640625,0.1483794189453125,0.1483487060546875,0.14831800537109374,0.14828734130859375,0.1482567138671875,0.14822608642578125,0.14819556884765625,0.148164990234375,0.1481344482421875,0.1481039794921875,0.1480735107421875,0.1480430908203125,0.1480127197265625,0.14798232421875,0.1479519775390625,0.1479216552734375,0.147891357421875,0.14786109619140625,0.14783087158203126,0.1478006591796875,0.1477704833984375,0.14774033203125,0.14771019287109374,0.14768013916015624,0.1476500244140625,0.1476200439453125,0.147589990234375,0.14756007080078126,0.147530078125,0.14750018310546875,0.147470263671875,0.1474404296875,0.1474105224609375,0.14738074951171876,0.147350927734375,0.14732117919921875,0.147291455078125,0.1472617431640625,0.1472320556640625,0.14720238037109376,0.14717276611328126,0.1471431884765625,0.1471135986328125,0.14708406982421876,0.14705457763671875,0.14702506103515625,0.1469956298828125,0.146966162109375,0.1469367919921875,0.1469073486328125,0.146877978515625,0.14684873046875,0.1468193603515625,0.1467900634765625,0.1467608154296875,0.146731591796875,0.1467024169921875,0.14667320556640626,0.14664405517578125,0.14661494140625,0.1465858642578125,0.14655673828125,0.1465277099609375,0.1464987060546875,0.1464697021484375,0.1464406982421875,0.1464117919921875,0.1463828857421875,0.14635396728515626,0.1463250732421875,0.146296240234375,0.146267431640625,0.14623865966796876,0.14620987548828124,0.14618115234375,0.146152392578125,0.1461237060546875,0.14609505615234375,0.14606639404296876,0.14603779296875,0.1460092041015625,0.1459806396484375,0.1459520751953125,0.1459235595703125,0.14589505615234374,0.1458666259765625,0.145838134765625,0.14580970458984374,0.1457813232421875,0.1457529541015625,0.145724609375,0.145696240234375,0.1456679931640625,0.14563970947265625,0.1456114990234375,0.1455832275390625,0.145555029296875,0.14552685546875,0.1454987060546875,0.1454705322265625,0.145442431640625,0.1454143798828125,0.14538631591796874,0.14535823974609374,0.1453302490234375,0.14530224609375,0.14527427978515625,0.1452463623046875,0.1452184326171875,0.14519053955078126,0.1451626953125,0.14513480224609376,0.14510701904296874,0.14507919921875,0.14505140380859374,0.14502364501953124,0.144995947265625,0.1449682373046875,0.14494049072265625,0.1449128662109375,0.14488524169921874,0.1448575927734375,0.14482999267578126,0.14480242919921876,0.14477486572265624,0.1447473388671875,0.14471982421875,0.144692333984375,0.14466490478515626,0.1446375,0.14461005859375,0.14458265380859375,0.1445552490234375,0.14452796630859374,0.14450062255859375,0.14447333984375,0.14444603271484374,0.14441876220703126,0.144391552734375,0.14436434326171876,0.1443371337890625,0.1443099365234375,0.1442827880859375,0.144255712890625,0.14422862548828125,0.14420150146484376,0.14417447509765624,0.1441473388671875,0.14412037353515625,0.144093408203125,0.14406640625,0.144039453125,0.14401251220703126,0.14398564453125,0.143958740234375,0.1439318603515625,0.143905029296875,0.14387821044921875,0.1438513916015625,0.1438246337890625,0.14379786376953124,0.143771142578125,0.14374443359375,0.14371768798828125,0.143691015625,0.1436643798828125,0.1436377197265625,0.1436111572265625,0.14358455810546875,0.14355799560546875,0.1435314208984375,0.14350491943359375,0.143478369140625,0.143451904296875,0.143425439453125,0.1433989990234375,0.14337254638671876,0.14334615478515625,0.14331978759765626,0.1432934326171875,0.14326705322265626,0.1432407470703125,0.143214453125,0.1431882080078125,0.1431619140625,0.143135693359375,0.143109423828125,0.14308323974609374,0.143057080078125,0.143030908203125,0.1430047607421875,0.14297862548828125,0.1429525634765625,0.14292646484375,0.14290040283203126,0.142874365234375,0.1428483642578125,0.1428223876953125,0.1427963623046875,0.14277041015625,0.14274447021484374,0.14271856689453125,0.14269266357421875,0.142666748046875,0.14264091796875,0.142615087890625,0.14258924560546876,0.1425634521484375,0.14253765869140625,0.1425118896484375,0.1424861572265625,0.1424604736328125,0.1424347412109375,0.142409033203125,0.1423833984375,0.14235771484375,0.14233211669921875,0.1423065185546875,0.142280908203125,0.14225535888671875,0.14222979736328126,0.14220426025390626,0.1421787841796875,0.1421532470703125,0.1421277587890625,0.1421023681640625,0.142076904296875,0.14205146484375,0.14202607421875,0.1420007080078125,0.1419753173828125,0.1419499755859375,0.14192467041015625,0.1418993408203125,0.1418740478515625,0.14184879150390625,0.1418235595703125,0.14179830322265624,0.1417731201171875,0.14174791259765626,0.14172274169921875,0.1416975830078125,0.14167242431640625,0.1416472900390625,0.141622216796875,0.1415971435546875,0.1415720458984375,0.14154703369140625,0.14152197265625,0.1414969970703125,0.14147197265625,0.141447021484375,0.14142203369140624,0.14139710693359375,0.141372216796875,0.1413473388671875,0.14132239990234374,0.14129755859375,0.14127265625,0.1412478271484375,0.141223046875,0.14119825439453124,0.1411734619140625,0.14114871826171874,0.141123974609375,0.141099267578125,0.14107452392578124,0.141049853515625,0.1410251708984375,0.1410005126953125,0.14097591552734376,0.1409512451171875,0.14092667236328124,0.1409020751953125,0.14087752685546875,0.140852978515625,0.1408284423828125,0.14080390625,0.1407794189453125,0.140754931640625,0.1407304931640625,0.1407060546875,0.14068162841796875,0.1406572265625,0.1406328369140625,0.140608447265625,0.14058406982421875,0.14055972900390626,0.140535400390625,0.140511083984375,0.14048681640625,0.140462548828125,0.14043831787109376,0.1404140380859375,0.1403898193359375,0.140365576171875,0.14034144287109376,0.140317236328125,0.1402930908203125,0.1402689697265625,0.1402448486328125,0.14022073974609375,0.14019666748046875,0.14017255859375,0.1401485595703125,0.14012445068359375,0.1401004638671875,0.14007645263671875,0.1400524658203125,0.140028515625,0.1400045654296875,0.139980615234375,0.139956689453125,0.1399327880859375,0.13990888671875,0.1398850341796875,0.1398611328125,0.13983731689453124,0.139813525390625,0.139789697265625,0.139765869140625,0.1397421142578125,0.139718359375,0.1396946044921875,0.1396708740234375,0.13964716796875,0.1396234619140625,0.13959981689453124,0.139576123046875,0.13955250244140624,0.1395288330078125,0.1395052490234375,0.139481640625,0.1394580810546875,0.1394344970703125,0.1394109375,0.13938740234375,0.1393638916015625,0.13934039306640625,0.13931689453125,0.13929339599609375,0.13926995849609375,0.139246533203125,0.1392230712890625,0.1391996826171875,0.13917625732421876,0.1391528564453125,0.13912952880859375,0.139106201171875,0.139082861328125,0.1390595703125,0.13903623046875,0.1390129150390625,0.1389896728515625,0.1389664306640625,0.13894317626953126,0.1389199462890625,0.13889677734375,0.13887354736328125,0.13885035400390625,0.1388272216796875,0.1388040283203125,0.1387809326171875,0.1387578369140625,0.1387346923828125,0.1387115966796875,0.13868857421875,0.1386654541015625,0.1386424560546875,0.13861943359375,0.1385964111328125,0.1385734130859375,0.138550439453125,0.1385274658203125,0.13850452880859376,0.1384815673828125,0.138458642578125,0.1384357666015625,0.13841282958984374,0.13838997802734376,0.13836712646484375,0.138344287109375,0.1383214111328125,0.13829862060546874,0.138275830078125,0.13825303955078125,0.13823026123046875,0.138207470703125,0.138184716796875,0.13816202392578125,0.1381392822265625,0.13811658935546875,0.138093896484375,0.1380712158203125,0.13804857177734375,0.138025927734375,0.13800328369140624,0.1379806884765625,0.13795809326171876,0.13793548583984375,0.137912939453125,0.1378903564453125,0.1378677978515625,0.1378453125,0.1378227783203125,0.13780028076171874,0.1377778076171875,0.1377553466796875,0.137732861328125,0.1377104248046875,0.13768800048828125,0.13766558837890625,0.13764315185546874,0.1376208251953125,0.1375984375,0.13757607421875,0.1375537353515625,0.13753140869140626,0.13750908203125,0.137486767578125,0.137464501953125,0.137442236328125,0.13741995849609376,0.1373976806640625,0.13737545166015624,0.137353271484375,0.1373310546875,0.137308837890625,0.13728665771484375,0.1372645263671875,0.13724237060546876,0.1372202392578125,0.13719810791015624,0.1371760009765625,0.1371539306640625,0.1371318359375,0.13710975341796874,0.13708770751953125,0.137065673828125,0.1370436279296875,0.1370216064453125,0.13699962158203124,0.13697762451171874,0.1369556396484375,0.13693370361328125,0.13691175537109376,0.1368898193359375,0.1368678955078125,0.13684600830078125,0.136824072265625,0.13680224609375,0.13678037109375,0.13675849609375,0.1367366943359375,0.1367148681640625,0.1366930419921875,0.13667125244140624,0.1366494140625,0.136627685546875,0.1366059326171875,0.1365841552734375,0.1365624267578125,0.1365407470703125,0.1365190185546875,0.13649732666015624,0.13647568359375,0.13645396728515624,0.1364323486328125,0.136410693359375,0.1363890625,0.13636744384765626,0.136345849609375,0.1363242431640625,0.136302685546875,0.13628115234375,0.13625955810546875,0.13623804931640626,0.1362165283203125,0.1361949951171875,0.1361735107421875,0.1361520263671875,0.1361305419921875,0.13610911865234376,0.13608760986328125,0.1360662109375,0.1360447998046875,0.1360234375,0.136002001953125,0.135980615234375,0.135959228515625,0.13593787841796875,0.1359165283203125,0.1358951904296875,0.1358738525390625,0.1358525634765625,0.1358312744140625,0.1358099853515625,0.135788720703125,0.13576749267578125,0.1357462158203125,0.1357249755859375,0.135703759765625,0.13568255615234376,0.1356614013671875,0.135640185546875,0.13561904296875,0.13559786376953126,0.13557672119140626,0.13555562744140626,0.1355344482421875,0.13551337890625,0.13549227294921876,0.13547119140625,0.13545009765625,0.135429052734375,0.1354080078125,0.135386962890625,0.1353659423828125,0.1353449462890625,0.1353239501953125,0.1353029296875,0.135281982421875,0.13526103515625,0.1352400634765625,0.135219140625,0.1351982177734375,0.1351772705078125,0.13515640869140624,0.135135498046875,0.1351146240234375,0.1350937744140625,0.1350729248046875,0.13505206298828126,0.1350312255859375,0.13501044921875,0.1349896240234375,0.13496884765625,0.13494810791015624,0.1349272705078125,0.13490653076171874,0.134885791015625,0.1348650634765625,0.1348443359375,0.1348236572265625,0.1348029296875,0.1347822265625,0.13476158447265624,0.1347408935546875,0.1347202392578125,0.1346996337890625,0.1346789794921875,0.13465833740234376,0.1346377685546875,0.1346171630859375,0.13459656982421875,0.1345760498046875,0.13455545654296874,0.134534912109375,0.1345143798828125,0.134493896484375,0.1344733642578125,0.1344528564453125,0.1344323974609375,0.134411865234375,0.134391455078125,0.134370947265625,0.1343505615234375,0.13433006591796876,0.13430968017578124,0.1342892822265625,0.13426888427734374,0.13424849853515625,0.134228125,0.134207763671875,0.1341873779296875,0.13416707763671876,0.1341467529296875,0.134126416015625,0.134106103515625,0.13408583984375,0.1340655517578125,0.13404525146484375,0.134025,0.134004736328125,0.13398447265625,0.1339642822265625,0.133944091796875,0.133923876953125,0.13390367431640626,0.13388349609375,0.13386331787109376,0.133843115234375,0.13382303466796874,0.1338029052734375,0.1337827392578125,0.133762646484375,0.13374251708984375,0.133722412109375,0.1337023681640625,0.13368228759765624,0.1336622314453125,0.1336421630859375,0.133622119140625,0.133602099609375,0.133582080078125,0.1335620849609375,0.13354208984375,0.1335220947265625,0.13350211181640625,0.133482177734375,0.13346220703125,0.13344228515625,0.13342235107421874,0.13340244140625,0.1333824951171875,0.1333626220703125,0.133342724609375,0.13332286376953126,0.1333030029296875,0.133283154296875,0.13326329345703125,0.13324344482421874,0.13322364501953124,0.13320382080078125,0.1331840087890625,0.1331642333984375,0.13314447021484374,0.1331246826171875,0.1331049560546875,0.133085205078125,0.1330654296875,0.1330457275390625,0.13302601318359375,0.1330062744140625,0.13298658447265624,0.13296690673828124,0.1329472412109375,0.13292760009765625,0.1329079345703125,0.13288828125,0.13286864013671876,0.1328490234375,0.13282945556640624,0.132809814453125,0.132790234375,0.13277064208984374,0.1327510986328125,0.13273154296875,0.1327119873046875,0.1326924560546875,0.13267288818359374,0.13265338134765625,0.1326338623046875,0.13261439208984374,0.132594921875,0.132575390625,0.1325559814453125,0.1325364990234375,0.132517041015625,0.1324976318359375,0.1324781982421875,0.13245875244140626,0.13243935546875,0.1324199951171875,0.1324005859375,0.1323811767578125,0.13236182861328125,0.1323425048828125,0.13232314453125,0.1323038330078125,0.1322844970703125,0.13226517333984375,0.1322458984375,0.1322265625,0.1322072998046875,0.132188037109375,0.1321687744140625,0.1321494873046875,0.1321302490234375,0.13211103515625,0.132091796875,0.13207259521484374,0.13205340576171876,0.1320342041015625,0.1320150146484375,0.13199583740234375,0.13197669677734375,0.13195753173828126,0.13193839111328126,0.13191923828125,0.13190009765625,0.131881005859375,0.131861865234375,0.13184281005859375,0.13182371826171874,0.13180462646484375,0.13178558349609376,0.13176650390625,0.1317474853515625,0.13172840576171874,0.1317093994140625,0.1316904052734375,0.13167138671875,0.131652392578125,0.13163336181640625,0.1316144287109375,0.131595458984375,0.1315764892578125,0.1315575439453125,0.131538623046875,0.13151966552734376,0.13150078125,0.13148184814453126,0.13146295166015626,0.13144404296875,0.1314251708984375,0.1314063232421875,0.1313874267578125,0.13136856689453125,0.13134974365234375,0.1313308837890625,0.1313120849609375,0.13129327392578125,0.1312744384765625,0.1312556396484375,0.1312368408203125,0.13121807861328125,0.13119931640625,0.13118057861328125,0.13116181640625,0.13114305419921876,0.1311243408203125,0.131105615234375,0.1310869140625,0.13106820068359376,0.1310495361328125,0.13103082275390626,0.1310121826171875,0.130993505859375,0.13097484130859374,0.130956201171875,0.130937548828125,0.1309189208984375,0.13090032958984374,0.1308817138671875,0.1308630859375,0.13084453125,0.130825927734375,0.13080732421875,0.130788818359375,0.130770263671875,0.13075169677734375,0.1307331298828125,0.1307146484375,0.13069610595703124,0.1306776123046875,0.13065909423828126,0.130640625,0.1306221435546875,0.13060364990234374,0.130585205078125,0.1305667236328125,0.1305483154296875,0.130529833984375,0.13051146240234374,0.130493017578125,0.1304746337890625,0.13045621337890626,0.13043782958984376,0.13041943359375,0.13040107421875,0.13038272705078124,0.1303643798828125,0.1303460205078125,0.130327685546875,0.1303093505859375,0.1302910400390625,0.1302727294921875,0.13025439453125,0.1302361328125,0.1302178466796875,0.130199609375,0.1301812744140625,0.130163037109375,0.1301447998046875,0.13012659912109376,0.1301083251953125,0.13009012451171875,0.1300718994140625,0.1300537109375,0.13003548583984376,0.130017333984375,0.12999915771484374,0.129981005859375,0.12996280517578124,0.12994468994140626,0.12992655029296876,0.1299084228515625,0.1298902587890625,0.12987216796875,0.129854052734375,0.1298359619140625,0.12981785888671876,0.1297997802734375,0.12978173828125,0.12976361083984375,0.12974561767578124,0.1297275634765625,0.1297094970703125,0.12969146728515624,0.1296734375,0.1296554443359375,0.12963746337890625,0.12961943359375,0.12960150146484375,0.12958350830078125,0.12956552734375,0.1295475341796875,0.12952960205078126,0.129511669921875,0.129493701171875,0.12947584228515624,0.12945791015625,0.12944000244140624,0.1294220947265625,0.12940421142578126,0.129386279296875,0.129368408203125,0.12935054931640624,0.12933271484375,0.12931485595703124,0.129297021484375,0.1292791748046875,0.12926136474609376,0.1292435546875,0.129225732421875,0.1292079345703125,0.1291901123046875,0.1291723388671875,0.1291545654296875,0.12913680419921875,0.12911904296875,0.1291012939453125,0.129083544921875,0.1290658203125,0.12904813232421875,0.129030419921875,0.1290126953125,0.1289949951171875,0.12897728271484374,0.12895960693359376,0.1289419189453125,0.12892427978515625,0.12890662841796874,0.1288889404296875,0.1288713134765625,0.1288536865234375,0.1288360595703125,0.12881845703125,0.12880086669921875,0.1287832275390625,0.128765625,0.128748046875,0.1287304443359375,0.12871287841796875,0.1286953369140625,0.1286778076171875,0.1286602294921875,0.12864271240234376,0.1286251708984375,0.128607666015625,0.1285901611328125,0.12857265625,0.1285551513671875,0.1285376708984375,0.12852017822265624,0.1285027099609375,0.12848524169921874,0.128467822265625,0.12845035400390625,0.12843287353515626,0.12841549072265626,0.128398046875,0.1283806640625,0.128363232421875,0.1283458251953125,0.12832845458984374,0.12831107177734374,0.1282936767578125,0.1282763427734375,0.128258935546875,0.1282415771484375,0.12822421875,0.12820692138671874,0.1281895751953125,0.128172265625,0.128154931640625,0.1281376708984375,0.128120361328125,0.128103076171875,0.128085791015625,0.12806851806640626,0.1280512451171875,0.12803399658203124,0.1280167236328125,0.12799949951171874,0.12798226318359374,0.1279650390625,0.127947802734375,0.127930615234375,0.1279134033203125,0.1278962158203125,0.1278790283203125,0.12786182861328124,0.12784466552734375,0.127827490234375,0.1278103515625,0.12779320068359376,0.1277760498046875,0.127758935546875,0.127741796875,0.127724658203125,0.127707568359375,0.12769044189453124,0.127673388671875,0.12765626220703125,0.127639208984375,0.127622119140625,0.12760506591796875,0.12758797607421876,0.127570947265625,0.12755391845703126,0.12753687744140624,0.1275198486328125,0.12750283203125,0.1274858154296875,0.127468798828125,0.12745181884765625,0.127434814453125,0.1274178466796875,0.12740089111328126,0.12738387451171876,0.1273669189453125,0.12734996337890625,0.127333056640625,0.12731610107421876,0.12729918212890626,0.1272822509765625,0.1272653076171875,0.127248388671875,0.1272315185546875,0.12721463623046875,0.12719775390625,0.12718087158203126,0.12716400146484375,0.127147119140625,0.1271302978515625,0.1271134521484375,0.12709659423828126,0.12707974853515625,0.12706292724609375,0.1270461181640625,0.127029296875,0.12701248779296875,0.12699569091796875,0.12697890625,0.126962158203125,0.12694534912109376,0.1269285888671875,0.1269118408203125,0.12689508056640625,0.1268783203125,0.126861572265625,0.126844873046875,0.1268281494140625,0.12681142578125,0.1267947021484375,0.12677802734375,0.1267613037109375,0.1267446044921875,0.1267279541015625,0.1267113037109375,0.1266946044921875,0.1266779541015625,0.12666131591796875,0.1266446533203125,0.1266280029296875,0.12661138916015624,0.126594775390625,0.12657813720703126,0.12656156005859376,0.1265449462890625,0.126528369140625,0.12651182861328125,0.1264951904296875,0.1264786376953125,0.12646207275390625,0.1264455078125,0.1264289794921875,0.12641241455078125,0.12639588623046874,0.12637933349609376,0.12636279296875,0.1263463134765625,0.12632982177734375,0.12631329345703124,0.1262968017578125,0.126280322265625,0.126263818359375,0.12624735107421875,0.1262308837890625,0.12621444091796874,0.12619794921875,0.12618155517578125,0.1261651123046875,0.1261486572265625,0.1261322265625,0.1261158203125,0.1260994140625,0.1260830078125,0.12606661376953124,0.1260501953125,0.12603385009765625,0.12601746826171875,0.12600111083984375,0.1259847412109375,0.12596837158203125,0.1259520263671875,0.12593568115234374,0.1259193603515625,0.12590303955078125,0.12588670654296874,0.12587039794921875,0.12585408935546874,0.12583779296875,0.125821484375,0.1258052001953125,0.12578895263671874,0.12577266845703125,0.125756396484375,0.12574012451171876,0.1257239013671875,0.12570762939453126,0.12569139404296875,0.1256751708984375,0.1256589111328125,0.125642724609375,0.125626513671875,0.1256102783203125,0.1255941162109375,0.1255779052734375,0.1255617431640625,0.12554556884765625,0.12552939453125,0.1255132080078125,0.12549705810546874,0.125480908203125,0.12546475830078124,0.1254486328125,0.12543253173828126,0.12541639404296875,0.125400244140625,0.12538416748046874,0.12536806640625,0.1253519775390625,0.1253358642578125,0.12531978759765625,0.1253037109375,0.12528763427734374,0.12527159423828124,0.1252555419921875,0.125239453125,0.1252234130859375,0.125207373046875,0.1251913330078125,0.12517535400390625,0.12515933837890625,0.125143310546875,0.1251273193359375,0.12511129150390626,0.1250953369140625,0.12507930908203124,0.1250633544921875,0.1250473876953125,0.1250314208984375,0.125015478515625,0.12499949951171875,0.1249835693359375,0.124967626953125,0.12495164794921874,0.12493577880859374,0.1249198486328125,0.1249039306640625,0.12488804931640625,0.124872119140625,0.12485625,0.1248403564453125,0.12482447509765625,0.12480860595703125,0.1247927490234375,0.12477685546875,0.12476103515625,0.1247451904296875,0.124729345703125,0.1247134765625,0.1246976806640625,0.1246818359375,0.12466602783203125,0.1246501953125,0.124634423828125,0.1246186279296875,0.124602783203125,0.1245870361328125,0.1245712646484375,0.1245554931640625,0.12453975830078125,0.124523974609375,0.1245082275390625,0.12449249267578125,0.1244767333984375,0.1244610107421875,0.1244452880859375,0.1244295654296875,0.1244138427734375,0.12439814453125,0.12438243408203126,0.12436676025390625,0.12435107421875,0.1243353515625,0.12431971435546875,0.1243040283203125,0.12428836669921875,0.12427274169921874,0.12425706787109375,0.12424140625,0.1242258056640625,0.12421015625,0.12419453125,0.12417890625,0.12416328125,0.1241476806640625,0.124132080078125,0.12411650390625,0.1241009033203125,0.1240853515625,0.12406973876953124,0.124054150390625,0.1240385986328125,0.1240230224609375,0.12400748291015624,0.1239919189453125,0.12397642822265625,0.123960888671875,0.123945361328125,0.12392982177734375,0.1239143310546875,0.12389884033203125,0.1238833251953125,0.123867822265625,0.1238523193359375,0.12383682861328126,0.1238213623046875,0.123805908203125,0.1237904296875,0.12377496337890626,0.123759521484375,0.1237440673828125,0.12372861328125,0.12371318359375,0.123697802734375,0.1236823486328125,0.123666943359375,0.123651513671875,0.1236361328125,0.123620703125,0.1236053466796875,0.12358992919921875,0.12357451171875,0.1235591796875,0.12354381103515626,0.123528466796875,0.1235130859375,0.1234977294921875,0.1234823974609375,0.1234670654296875,0.1234517333984375,0.123436376953125,0.12342105712890625,0.12340576171875,0.12339046630859375,0.1233751708984375,0.1233598388671875,0.1233445556640625,0.123329296875,0.1233140380859375,0.12329874267578125,0.1232834716796875,0.1232681884765625]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss by time\"};\n",
       "\n",
       "  Plotly.plot('plot-1486598411', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mlossSeq\u001b[39m: \u001b[32mIndexedSeq\u001b[39m[\u001b[32mcmd15Wrapper\u001b[39m.\u001b[32m<refinement>\u001b[39m.this.type.\u001b[32mOutputData\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m0.23026494140625\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mplot\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mScatter\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mScatter\u001b[39m(\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mres18_2\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-1486598411\"\u001b[39m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lossSeq = for (iteration <- 0 until 2000) yield {\n",
    "  val loss = lossFunction.train(trainData :: vectorizedTrainExpectResult :: HNil)\n",
    "  if(iteration % 100 == 0){\n",
    "    println(s\"at iteration $iteration loss is $loss\")\n",
    "  }\n",
    "  loss\n",
    "}\n",
    "\n",
    "plotly.JupyterScala.init()\n",
    "\n",
    "val plot = Seq(\n",
    "  Scatter(lossSeq.indices, lossSeq)\n",
    ")\n",
    "\n",
    "plot.plot(\n",
    "  title = \"loss by time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the neural network and predict the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the processed test data to verify the prediction result of the neural network and compute the accuracy. The accuracy shall be about 32%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result is 32.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mright\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m32.0\u001b[39m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val right = Utils.getAccuracy(myNeuralNetwork.predict(testData), testExpectResult)\n",
    "println(s\"the result is $right %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have learned the follows in this article:\n",
    "\n",
    "* Prepare and process CIFAR10 data\n",
    "* Write softmax classifier\n",
    "* Use the prediction image of the neural network written by softmax classifier to match with the probability of each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Complete code](https://github.com/izhangzhihao/deeplearning-tutorial/blob/master/src/main/scala/com/thoughtworks/deeplearning/tutorial/SoftmaxLinearClassifier.scala)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
