{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景\n",
    "\n",
    "这篇文章我们将使用softmax分类器一起来构建一个简单的图像分类神经网络，其准确率可以达到32%。Softmax分类器是二元逻辑回归泛化到多元的情况。Softmax分类器会输出对应类别的概率。\n",
    "\n",
    "我们会先定义一个softmax分类器，然后使用[cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)的训练集来训练这个神经网络。最后使用测试集来测试神经网络的准确率。\n",
    "\n",
    "我们开始吧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入依赖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似前一篇教程[GetStarted](https://thoughtworksinc.github.io/DeepLearning.scala/demo/GettingStarted.html)，我们需要引入DeepLearning.scala的各个类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                 \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.{FileInputStream, InputStream}\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableHList._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableDouble._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableAny._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.{\n",
       "  DifferentiableHList,\n",
       "  DifferentiableINDArray,\n",
       "  Layer,\n",
       "  Symbolic\n",
       "}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Layer.Batch\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathFunctions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathMethods./\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathOps\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.cpu.nativecpu.NDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.ops.transforms.Transforms\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4s.Implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mshapeless._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.immutable.IndexedSeq\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $plugin.$ivy.`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:2.0.0`\n",
    "\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableany:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablenothing:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableseq:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiabledouble:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablefloat:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablehlist:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0-RC7`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableindarray:1.0.0-RC7`\n",
    "import $ivy.`org.rauschig:jarchivelib:0.5.0`\n",
    "\n",
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.0`\n",
    "\n",
    "import java.io.{FileInputStream, InputStream}\n",
    "\n",
    "\n",
    "import com.thoughtworks.deeplearning\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import com.thoughtworks.deeplearning.DifferentiableHList._\n",
    "import com.thoughtworks.deeplearning.DifferentiableDouble._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray._\n",
    "import com.thoughtworks.deeplearning.DifferentiableAny._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
    "import com.thoughtworks.deeplearning.{\n",
    "  DifferentiableHList,\n",
    "  DifferentiableINDArray,\n",
    "  Layer,\n",
    "  Symbolic\n",
    "}\n",
    "import com.thoughtworks.deeplearning.Layer.Batch\n",
    "import com.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
    "import com.thoughtworks.deeplearning.Symbolic._\n",
    "import com.thoughtworks.deeplearning.Poly.MathFunctions._\n",
    "import com.thoughtworks.deeplearning.Poly.MathMethods./\n",
    "import com.thoughtworks.deeplearning.Poly.MathOps\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.cpu.nativecpu.NDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
    "import org.nd4j.linalg.ops.transforms.Transforms\n",
    "import org.nd4s.Implicits._\n",
    "import shapeless._\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._\n",
    "\n",
    "import scala.collection.immutable.IndexedSeq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了减少`jupyter-scala`输出的行数，避免页面输出太长，需要设置`pprintConfig`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pprintConfig() = pprintConfig().copy(height = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使用softmax分类器(softmax分类器是softmax和一个全连接组合起来的神经网络)，我们需要先编写softmax函数,公式：![](https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msoftmax\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(implicit scores: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val expScores = exp(scores)\n",
    "  expScores / expScores.sum(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全连接层需要设置学习率，学习率是Weight变化的快慢的直观描述，学习率设置的过小会导致loss下降的很慢，需要更长时间来训练，学习率设置的过大虽然刚开始下降很快但是会导致在接近最低点的时候在附近徘徊loss下降会非常慢。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36moptimizer\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit def optimizer: Optimizer = new LearningRate {\n",
    "  def currentLearningRate() = 0.00001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 组合神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个全连接层并初始化[Weight](https://github.com/ThoughtWorksInc/DeepLearning.scala/wiki/Getting-Started#231--weight-intialization)，Weight应该是一个N*NumberOfClasses的INDArray,每个图片对应每个分类都有一个评分。评分就是该图片对应每个分类的可能概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mNumberOfClasses\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcreateMyNeuralNetwork\u001b[39m\n",
       "\u001b[36mmyNeuralNetwork\u001b[39m: (\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m[\u001b[32mINDArray\u001b[39m]{type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray;type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray})#\u001b[32m@\u001b[39m = Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum(Exp(Identity()),WrappedArray(1)))),Dot(Identity(),Weight([[0.00, -0.00, -0.00, 0.00, -0.00, -0.00, -0.00\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//CIFAR10中的图片共有10个分类(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
    "val NumberOfClasses: Int = 10\n",
    "\n",
    "def createMyNeuralNetwork(implicit input: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val initialValueOfWeight = Nd4j.randn(3072, NumberOfClasses) * 0.001\n",
    "  val weight: INDArray @Symbolic = initialValueOfWeight.toWeight\n",
    "  val result: INDArray @Symbolic = input dot weight\n",
    "  softmax.compose(result) //调用softmax方法，压缩结果值在0到1之间方便处理\n",
    "}\n",
    "val myNeuralNetwork = createMyNeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 组合LossFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了判断神经网络判断的结果好坏，我们需要编写损失函数Loss Function，这里我们使用cross-entropy loss将此次判断的结果和真实结果进行对比并返回评分，公式：\n",
    "![](https://zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlossFunction\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lossFunction(implicit pair: (INDArray :: INDArray :: HNil) @Symbolic): Double @Symbolic = {\n",
    "  val input = pair.head\n",
    "  val expectedOutput = pair.tail.head\n",
    "  val probabilities = myNeuralNetwork.compose(input)\n",
    "\n",
    "  -(expectedOutput * log(probabilities)).mean //此处上面的交叉熵损失公式对应\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "为了从CIFAR10 database中读取训练数据和测试数据的图片和分类信息。我们需要[`import $file.ReadCIFAR10ToNDArray`](https://github.com/ThoughtWorksInc/DeepLearning.scala-website/blob/master/ipynbs/ReadCIFAR10ToNDArray.sc)。这是一个包含读取和处理CIFAR10 数据的脚本文件，由本教程提供。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling ReadCIFAR10ToNDArray.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                   \n",
       "\n",
       "//加载train数据,我们读取1000条数据作为训练数据\n",
       "\u001b[39m\n",
       "\u001b[36mtrainNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.23, 0.17, 0.20, 0.27, 0.38, 0.46, 0.54, 0.57, 0.58, 0.58, 0.51, 0.49, 0.55, 0.56, 0.54, 0.50, 0.54, 0.52, 0.48, 0.54, 0.54, 0.52, 0.53, 0.54, 0.59, 0.64, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.ReadCIFAR10ToNDArray\n",
    "\n",
    "//加载train数据,我们读取1000条数据作为训练数据\n",
    "val trainNDArray =\n",
    "  ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/data_batch_1.bin\", 1000)\n",
    "\n",
    "//加载测试数据，我们读取100条作为测试数据\n",
    "val testNDArray =\n",
    "  ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/test_batch.bin\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便softmax分类器处理数据，我们先处理标签数据([one hot encoding](https://en.wikipedia.org/wiki/One-hot))：将N行一列的NDArray转换为N行NumberOfClasses列的NDArray，每行对应的正确分类的值为1，其它列的值为0。这里区分训练集和测试集的原因是为了能看出网络是否被过度训练导致[过拟合](https://en.wikipedia.org/wiki/Overfitting)。处理标签数据的时候我们使用了[Utils](https://github.com/ThoughtWorksInc/DeepLearning.scala-website/blob/master/ipynbs/Utils.sc)，也由本教程提供。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Utils.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.23, 0.17, 0.20, 0.27, 0.38, 0.46, 0.54, 0.57, 0.58, 0.58, 0.51, 0.49, 0.55, 0.56, 0.54, 0.50, 0.54, 0.52, 0.48, 0.54, 0.54, 0.52, 0.53, 0.54, 0.59, 0.64, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtrainExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [6.00, 9.00, 9.00, 4.00, 1.00, 1.00, 2.00, 7.00, 8.00, 3.00, 4.00, 7.00, 7.00, 2.00, 9.00, 9.00, 9.00, 3.00, 2.00, 6.00, 4.00, 3.00, 6.00, 6.00, 2.00, 6.00, 3.0\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.0\u001b[33m...\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$    \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mvectorizedTrainExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00],\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mvectorizedTestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainData = trainNDArray.head\n",
    "val testData = testNDArray.head\n",
    "\n",
    "val trainExpectResult = trainNDArray.tail.head\n",
    "val testExpectResult = testNDArray.tail.head\n",
    "\n",
    "import $file.Utils\n",
    "\n",
    "val vectorizedTrainExpectResult = Utils.makeVectorized(trainExpectResult, NumberOfClasses)\n",
    "val vectorizedTestExpectResult = Utils.makeVectorized(testExpectResult, NumberOfClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了观察神经网络训练的过程，我们需要输出`loss`，在训练神经网络时，loss的变化趋势应该是越来越低的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at iteration 0 loss is 0.2304002685546875\n",
      "at iteration 100 loss is 0.1909240234375\n",
      "at iteration 200 loss is 0.17821024169921876\n",
      "at iteration 300 loss is 0.170245556640625\n",
      "at iteration 400 loss is 0.164287255859375\n",
      "at iteration 500 loss is 0.1594460693359375\n",
      "at iteration 600 loss is 0.1553224365234375\n",
      "at iteration 700 loss is 0.151703857421875\n",
      "at iteration 800 loss is 0.1484637939453125\n",
      "at iteration 900 loss is 0.14552034912109374\n",
      "at iteration 1000 loss is 0.1428169677734375\n",
      "at iteration 1100 loss is 0.140312744140625\n",
      "at iteration 1200 loss is 0.137976904296875\n",
      "at iteration 1300 loss is 0.1357855224609375\n",
      "at iteration 1400 loss is 0.1337197021484375\n",
      "at iteration 1500 loss is 0.13176416015625\n",
      "at iteration 1600 loss is 0.1299063232421875\n",
      "at iteration 1700 loss is 0.12813583984375\n",
      "at iteration 1800 loss is 0.1264439208984375\n",
      "at iteration 1900 loss is 0.1248230712890625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-922967534\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0,1989.0,1990.0,1991.0,1992.0,1993.0,1994.0,1995.0,1996.0,1997.0,1998.0,1999.0],\"y\":[0.2304002685546875,0.2286763427734375,0.22770341796875,0.2268083984375,0.22594404296875,0.2251056640625,0.224291650390625,0.223500927734375,0.222732568359375,0.221985693359375,0.221259228515625,0.220552587890625,0.21986474609375,0.2191949951171875,0.218542626953125,0.21790703125,0.2172873291015625,0.2166830078125,0.216093505859375,0.215518115234375,0.214956396484375,0.214407763671875,0.21387177734375,0.2133478759765625,0.212835693359375,0.2123347412109375,0.21184462890625,0.211364892578125,0.2108952880859375,0.210435400390625,0.2099847900390625,0.2095432861328125,0.2091104736328125,0.20868603515625,0.20826982421875,0.207861474609375,0.207460693359375,0.2070673828125,0.2066810791015625,0.20630166015625,0.2059289794921875,0.2055627685546875,0.2052028076171875,0.204848876953125,0.20450093994140625,0.2041586181640625,0.20382191162109375,0.2034905517578125,0.2031644287109375,0.202843359375,0.20252724609375,0.202215966796875,0.201909228515625,0.20160716552734376,0.20130941162109375,0.20101597900390625,0.2007266845703125,0.2004414794921875,0.2001602294921875,0.199882861328125,0.1996092041015625,0.1993392333984375,0.199072802734375,0.19880986328125,0.19855030517578126,0.19829405517578125,0.19804105224609375,0.19779114990234375,0.1975443603515625,0.19730059814453124,0.1970597412109375,0.19682174072265626,0.1965865478515625,0.1963540771484375,0.1961242431640625,0.19589713134765624,0.19567249755859376,0.195450341796875,0.1952306884765625,0.19501339111328125,0.1947984619140625,0.1945857177734375,0.1943753662109375,0.1941671142578125,0.19396103515625,0.1937571044921875,0.193555224609375,0.1933553466796875,0.193157470703125,0.1929614990234375,0.19276744384765626,0.19257528076171876,0.1923849365234375,0.1921963623046875,0.19200958251953126,0.19182451171875,0.1916411376953125,0.19145947265625,0.19127939453125,0.19110091552734376,0.1909240234375,0.1907487060546875,0.19057490234375,0.1904025634765625,0.1902317138671875,0.190062255859375,0.189894287109375,0.1897276611328125,0.18956240234375,0.18939852294921874,0.18923592529296876,0.1890746337890625,0.1889146240234375,0.1887558837890625,0.1885984375,0.18844212646484376,0.188287060546875,0.18813310546875,0.18798037109375,0.187828759765625,0.187678271484375,0.18752890625,0.1873805908203125,0.18723341064453125,0.1870871826171875,0.1869420654296875,0.186797998046875,0.186654931640625,0.18651279296875,0.18637169189453126,0.1862315673828125,0.1860923095703125,0.1859540771484375,0.18581673583984376,0.1856803466796875,0.1855447998046875,0.1854101806640625,0.185276416015625,0.1851435546875,0.1850114501953125,0.1848802734375,0.18474984130859376,0.18462027587890625,0.184491552734375,0.18436357421875,0.1842364013671875,0.18410997314453126,0.1839843017578125,0.183859423828125,0.183735302734375,0.1836119140625,0.1834891845703125,0.1833672607421875,0.18324598388671876,0.1831253662109375,0.1830054931640625,0.182886328125,0.1827677734375,0.182649951171875,0.1825327392578125,0.182416162109375,0.18230025634765626,0.1821849853515625,0.1820703369140625,0.18195631103515625,0.1818428955078125,0.18173009033203125,0.18161787109375,0.18150621337890624,0.18139521484375,0.1812846923828125,0.18117479248046875,0.1810654541015625,0.18095673828125,0.180848486328125,0.18074080810546875,0.1806336669921875,0.1805270751953125,0.1804209716796875,0.18031546630859374,0.1802103759765625,0.18010584716796876,0.18000184326171875,0.17989833984375,0.1797952880859375,0.1796927490234375,0.17959071044921876,0.1794891357421875,0.179388037109375,0.17928740234375,0.17918724365234376,0.179087548828125,0.17898828125,0.17888951416015625,0.1787911865234375,0.17869327392578124,0.17859581298828126,0.178498779296875,0.1784021728515625,0.178306005859375,0.17821024169921876,0.17811488037109374,0.17801995849609376,0.177925439453125,0.177831298828125,0.1777375732421875,0.1776443115234375,0.1775513427734375,0.17745882568359375,0.1773667236328125,0.177274951171875,0.177183544921875,0.17709256591796874,0.177001904296875,0.1769116455078125,0.17682176513671874,0.1767322509765625,0.1766430908203125,0.17655423583984375,0.1764657958984375,0.1763776611328125,0.1762899169921875,0.1762024658203125,0.1761153564453125,0.1760286376953125,0.1759422119140625,0.17585618896484376,0.1757703857421875,0.175685009765625,0.17559986572265626,0.175515087890625,0.175430615234375,0.1753465087890625,0.1752626220703125,0.175179150390625,0.1750959228515625,0.17501302490234374,0.17493040771484375,0.17484810791015626,0.1747660888671875,0.174684375,0.1746029296875,0.174521826171875,0.174440966796875,0.17436041259765625,0.17428017578125,0.1742001708984375,0.1741204345703125,0.174041064453125,0.17396185302734374,0.173882958984375,0.1738043701171875,0.17372598876953124,0.1736479248046875,0.173570068359375,0.1734925537109375,0.1734152587890625,0.17333824462890626,0.1732614501953125,0.1731849609375,0.17310869140625,0.17303265380859376,0.172956884765625,0.17288135986328124,0.1728060791015625,0.1727310546875,0.1726562255859375,0.172581689453125,0.17250738525390624,0.17243328857421875,0.1723594970703125,0.17228585205078126,0.1722124755859375,0.1721393310546875,0.1720663818359375,0.1719936767578125,0.171921240234375,0.171848974609375,0.1717769287109375,0.1717051513671875,0.17163359375,0.17156220703125,0.171491015625,0.1714200927734375,0.171349365234375,0.17127886962890626,0.17120859375,0.17113843994140626,0.1710685791015625,0.1709989013671875,0.17092939453125,0.170860107421875,0.17079105224609376,0.17072216796875,0.1706534912109375,0.1705850341796875,0.17051673583984375,0.1704486572265625,0.17038076171875,0.170313037109375,0.170245556640625,0.1701781982421875,0.17011104736328125,0.1700441162109375,0.16997734375,0.169910791015625,0.1698444091796875,0.16977818603515624,0.1697121826171875,0.1696463134765625,0.1695806396484375,0.1695151611328125,0.16944984130859375,0.169384716796875,0.1693197265625,0.1692549560546875,0.16919036865234374,0.16912587890625,0.16906162109375,0.1689974853515625,0.16893359375,0.16886982421875,0.1688061767578125,0.1687427490234375,0.1686794921875,0.1686163818359375,0.1685534423828125,0.1684906494140625,0.16842802734375,0.1683655517578125,0.1683032470703125,0.16824111328125,0.16817911376953126,0.1681173095703125,0.168055615234375,0.1679940185546875,0.1679326904296875,0.16787147216796874,0.1678103759765625,0.16774947509765625,0.167688720703125,0.1676281005859375,0.167567578125,0.1675072998046875,0.16744708251953125,0.16738707275390624,0.16732713623046874,0.16726739501953125,0.16720780029296875,0.16714833984375,0.1670890380859375,0.16702987060546876,0.1669708251953125,0.1669119140625,0.16685316162109376,0.16679454345703126,0.16673602294921874,0.16667764892578124,0.1666194580078125,0.16656134033203124,0.16650340576171874,0.1664455810546875,0.1663878662109375,0.1663303466796875,0.16627291259765625,0.166215625,0.166158447265625,0.1661013916015625,0.1660445068359375,0.165987744140625,0.16593106689453124,0.165874560546875,0.16581815185546875,0.16576187744140625,0.165705712890625,0.16564970703125,0.1655937744140625,0.1655380126953125,0.16548233642578125,0.1654267822265625,0.1653713623046875,0.16531605224609375,0.1652608642578125,0.165205810546875,0.16515087890625,0.16509608154296876,0.165041357421875,0.1649867431640625,0.1649322509765625,0.16487786865234375,0.16482366943359375,0.1647694580078125,0.16471544189453124,0.1646615478515625,0.164607763671875,0.164554052734375,0.1645005126953125,0.1644470458984375,0.16439365234375,0.16434041748046874,0.164287255859375,0.164234228515625,0.1641813232421875,0.164128515625,0.164075830078125,0.164023193359375,0.163970703125,0.163918310546875,0.163866015625,0.1638138427734375,0.16376171875,0.16370975341796876,0.1636579345703125,0.16360609130859374,0.16355440673828125,0.1635028564453125,0.1634514404296875,0.1634000244140625,0.16334881591796874,0.16329764404296876,0.16324654541015626,0.163195556640625,0.163144677734375,0.1630939208984375,0.16304324951171875,0.16299267578125,0.1629421875,0.162891796875,0.16284150390625,0.1627913330078125,0.1627412353515625,0.16269117431640626,0.162641259765625,0.162591455078125,0.16254171142578125,0.16249208984375,0.16244251708984375,0.1623930908203125,0.16234368896484375,0.16229442138671876,0.162245263671875,0.16219615478515625,0.16214715576171876,0.16209825439453124,0.1620493896484375,0.1620007080078125,0.16195203857421875,0.16190347900390625,0.16185498046875,0.16180660400390626,0.16175831298828125,0.161710107421875,0.161661962890625,0.1616139892578125,0.161565966796875,0.161518115234375,0.1614703125,0.161422607421875,0.1613749755859375,0.16132744140625,0.1612800048828125,0.1612325927734375,0.16118533935546875,0.161138134765625,0.161091015625,0.16104395751953124,0.16099698486328126,0.16095009765625,0.16090330810546874,0.1608565673828125,0.1608099365234375,0.1607633544921875,0.1607168701171875,0.1606704833984375,0.160624169921875,0.160577880859375,0.16053173828125,0.16048564453125,0.160439599609375,0.160393701171875,0.1603478271484375,0.16030205078125,0.160256298828125,0.16021064453125,0.16016510009765625,0.16011959228515624,0.16007421875,0.160028857421875,0.15998355712890625,0.1599384033203125,0.1598932861328125,0.1598482421875,0.159803271484375,0.1597583740234375,0.15971353759765625,0.15966878662109374,0.1596240966796875,0.15957950439453125,0.1595349609375,0.159490478515625,0.1594460693359375,0.15940174560546874,0.159357470703125,0.15931329345703124,0.1592692138671875,0.1592251220703125,0.1591811767578125,0.15913721923828125,0.15909339599609376,0.15904962158203126,0.159005908203125,0.1589622802734375,0.15891868896484376,0.1588752197265625,0.1588317626953125,0.1587884033203125,0.1587450927734375,0.1587018310546875,0.1586586669921875,0.15861556396484375,0.15857255859375,0.15852957763671874,0.15848662109375,0.15844378662109376,0.1584010009765625,0.15835828857421874,0.158315576171875,0.15827305908203124,0.158230517578125,0.1581880615234375,0.1581456298828125,0.15810330810546874,0.1580610595703125,0.1580188232421875,0.15797664794921876,0.157934619140625,0.1578926025390625,0.1578506103515625,0.15780872802734375,0.15776685791015624,0.1577250732421875,0.15768336181640624,0.15764171142578126,0.1576001220703125,0.1575585693359375,0.15751707763671874,0.15747568359375,0.15743431396484375,0.157393017578125,0.1573517822265625,0.15731058349609375,0.1572694580078125,0.15722841796875,0.1571873779296875,0.1571464599609375,0.1571055908203125,0.15706475830078126,0.1570239501953125,0.156983251953125,0.156942578125,0.156902001953125,0.15686141357421876,0.1568209228515625,0.15678050537109375,0.15674012451171876,0.1566998291015625,0.1566595458984375,0.1566193359375,0.15657921142578124,0.15653907470703124,0.1564989990234375,0.156459033203125,0.1564191162109375,0.15637923583984376,0.15633939208984374,0.156299658203125,0.1562599365234375,0.15622025146484375,0.1561806640625,0.15614107666015625,0.15610159912109375,0.1560621337890625,0.15602271728515624,0.1559833984375,0.1559441162109375,0.15590484619140624,0.155865673828125,0.155826513671875,0.15578743896484376,0.15574837646484374,0.1557094482421875,0.1556705322265625,0.1556316650390625,0.1555927734375,0.15555404052734376,0.15551529541015624,0.1554766357421875,0.15543804931640626,0.1553994140625,0.15536092529296874,0.1553224365234375,0.1552840087890625,0.1552456298828125,0.1552072998046875,0.15516904296875,0.1551308349609375,0.155092626953125,0.15505452880859374,0.1550164306640625,0.1549783935546875,0.1549404296875,0.15490250244140624,0.15486458740234374,0.1548267333984375,0.15478896484375,0.15475123291015624,0.1547135498046875,0.15467589111328126,0.1546383056640625,0.1546007568359375,0.15456324462890625,0.1545258056640625,0.1544883544921875,0.1544510009765625,0.154413671875,0.154376416015625,0.15433917236328126,0.15430203857421876,0.1542649169921875,0.15422779541015624,0.154190771484375,0.1541537841796875,0.1541167724609375,0.15407991943359375,0.1540430419921875,0.1540062255859375,0.15396943359375,0.15393271484375,0.15389603271484376,0.153859423828125,0.15382279052734374,0.15378626708984375,0.1537497802734375,0.15371331787109374,0.1536768798828125,0.1536405517578125,0.1536041748046875,0.15356788330078125,0.15353165283203124,0.15349547119140625,0.1534592529296875,0.1534232177734375,0.15338709716796875,0.15335108642578124,0.153315087890625,0.153279150390625,0.15324326171875,0.1532073974609375,0.15317156982421876,0.15313582763671876,0.153100048828125,0.15306439208984374,0.15302874755859375,0.1529931396484375,0.15295755615234374,0.15292203369140625,0.1528865966796875,0.1528510986328125,0.15281568603515625,0.152780322265625,0.15274501953125,0.15270975341796875,0.15267449951171874,0.152639306640625,0.15260416259765625,0.15256904296875,0.15253397216796874,0.15249896240234376,0.152463916015625,0.152428955078125,0.15239404296875,0.1523591796875,0.15232430419921875,0.1522895263671875,0.1522547607421875,0.1522199951171875,0.1521853759765625,0.1521507080078125,0.15211614990234376,0.15208154296875,0.15204700927734374,0.1520125244140625,0.1519781005859375,0.1519436767578125,0.15190927734375,0.15187498779296876,0.15184068603515624,0.15180640869140624,0.151772216796875,0.1517380126953125,0.151703857421875,0.1516697265625,0.15163565673828125,0.15160157470703126,0.15156763916015625,0.15153367919921876,0.15149974365234375,0.1514658447265625,0.15143203125,0.1513982177734375,0.151364404296875,0.15133070068359375,0.1512969970703125,0.15126331787109376,0.1512296630859375,0.15119610595703126,0.151162548828125,0.1511290283203125,0.15109552001953125,0.15106207275390626,0.15102861328125,0.150995263671875,0.1509619384765625,0.15092861328125,0.150895361328125,0.1508620849609375,0.1508289306640625,0.15079576416015625,0.1507626220703125,0.1507295166015625,0.15069642333984376,0.1506634033203125,0.150630419921875,0.15059747314453126,0.1505645263671875,0.1505316650390625,0.1504988037109375,0.1504659423828125,0.15043319091796875,0.150400439453125,0.1503677490234375,0.15033505859375,0.15030240478515625,0.15026978759765625,0.15023717041015625,0.150204638671875,0.150172119140625,0.15013966064453124,0.1501072021484375,0.15007479248046876,0.15004241943359375,0.1500100341796875,0.1499777099609375,0.14994542236328126,0.14991314697265626,0.14988095703125,0.1498487548828125,0.1498166015625,0.14978447265625,0.1497524169921875,0.1497203369140625,0.149688330078125,0.1496563232421875,0.149624365234375,0.1495924560546875,0.149560546875,0.149528662109375,0.14949681396484374,0.1494650390625,0.149433251953125,0.14940152587890626,0.1493698486328125,0.1493381591796875,0.1493065185546875,0.14927490234375,0.149243359375,0.14921175537109374,0.14918026123046876,0.1491487548828125,0.1491173095703125,0.149085888671875,0.14905450439453125,0.14902314453125,0.1489918212890625,0.148960498046875,0.148929248046875,0.148897998046875,0.14886678466796874,0.14883564453125,0.1488044677734375,0.14877333984375,0.148742236328125,0.1487112060546875,0.14868013916015624,0.148649169921875,0.14861817626953125,0.14858724365234374,0.1485563232421875,0.14852545166015624,0.148494580078125,0.1484637939453125,0.1484329833984375,0.14840224609375,0.148371484375,0.14834075927734375,0.1483101318359375,0.1482794677734375,0.148248876953125,0.14821824951171875,0.1481877197265625,0.148157177734375,0.14812667236328125,0.1480962158203125,0.1480657470703125,0.14803531494140626,0.1480049560546875,0.14797462158203126,0.147944287109375,0.14791397705078124,0.14788369140625,0.147853466796875,0.1478232421875,0.14779307861328125,0.147762890625,0.14773275146484374,0.14770267333984374,0.1476725830078125,0.1476425048828125,0.1476124755859375,0.14758253173828126,0.1475525634765625,0.1475226318359375,0.14749268798828125,0.147462841796875,0.147432958984375,0.1474031494140625,0.1473733642578125,0.147343603515625,0.147313818359375,0.147284130859375,0.14725440673828125,0.1472247314453125,0.1471951171875,0.14716549072265625,0.147135888671875,0.14710634765625,0.1470768310546875,0.1470473388671875,0.1470178466796875,0.14698837890625,0.146958984375,0.1469295654296875,0.1469002197265625,0.146870849609375,0.1468415283203125,0.14681220703125,0.1467829833984375,0.14675374755859374,0.1467244873046875,0.1466953369140625,0.14666612548828126,0.1466370361328125,0.14660791015625,0.14657880859375,0.14654974365234374,0.146520751953125,0.1464917236328125,0.1464627197265625,0.1464337890625,0.1464048583984375,0.1463759521484375,0.1463470703125,0.146318212890625,0.14628939208984376,0.1462606201171875,0.1462318359375,0.14620302734375,0.1461742919921875,0.1461456298828125,0.14611693115234375,0.14608828125,0.14605966796875,0.14603104248046875,0.14600244140625,0.145973876953125,0.1459453857421875,0.145916845703125,0.1458884033203125,0.145859912109375,0.145831494140625,0.145803076171875,0.1457747314453125,0.1457463623046875,0.1457180419921875,0.145689697265625,0.14566141357421875,0.1456331298828125,0.14560491943359374,0.14557669677734375,0.1455485107421875,0.14552034912109374,0.1454921630859375,0.1454640625,0.1454359619140625,0.1454078857421875,0.145379833984375,0.145351806640625,0.145323828125,0.14529583740234375,0.1452679443359375,0.14524000244140625,0.145212060546875,0.14518416748046875,0.14515634765625,0.145128466796875,0.14510069580078125,0.1450729248046875,0.1450450927734375,0.1450173583984375,0.14498966064453125,0.1449619384765625,0.14493427734375,0.1449066162109375,0.14487899169921875,0.14485140380859374,0.14482379150390626,0.14479622802734374,0.14476868896484374,0.1447411865234375,0.1447136474609375,0.1446862060546875,0.1446587890625,0.14463135986328124,0.1446039306640625,0.1445765625,0.14454921875,0.1445218505859375,0.1444945556640625,0.14446728515625,0.14443997802734376,0.144412744140625,0.1443855224609375,0.14435828857421876,0.1443311767578125,0.14430400390625,0.144276904296875,0.14424971923828125,0.14422264404296875,0.1441955810546875,0.1441685302734375,0.14414151611328124,0.1441144775390625,0.1440875244140625,0.1440605224609375,0.14403359375,0.144006689453125,0.14397978515625,0.1439529052734375,0.1439260498046875,0.14389921875,0.1438724365234375,0.1438455810546875,0.1438188232421875,0.1437920654296875,0.1437653564453125,0.1437386474609375,0.1437119873046875,0.143685302734375,0.14365865478515624,0.14363203125,0.14360543212890625,0.14357886962890626,0.143552294921875,0.1435257568359375,0.14349925537109376,0.14347275390625,0.14344625244140624,0.14341983642578124,0.14339337158203125,0.1433669921875,0.143340576171875,0.143314208984375,0.143287890625,0.143261474609375,0.1432351806640625,0.14320889892578126,0.1431826171875,0.14315638427734376,0.14313016357421876,0.143103955078125,0.14307777099609376,0.1430515869140625,0.143025439453125,0.14299931640625,0.1429732421875,0.14294710693359375,0.14292105712890624,0.14289501953125,0.1428689697265625,0.14284296875,0.1428169677734375,0.1427909912109375,0.1427650634765625,0.1427391357421875,0.14271322021484376,0.1426873291015625,0.14266143798828124,0.14263563232421875,0.1426097900390625,0.142583984375,0.1425581787109375,0.142532421875,0.14250665283203126,0.1424809326171875,0.14245518798828125,0.1424294677734375,0.14240382080078126,0.142378125,0.1423525146484375,0.14232689208984375,0.142301318359375,0.1422757080078125,0.14225015869140625,0.1422246337890625,0.14219913330078124,0.1421736083984375,0.14214813232421875,0.1421226318359375,0.14209720458984376,0.14207177734375,0.14204635009765626,0.1420209716796875,0.14199560546875,0.14197022705078124,0.1419449462890625,0.1419196044921875,0.14189432373046876,0.1418690185546875,0.141843798828125,0.141818505859375,0.14179332275390624,0.1417680908203125,0.1417428955078125,0.1417177490234375,0.1416926025390625,0.14166748046875,0.1416423583984375,0.1416172607421875,0.1415921875,0.14156715087890626,0.14154208984375,0.1415170654296875,0.141492041015625,0.14146708984375,0.1414421142578125,0.1414171875,0.141392236328125,0.141367333984375,0.141342431640625,0.1413175537109375,0.1412927001953125,0.14126787109375,0.14124305419921876,0.14121822509765625,0.1411934326171875,0.14116866455078125,0.1411439697265625,0.1411192138671875,0.1410945068359375,0.14106978759765626,0.1410450927734375,0.14102041015625,0.1409957763671875,0.14097115478515626,0.1409465576171875,0.14092197265625,0.14089739990234376,0.1408728271484375,0.1408483154296875,0.14082376708984376,0.1407992431640625,0.1407748046875,0.14075029296875,0.14072587890625,0.140701416015625,0.14067701416015624,0.1406525634765625,0.14062821044921875,0.1406038818359375,0.14057950439453126,0.1405551513671875,0.140530859375,0.14050654296875,0.14048228759765624,0.1404580078125,0.14043377685546876,0.14040950927734375,0.14038529052734375,0.1403611083984375,0.1403369384765625,0.140312744140625,0.1402885986328125,0.1402644775390625,0.14024039306640626,0.140216259765625,0.1401921875,0.14016812744140625,0.14014407958984376,0.14012005615234374,0.140096044921875,0.140072021484375,0.1400480712890625,0.14002408447265624,0.140000146484375,0.13997623291015626,0.139952294921875,0.13992840576171875,0.1399045166015625,0.13988062744140625,0.13985679931640624,0.1398330078125,0.139809130859375,0.1397853515625,0.1397615478515625,0.1397377685546875,0.1397140380859375,0.1396903076171875,0.1396666015625,0.13964287109375,0.139619189453125,0.139595556640625,0.139571875,0.1395482177734375,0.13952459716796875,0.1395009765625,0.13947742919921874,0.139453857421875,0.1394302978515625,0.13940672607421875,0.139383203125,0.13935970458984376,0.1393362060546875,0.1393126953125,0.13928924560546874,0.139265771484375,0.13924234619140624,0.1392189453125,0.13919552001953125,0.13917216796875,0.1391487548828125,0.1391254150390625,0.1391020751953125,0.139078759765625,0.1390554443359375,0.13903212890625,0.13900887451171876,0.1389856201171875,0.138962353515625,0.1389391357421875,0.1389158935546875,0.13889271240234374,0.13886951904296874,0.138846337890625,0.13882315673828124,0.13880006103515624,0.13877689208984376,0.13875380859375,0.13873070068359375,0.13870762939453124,0.1386845458984375,0.13866151123046874,0.13863848876953125,0.138615478515625,0.1385924560546875,0.1385694580078125,0.13854649658203125,0.13852352294921874,0.13850059814453125,0.1384776611328125,0.1384547119140625,0.13843182373046875,0.138408984375,0.1383860595703125,0.1383632080078125,0.138340380859375,0.1383175537109375,0.13829473876953124,0.138271923828125,0.1382491455078125,0.1382263671875,0.13820361328125,0.1381808837890625,0.1381581787109375,0.13813544921875,0.13811279296875,0.13809010009765624,0.138067431640625,0.13804478759765626,0.1380221435546875,0.13799949951171875,0.137976904296875,0.137954296875,0.13793170166015625,0.137909130859375,0.13788658447265625,0.1378640625,0.1378415771484375,0.13781904296875,0.13779654541015626,0.1377740478515625,0.13775159912109375,0.137729150390625,0.13770670166015625,0.1376843017578125,0.1376618896484375,0.1376394775390625,0.13761708984375,0.1375947509765625,0.137572412109375,0.1375500732421875,0.137527734375,0.1375053955078125,0.13748311767578125,0.13746083984375,0.13743856201171875,0.1374162841796875,0.13739404296875,0.1373718505859375,0.13734962158203126,0.13732740478515626,0.1373052490234375,0.13728306884765626,0.13726094970703126,0.13723878173828125,0.137216650390625,0.13719453125,0.1371724365234375,0.13715032958984374,0.1371282470703125,0.137106201171875,0.1370841796875,0.137062109375,0.1370401123046875,0.1370180908203125,0.1369961181640625,0.13697410888671874,0.13695216064453125,0.13693018798828124,0.1369082763671875,0.1368863525390625,0.136864453125,0.136842578125,0.1368206298828125,0.13679876708984376,0.13677689208984375,0.136755078125,0.1367332275390625,0.13671143798828125,0.136689599609375,0.136667822265625,0.136646044921875,0.13662427978515626,0.1366025146484375,0.13658076171875,0.1365590576171875,0.13653736572265626,0.13651563720703125,0.13649393310546876,0.13647227783203125,0.1364505859375,0.136428955078125,0.13640728759765625,0.1363857177734375,0.13636409912109376,0.1363424560546875,0.13632093505859375,0.1362993408203125,0.1362778076171875,0.1362562255859375,0.13623470458984374,0.1362132080078125,0.13619166259765625,0.13617021484375,0.1361487060546875,0.13612724609375,0.13610576171875,0.13608436279296876,0.13606292724609376,0.1360415283203125,0.13602010498046874,0.13599873046875,0.1359773193359375,0.13595595703125,0.13593460693359374,0.13591328125,0.135891943359375,0.135870654296875,0.13584932861328125,0.135828076171875,0.1358067626953125,0.1357855224609375,0.1357642333984375,0.13574300537109374,0.13572178955078126,0.1357005615234375,0.13567935791015626,0.1356581787109375,0.13563701171875,0.1356158447265625,0.135594677734375,0.1355735595703125,0.1355524169921875,0.13553133544921875,0.135510205078125,0.13548912353515624,0.13546806640625,0.13544697265625,0.135425927734375,0.1354048583984375,0.1353838623046875,0.13536285400390624,0.1353418701171875,0.13532083740234374,0.1352998779296875,0.13527890625,0.13525794677734376,0.1352369873046875,0.135216064453125,0.13519515380859376,0.1351742431640625,0.13515335693359376,0.13513245849609376,0.135111572265625,0.135090771484375,0.13506990966796875,0.1350490478515625,0.1350281982421875,0.135007373046875,0.13498660888671876,0.1349657958984375,0.1349450439453125,0.13492427978515625,0.1349035400390625,0.13488277587890626,0.1348620361328125,0.1348413330078125,0.134820654296875,0.13479993896484374,0.134779296875,0.13475860595703126,0.13473795166015626,0.13471729736328125,0.1346966552734375,0.134676025390625,0.13465543212890624,0.134634814453125,0.13461422119140626,0.13459365234375,0.13457308349609376,0.1345525146484375,0.13453199462890625,0.134511474609375,0.134490966796875,0.1344704345703125,0.1344499267578125,0.1344294677734375,0.13440899658203126,0.1343885498046875,0.13436806640625,0.13434765625,0.13432724609375,0.1343068115234375,0.13428638916015626,0.13426605224609375,0.1342456298828125,0.134225244140625,0.13420491943359375,0.1341845703125,0.134164208984375,0.13414390869140624,0.13412359619140626,0.134103271484375,0.13408299560546874,0.1340627197265625,0.13404248046875,0.134022216796875,0.13400194091796874,0.13398170166015624,0.13396148681640624,0.1339412841796875,0.1339211181640625,0.1339009033203125,0.13388070068359376,0.133860546875,0.13384039306640624,0.1338202392578125,0.1338001220703125,0.13378004150390624,0.1337598876953125,0.1337398193359375,0.1337197021484375,0.1336995849609375,0.13367952880859374,0.1336594970703125,0.133639404296875,0.13361942138671876,0.13359940185546876,0.13357939453125,0.133559375,0.1335393798828125,0.1335194091796875,0.1334994140625,0.1334794677734375,0.1334595703125,0.1334396240234375,0.13341966552734374,0.1333997314453125,0.13337984619140625,0.1333599609375,0.1333400634765625,0.1333201904296875,0.133300341796875,0.13328048095703124,0.13326063232421875,0.1332408203125,0.13322100830078126,0.1332011962890625,0.133181396484375,0.1331615966796875,0.1331418701171875,0.1331220703125,0.1331023193359375,0.133082568359375,0.1330628173828125,0.133043115234375,0.13302340087890624,0.13300367431640625,0.13298399658203125,0.1329643310546875,0.13294462890625,0.13292501220703126,0.13290535888671876,0.13288570556640625,0.132866064453125,0.13284647216796874,0.1328268798828125,0.132807275390625,0.1327876708984375,0.13276810302734374,0.13274852294921874,0.1327289794921875,0.13270943603515625,0.1326899169921875,0.13267039794921875,0.1326508544921875,0.1326313720703125,0.13261182861328125,0.1325923828125,0.1325729248046875,0.13255345458984374,0.13253399658203124,0.1325145751953125,0.1324950927734375,0.13247574462890624,0.132456298828125,0.1324369140625,0.13241749267578126,0.13239810791015624,0.1323787353515625,0.132359375,0.13234002685546875,0.1323206787109375,0.1323013427734375,0.13228201904296874,0.1322627197265625,0.1322434326171875,0.132224169921875,0.132204833984375,0.13218558349609374,0.13216630859375,0.1321470947265625,0.13212784423828125,0.132108642578125,0.1320893798828125,0.132070166015625,0.1320509765625,0.1320318115234375,0.13201263427734375,0.13199344482421874,0.13197427978515625,0.131955126953125,0.131935986328125,0.13191685791015625,0.131897705078125,0.1318785888671875,0.131859521484375,0.1318404296875,0.131821337890625,0.13180224609375,0.131783203125,0.13176416015625,0.1317451171875,0.13172607421875,0.13170706787109376,0.1316880126953125,0.1316690673828125,0.1316500244140625,0.1316310546875,0.131612060546875,0.131593115234375,0.131574169921875,0.13155518798828125,0.1315362548828125,0.131517333984375,0.13149844970703126,0.1314795166015625,0.13146063232421876,0.131441748046875,0.1314228515625,0.13140400390625,0.13138511962890626,0.1313662841796875,0.131347412109375,0.13132860107421876,0.1313097900390625,0.13129100341796876,0.13127218017578124,0.131253369140625,0.1312345947265625,0.1312158203125,0.1311970703125,0.131178271484375,0.1311595458984375,0.1311408447265625,0.1311220947265625,0.13110338134765626,0.13108466796875,0.1310659912109375,0.13104727783203124,0.131028564453125,0.13100992431640626,0.130991259765625,0.1309726318359375,0.13095400390625,0.13093533935546875,0.130916748046875,0.13089810791015624,0.1308794921875,0.13086090087890626,0.1308423095703125,0.13082374267578126,0.13080516357421876,0.13078660888671875,0.1307680419921875,0.13074951171875,0.13073095703125,0.13071243896484375,0.13069395751953125,0.130675439453125,0.13065692138671875,0.13063843994140625,0.1306199462890625,0.13060147705078126,0.13058304443359375,0.130564599609375,0.130546142578125,0.1305277099609375,0.130509326171875,0.13049090576171876,0.1304724853515625,0.1304541015625,0.1304357177734375,0.13041729736328125,0.13039893798828125,0.1303806396484375,0.130362255859375,0.130343896484375,0.1303255615234375,0.1303072265625,0.13028890380859376,0.13027061767578124,0.1302523193359375,0.13023402099609374,0.13021575927734375,0.1301974853515625,0.13017918701171874,0.1301609619140625,0.13014273681640626,0.1301244873046875,0.13010625,0.130088037109375,0.13006983642578124,0.13005164794921875,0.130033447265625,0.1300152587890625,0.1299970947265625,0.1299788818359375,0.1299607666015625,0.129942626953125,0.12992447509765626,0.1299063232421875,0.12988824462890625,0.1298701171875,0.1298520263671875,0.1298339111328125,0.12981583251953124,0.12979775390625,0.1297796630859375,0.1297616455078125,0.1297435791015625,0.12972552490234374,0.129707470703125,0.12968946533203124,0.1296714599609375,0.1296534423828125,0.12963544921875,0.1296174072265625,0.129599462890625,0.129581494140625,0.12956353759765624,0.129545556640625,0.1295276123046875,0.12950966796875,0.1294917724609375,0.1294738037109375,0.1294559326171875,0.1294380126953125,0.1294201171875,0.12940223388671876,0.129384326171875,0.12936646728515624,0.1293486083984375,0.12933074951171875,0.1293129150390625,0.129295068359375,0.1292772216796875,0.1292593994140625,0.1292416259765625,0.1292237548828125,0.1292059814453125,0.12918822021484375,0.12917039794921875,0.12915267333984376,0.12913487548828126,0.129117138671875,0.1290993896484375,0.129081640625,0.12906390380859376,0.12904617919921876,0.12902847900390624,0.1290107666015625,0.12899306640625,0.12897535400390625,0.12895770263671874,0.1289400146484375,0.12892235107421876,0.1289047119140625,0.128887060546875,0.1288694091796875,0.128851806640625,0.1288341796875,0.128816552734375,0.12879896240234376,0.12878134765625,0.1287637451171875,0.1287461669921875,0.1287285888671875,0.1287110595703125,0.12869345703125,0.12867593994140625,0.1286583984375,0.128640869140625,0.1286233154296875,0.1286057861328125,0.1285883056640625,0.12857078857421875,0.1285533203125,0.1285358154296875,0.12851837158203125,0.12850089111328125,0.1284834228515625,0.128465966796875,0.12844853515625,0.12843104248046874,0.1284136474609375,0.1283962158203125,0.12837880859375,0.1283614013671875,0.1283440185546875,0.128326611328125,0.12830924072265626,0.12829183349609374,0.12827447509765624,0.1282571533203125,0.128239794921875,0.1282224365234375,0.1282051025390625,0.12818778076171874,0.12817047119140626,0.1281531494140625,0.12813583984375,0.12811854248046875,0.12810126953125,0.128083984375,0.1280667236328125,0.1280494873046875,0.128032177734375,0.12801494140625,0.1279977294921875,0.12798048095703124,0.1279632568359375,0.1279460693359375,0.12792880859375,0.12791158447265624,0.12789442138671875,0.12787724609375,0.1278600830078125,0.127842919921875,0.127825732421875,0.12780858154296876,0.127791455078125,0.12777427978515624,0.12775716552734376,0.1277400390625,0.12772294921875,0.127705810546875,0.12768870849609376,0.12767161865234375,0.1276545166015625,0.1276374267578125,0.1276203857421875,0.12760333251953124,0.1275862548828125,0.1275692138671875,0.12755218505859375,0.1275351806640625,0.127518115234375,0.127501123046875,0.12748406982421875,0.12746707763671875,0.12745006103515624,0.12743311767578125,0.12741612548828124,0.127399169921875,0.127382177734375,0.12736522216796875,0.12734830322265625,0.12733134765625,0.12731439208984374,0.1272974853515625,0.12728055419921874,0.12726363525390624,0.1272467529296875,0.127229833984375,0.12721295166015625,0.1271960693359375,0.12717919921875,0.12716234130859375,0.127145458984375,0.12712861328125,0.1271117431640625,0.12709488525390625,0.127078076171875,0.12706124267578126,0.1270444580078125,0.12702760009765626,0.1270108642578125,0.1269940185546875,0.126977294921875,0.12696046142578124,0.1269436767578125,0.12692694091796874,0.1269101806640625,0.1268934326171875,0.12687669677734376,0.12685994873046874,0.126843212890625,0.126826513671875,0.126809814453125,0.1267930908203125,0.1267763427734375,0.12675970458984376,0.12674300537109376,0.1267263427734375,0.12670963134765625,0.12669302978515626,0.12667635498046875,0.12665968017578125,0.1266430419921875,0.126626416015625,0.1266097900390625,0.1265931640625,0.1265765380859375,0.1265599365234375,0.1265433349609375,0.12652677001953125,0.12651016845703125,0.126493603515625,0.12647703857421874,0.1264604248046875,0.1264439208984375,0.12642733154296876,0.1264108154296875,0.12639427490234376,0.12637774658203124,0.12636126708984374,0.12634468994140624,0.1263281982421875,0.1263116943359375,0.12629521484375,0.1262787109375,0.126262255859375,0.1262458251953125,0.126229296875,0.12621287841796874,0.1261964111328125,0.12617996826171876,0.126163525390625,0.1261470947265625,0.1261306640625,0.1261142578125,0.12609783935546875,0.1260814453125,0.12606505126953124,0.126048681640625,0.126032275390625,0.12601591796875,0.125999560546875,0.1259831787109375,0.125966845703125,0.1259505126953125,0.1259341552734375,0.125917822265625,0.1259014892578125,0.125885205078125,0.125868896484375,0.12585255126953124,0.12583624267578125,0.125819970703125,0.1258036865234375,0.12578741455078124,0.12577113037109375,0.12575489501953124,0.125738623046875,0.12572237548828125,0.125706103515625,0.125689892578125,0.1256736572265625,0.12565743408203126,0.12564122314453124,0.125625,0.1256088134765625,0.12559259033203124,0.125576416015625,0.12556024169921876,0.1255440673828125,0.1255279052734375,0.1255117431640625,0.12549556884765625,0.12547943115234375,0.12546329345703125,0.12544716796875,0.1254310302734375,0.1254149169921875,0.125398779296875,0.1253826904296875,0.12536656494140624,0.12535048828125,0.1253343994140625,0.125318310546875,0.12530223388671874,0.1252861572265625,0.1252701171875,0.125254052734375,0.12523798828125,0.1252219482421875,0.1252059326171875,0.1251898681640625,0.1251739013671875,0.125157861328125,0.125141845703125,0.125125830078125,0.12510985107421874,0.12509385986328125,0.125077880859375,0.1250618896484375,0.1250459228515625,0.12502996826171875,0.125014013671875,0.124998046875,0.12498212890625,0.12496619873046876,0.124950244140625,0.124934326171875,0.1249184326171875,0.1249025146484375,0.1248865966796875,0.12487069091796875,0.1248548095703125,0.12483895263671875,0.1248230712890625,0.1248072021484375,0.12479130859375,0.12477548828125,0.12475960693359375,0.12474375,0.12472791748046876,0.12471207275390625,0.12469627685546875,0.12468046875,0.1246646240234375,0.1246488037109375,0.12463299560546875,0.1246172119140625,0.124601416015625,0.12458564453125,0.12456988525390625,0.1245541259765625,0.1245383056640625,0.1245225830078125,0.12450684814453125,0.12449107666015626,0.1244753662109375,0.12445965576171875,0.12444388427734375,0.12442818603515625,0.1244124755859375,0.12439676513671875,0.1243810791015625,0.1243653564453125,0.1243496826171875,0.12433399658203124,0.12431832275390625,0.12430264892578125,0.12428699951171875,0.1242713134765625,0.1242556884765625,0.1242400390625,0.1242244384765625,0.1242087646484375,0.1241931640625,0.12417752685546875,0.12416192626953125,0.1241463134765625,0.12413072509765626,0.1241151123046875,0.124099560546875,0.1240839599609375,0.124068359375,0.1240528076171875,0.124037255859375,0.1240217041015625,0.12400616455078126,0.123990576171875,0.1239750732421875,0.1239595458984375,0.12394404296875,0.1239284912109375,0.12391298828125,0.1238974853515625,0.1238820068359375,0.1238664794921875,0.1238510009765625,0.12383551025390625,0.1238200439453125,0.1238045654296875,0.1237890869140625,0.12377364501953125,0.12375819091796875,0.12374273681640625,0.12372730712890626,0.12371187744140626,0.1236964599609375,0.12368104248046875,0.1236656005859375,0.12365018310546876,0.12363477783203125,0.12361939697265625,0.12360400390625,0.12358861083984375,0.1235732666015625,0.1235578857421875,0.12354251708984375,0.1235271484375,0.123511767578125,0.123496435546875,0.1234811279296875,0.12346575927734375,0.1234504150390625,0.12343509521484375,0.123419775390625,0.1234044677734375,0.12338916015625,0.12337384033203125,0.12335855712890625,0.1233432861328125,0.1233280029296875,0.1233127197265625,0.1232974365234375,0.12328216552734375]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss by time\"};\n",
       "\n",
       "  Plotly.plot('plot-922967534', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mlossSeq\u001b[39m: \u001b[32mIndexedSeq\u001b[39m[\u001b[32mcmd11Wrapper\u001b[39m.\u001b[32m<refinement>\u001b[39m.this.type.\u001b[32mOutputData\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m0.2304002685546875\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mplot\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mScatter\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mScatter\u001b[39m(\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mres14_2\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-922967534\"\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lossSeq = for (iteration <- 0 until 2000) yield {\n",
    "  val loss = lossFunction.train(trainData :: vectorizedTrainExpectResult :: HNil)\n",
    "  if(iteration % 100 == 0){\n",
    "    println(s\"at iteration $iteration loss is $loss\")\n",
    "  }\n",
    "  loss\n",
    "}\n",
    "\n",
    "val plot = Seq(\n",
    "  Scatter(lossSeq.indices, lossSeq)\n",
    ")\n",
    "\n",
    "plot.plot(\n",
    "  title = \"loss by time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证神经网络预测准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用已经处理好的测试数据来验证神经网络的准确率。准确率应该在32%左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result is 32.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mright\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m32.0\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val right = Utils.getAccuracy(myNeuralNetwork.predict(testData), testExpectResult)\n",
    "println(s\"the result is $right %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "在这节中我们学到了：\n",
    "\n",
    "* 准备和处理CIFAR10数据\n",
    "* 编写softmax分类器\n",
    "* 使用softmax分类器编写的神经网络预测图片对应于每个分类的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[完整代码](https://github.com/izhangzhihao/deeplearning-tutorial/blob/master/src/main/scala/com/thoughtworks/deeplearning/tutorial/SoftmaxLinearClassifier.scala)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
