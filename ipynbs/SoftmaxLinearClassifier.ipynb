{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景\n",
    "\n",
    "这篇文章我们将使用[softmax](https://en.wikipedia.org/wiki/Softmax_function)分类器一起来构建一个简单的图像分类神经网络，其准确率可以达到32%。Softmax分类器是二元逻辑回归泛化到多元的情况。Softmax分类器会输出对应类别的概率。\n",
    "\n",
    "我们会先定义一个softmax分类器，然后使用[CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html)的训练集来训练这个神经网络。最后使用测试集来验证神经网络的准确率。\n",
    "\n",
    "我们开始吧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入依赖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似前一篇教程[GettingStarted](https://thoughtworksinc.github.io/DeepLearning.scala/demo/GettingStarted.html)，我们需要引入DeepLearning.scala的各个类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                 \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.{FileInputStream, InputStream}\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableHList._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableDouble._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableAny._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.{\n",
       "  DifferentiableHList,\n",
       "  DifferentiableINDArray,\n",
       "  Layer,\n",
       "  Symbolic\n",
       "}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Layer.Tape\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Symbolic._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathFunctions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathMethods./\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.Poly.MathOps\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.cpu.nativecpu.NDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.ops.transforms.Transforms\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4s.Implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mshapeless._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.immutable.IndexedSeq\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $plugin.$ivy.`com.thoughtworks.implicit-dependent-type::implicit-dependent-type:2.0.0`\n",
    "\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableany:1.0.0-RC8`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablenothing:1.0.0-RC8`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableseq:1.0.0-RC8`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiabledouble:1.0.0-RC8`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablefloat:1.0.0-RC8`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablehlist:1.0.0-RC8`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiablecoproduct:1.0.0-RC8`\n",
    "import $ivy.`com.thoughtworks.deeplearning::differentiableindarray:1.0.0-RC8`\n",
    "import $ivy.`org.nd4j:nd4j-native-platform:0.8.0`\n",
    "import $ivy.`org.rauschig:jarchivelib:0.5.0`\n",
    "\n",
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.0`\n",
    "\n",
    "import java.io.{FileInputStream, InputStream}\n",
    "\n",
    "\n",
    "import com.thoughtworks.deeplearning\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import com.thoughtworks.deeplearning.DifferentiableHList._\n",
    "import com.thoughtworks.deeplearning.DifferentiableDouble._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray._\n",
    "import com.thoughtworks.deeplearning.DifferentiableAny._\n",
    "import com.thoughtworks.deeplearning.DifferentiableINDArray.Optimizers._\n",
    "import com.thoughtworks.deeplearning.{\n",
    "  DifferentiableHList,\n",
    "  DifferentiableINDArray,\n",
    "  Layer,\n",
    "  Symbolic\n",
    "}\n",
    "import com.thoughtworks.deeplearning.Layer.Tape\n",
    "import com.thoughtworks.deeplearning.Symbolic.Layers.Identity\n",
    "import com.thoughtworks.deeplearning.Symbolic._\n",
    "import com.thoughtworks.deeplearning.Poly.MathFunctions._\n",
    "import com.thoughtworks.deeplearning.Poly.MathMethods./\n",
    "import com.thoughtworks.deeplearning.Poly.MathOps\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4j.linalg.indexing.{INDArrayIndex, NDArrayIndex}\n",
    "import org.nd4j.linalg.ops.transforms.Transforms\n",
    "import org.nd4s.Implicits._\n",
    "import shapeless._\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._\n",
    "\n",
    "import scala.collection.immutable.IndexedSeq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了减少`jupyter-scala`输出的行数，避免页面输出太长，需要设置`pprintConfig`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pprintConfig() = pprintConfig().copy(height = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使用`softmax`分类器(softmax分类器是`softmax`和一个全连接组合起来的神经网络)，我们需要先编写softmax函数,公式：![](https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msoftmax\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(implicit scores: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val expScores = exp(scores)\n",
    "  expScores / expScores.sum(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全连接层需要设置学习率，学习率是`weight`变化的快慢的直观描述，学习率设置的过小会导致`loss`下降的很慢，需要更长时间来训练，学习率设置的过大的话，虽然刚开始下降很快但是会在接近最低点的时候在附近徘徊。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36moptimizer\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit def optimizer: Optimizer = new LearningRate {\n",
    "  def currentLearningRate() = 0.00001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 组合神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个全连接层并[初始化Weight](https://github.com/ThoughtWorksInc/DeepLearning.scala/wiki/Getting-Started#231--weight-intialization)，`Weight`应该是一个`NumberOfPixels × NumberOfClasses`的二维`INDArray`。`scores`是每个图片对应各个分类的评分，代表该图片对应每个分类的可能概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mNumberOfClasses\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36mNumberOfPixels\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m3072\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mcreateMyNeuralNetwork\u001b[39m\n",
       "\u001b[36mmyNeuralNetwork\u001b[39m: (\u001b[32mSymbolic\u001b[39m.\u001b[32mTo\u001b[39m[\u001b[32mINDArray\u001b[39m]{type OutputData = org.nd4j.linalg.api.ndarray.INDArray;type OutputDelta = org.nd4j.linalg.api.ndarray.INDArray;type InputData = org.nd4j.linalg.api.ndarray.INDArray;type InputDelta = org.nd4j.linalg.api.ndarray.INDArray})#\u001b[32m@\u001b[39m = Compose(MultiplyINDArray(Exp(Identity()),Reciprocal(Sum(Exp(Identity()),WrappedArray(1)))),Dot(Identity(),Weight([[-0.00, 0.00, 0.00, -0.00, 0.00, 0.00, -0.00, \u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//10 label of CIFAR10 images(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
    "val NumberOfClasses: Int = 10\n",
    "val NumberOfPixels: Int = 3072\n",
    "\n",
    "def createMyNeuralNetwork(implicit input: INDArray @Symbolic): INDArray @Symbolic = {\n",
    "  val initialValueOfWeight = Nd4j.randn(NumberOfPixels, NumberOfClasses) * 0.001\n",
    "  val weight: INDArray @Symbolic = initialValueOfWeight.toWeight\n",
    "  val scores: INDArray @Symbolic = input dot weight\n",
    "  softmax.compose(scores)\n",
    "}\n",
    "val myNeuralNetwork = createMyNeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 组合LossFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了知道神经网络预测结果的好坏，我们需要编写损失函数`lossFunction`，这里我们使用[cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy)将此次判断的结果和真实结果进行对比并返回评分，公式：\n",
    "![](https://zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlossFunction\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lossFunction(implicit pair: (INDArray :: INDArray :: HNil) @Symbolic): Double @Symbolic = {\n",
    "  val input = pair.head\n",
    "  val expectedOutput = pair.tail.head\n",
    "  val probabilities = myNeuralNetwork.compose(input)\n",
    "\n",
    "  -(expectedOutput * log(probabilities)).mean\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "为了从CIFAR10 database中读取训练数据和测试数据的图片和分类信息。我们需要[`import $file.ReadCIFAR10ToNDArray`](https://github.com/ThoughtWorksInc/DeepLearning.scala-website/blob/master/ipynbs/ReadCIFAR10ToNDArray.sc)。这是一个包含读取和处理CIFAR10 数据的脚本文件，由本教程提供。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling ReadCIFAR10ToNDArray.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                   \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mtrainNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.23, 0.17, 0.20, 0.27, 0.38, 0.46, 0.54, 0.57, 0.58, 0.58, 0.51, 0.49, 0.55, 0.56, 0.54, 0.50, 0.54, 0.52, 0.48, 0.54, 0.54, 0.52, 0.53, 0.54, 0.59, 0.64, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestNDArray\u001b[39m: \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mINDArray\u001b[39m \u001b[32m::\u001b[39m \u001b[32mHNil\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $file.ReadCIFAR10ToNDArray\n",
    "\n",
    "val trainNDArray =\n",
    "  ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/data_batch_1.bin\", 1000)\n",
    "\n",
    "val testNDArray =\n",
    "  ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/test_batch.bin\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便`softmax`分类器处理数据，我们先处理标签数据([one hot encoding](https://en.wikipedia.org/wiki/One-hot))：将`NumberOfPixels × 1`的INDArray转换为`NumberOfPixels × NumberOfClasses`的INDArray，每行对应的正确分类的值为1，其它列的值为0。这里区分训练集和测试集的原因是为了能看出网络是否被过度训练导致[过拟合](https://en.wikipedia.org/wiki/Overfitting)。处理标签数据的时候我们使用了[Utils](https://github.com/ThoughtWorksInc/DeepLearning.scala-website/blob/master/ipynbs/Utils.sc)，也由本教程提供。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Utils.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.23, 0.17, 0.20, 0.27, 0.38, 0.46, 0.54, 0.57, 0.58, 0.58, 0.51, 0.49, 0.55, 0.56, 0.54, 0.50, 0.54, 0.52, 0.48, 0.54, 0.54, 0.52, 0.53, 0.54, 0.59, 0.64, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtrainExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [6.00, 9.00, 9.00, 4.00, 1.00, 1.00, 2.00, 7.00, 8.00, 3.00, 4.00, 7.00, 7.00, 2.00, 9.00, 9.00, 9.00, 3.00, 2.00, 6.00, 4.00, 3.00, 6.00, 6.00, 2.00, 6.00, 3.0\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.0\u001b[33m...\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$    \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mvectorizedTrainExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00],\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mvectorizedTestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainData = trainNDArray.head\n",
    "val testData = testNDArray.head\n",
    "\n",
    "val trainExpectResult = trainNDArray.tail.head\n",
    "val testExpectResult = testNDArray.tail.head\n",
    "\n",
    "import $file.Utils\n",
    "\n",
    "val vectorizedTrainExpectResult = Utils.makeVectorized(trainExpectResult, NumberOfClasses)\n",
    "val vectorizedTestExpectResult = Utils.makeVectorized(testExpectResult, NumberOfClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了观察神经网络训练的过程，我们需要输出`loss`，在训练神经网络时，`loss`的变化趋势应该是下降。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at iteration 0 loss is 0.230375927734375\n",
      "at iteration 100 loss is 0.1908932861328125\n",
      "at iteration 200 loss is 0.17819312744140625\n",
      "at iteration 300 loss is 0.17023385009765624\n",
      "at iteration 400 loss is 0.164277880859375\n",
      "at iteration 500 loss is 0.159437890625\n",
      "at iteration 600 loss is 0.15531514892578124\n",
      "at iteration 700 loss is 0.15169727783203124\n",
      "at iteration 800 loss is 0.148457763671875\n",
      "at iteration 900 loss is 0.1455148681640625\n",
      "at iteration 1000 loss is 0.1428119140625\n",
      "at iteration 1100 loss is 0.14030811767578125\n",
      "at iteration 1200 loss is 0.13797265625\n",
      "at iteration 1300 loss is 0.135781591796875\n",
      "at iteration 1400 loss is 0.133716064453125\n",
      "at iteration 1500 loss is 0.13176082763671876\n",
      "at iteration 1600 loss is 0.12990330810546874\n",
      "at iteration 1700 loss is 0.12813306884765624\n",
      "at iteration 1800 loss is 0.12644134521484374\n",
      "at iteration 1900 loss is 0.12482071533203125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-761494798\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0,1989.0,1990.0,1991.0,1992.0,1993.0,1994.0,1995.0,1996.0,1997.0,1998.0,1999.0],\"y\":[0.230375927734375,0.228715380859375,0.2277423583984375,0.226842431640625,0.225973291015625,0.225130322265625,0.224312060546875,0.2235173828125,0.2227452392578125,0.22199482421875,0.2212651123046875,0.2205552978515625,0.21986455078125,0.2191920654296875,0.218537158203125,0.217899072265625,0.2172771728515625,0.21667080078125,0.216079296875,0.215502099609375,0.214938671875,0.2143884033203125,0.2138509521484375,0.21332568359375,0.212812158203125,0.2123099853515625,0.21181875,0.2113379638671875,0.210867333984375,0.210406591796875,0.2099552001953125,0.209512890625,0.209079345703125,0.2086543212890625,0.2082375,0.207828564453125,0.20742724609375,0.2070333984375,0.206646728515625,0.206266943359375,0.2058939453125,0.20552734375,0.20516708984375,0.20481298828125,0.20446474609375,0.2041222900390625,0.2037854248046875,0.20345389404296876,0.20312762451171876,0.20280648193359374,0.20249027099609376,0.2021788818359375,0.20187215576171874,0.2015699951171875,0.2012722412109375,0.2009787841796875,0.20068955078125,0.200404345703125,0.20012313232421874,0.19984576416015626,0.1995721923828125,0.19930228271484374,0.1990359619140625,0.198773046875,0.1985136474609375,0.19825745849609375,0.198004541015625,0.19775479736328125,0.197508154296875,0.197264453125,0.1970237060546875,0.196785888671875,0.1965508544921875,0.1963184814453125,0.1960888671875,0.19586185302734374,0.195637353515625,0.19541541748046876,0.1951958740234375,0.19497874755859376,0.19476396484375,0.1945514892578125,0.194341259765625,0.194133203125,0.19392733154296876,0.19372357177734376,0.1935218505859375,0.1933221435546875,0.1931244140625,0.192928662109375,0.19273477783203125,0.192542822265625,0.1923526123046875,0.19216427001953126,0.1919776611328125,0.1917927978515625,0.19160966796875,0.191428125,0.1912482421875,0.191069970703125,0.1908932861328125,0.1907181396484375,0.1905444580078125,0.19037230224609375,0.1902016845703125,0.19003243408203124,0.18986458740234374,0.1896981689453125,0.18953310546875,0.1893694091796875,0.18920701904296874,0.18904586181640626,0.18888609619140626,0.18872752685546876,0.1885701904296875,0.1884140869140625,0.1882591796875,0.188105419921875,0.18795284423828126,0.1878013916015625,0.18765111083984376,0.1875018798828125,0.18735372314453125,0.18720670166015624,0.1870607177734375,0.186915771484375,0.18677186279296876,0.1866289306640625,0.186486962890625,0.1863460205078125,0.1862060302734375,0.18606702880859374,0.18592890625,0.1857917236328125,0.18565546875,0.185520068359375,0.18538564453125,0.1852520263671875,0.18511925048828126,0.184987353515625,0.18485625,0.184726025390625,0.18459664306640625,0.184468017578125,0.1843402099609375,0.1842131591796875,0.18408687744140625,0.18396138916015625,0.18383662109375,0.18371259765625,0.1835893310546875,0.18346678466796876,0.1833449462890625,0.18322384033203126,0.1831033935546875,0.18298365478515624,0.1828645751953125,0.18274619140625,0.1826284423828125,0.1825114013671875,0.182394970703125,0.182279150390625,0.1821640380859375,0.1820494873046875,0.1819356201171875,0.18182225341796876,0.1817095458984375,0.1815974853515625,0.18148597412109374,0.181375048828125,0.1812646728515625,0.1811548828125,0.1810456787109375,0.18093699951171874,0.18082889404296876,0.1807213134765625,0.180614306640625,0.18050780029296876,0.180401806640625,0.1802963623046875,0.18019144287109376,0.18008701171875,0.17998304443359375,0.1798796875,0.179776708984375,0.1796743408203125,0.1795723388671875,0.179470849609375,0.1793698974609375,0.17926934814453124,0.1791692626953125,0.1790697021484375,0.178970556640625,0.178871826171875,0.17877353515625,0.1786757568359375,0.1785783447265625,0.1784814208984375,0.17838489990234374,0.17828883056640624,0.17819312744140625,0.17809788818359376,0.17800302734375,0.17790858154296876,0.1778146240234375,0.17772099609375,0.177627734375,0.17753485107421876,0.1774424072265625,0.1773503173828125,0.17725867919921875,0.17716737060546875,0.17707646484375,0.1769858642578125,0.176895703125,0.1768058837890625,0.1767163818359375,0.17662730712890626,0.1765385986328125,0.176450146484375,0.176362109375,0.1762744384765625,0.176187060546875,0.17610006103515624,0.17601336669921874,0.175927001953125,0.175840966796875,0.175755322265625,0.17566995849609374,0.1755849365234375,0.1755001953125,0.1754158447265625,0.1753317138671875,0.1752479248046875,0.1751644775390625,0.17508134765625,0.1749985107421875,0.1749159423828125,0.1748337158203125,0.1747517578125,0.1746701171875,0.1745887451171875,0.174507666015625,0.174426904296875,0.1743463623046875,0.1742661376953125,0.1741862060546875,0.174106591796875,0.174027197265625,0.17394805908203126,0.17386922607421876,0.173790673828125,0.17371236572265625,0.17363433837890624,0.1735565673828125,0.17347908935546874,0.1734018310546875,0.173324853515625,0.1732481201171875,0.173171630859375,0.1730954345703125,0.17301944580078124,0.17294373779296876,0.17286824951171875,0.1727929931640625,0.1727179931640625,0.1726432373046875,0.17256875,0.17249447021484374,0.17242041015625,0.1723466552734375,0.172273095703125,0.17219974365234375,0.17212664794921875,0.1720537841796875,0.171981103515625,0.17190867919921876,0.171836474609375,0.1717644775390625,0.17169268798828125,0.171621142578125,0.171549853515625,0.1714787109375,0.171407861328125,0.171337158203125,0.1712666748046875,0.17119639892578126,0.1711263427734375,0.171056494140625,0.17098680419921874,0.1709173583984375,0.17084813232421875,0.17077911376953125,0.1707102294921875,0.1706416259765625,0.170573193359375,0.1705049072265625,0.17043685302734374,0.17036898193359376,0.170301318359375,0.17023385009765624,0.1701666015625,0.1700994140625,0.17003255615234375,0.1699657958984375,0.16989925537109374,0.16983291015625,0.16976673583984375,0.16970074462890625,0.1696349365234375,0.16956929931640624,0.16950380859375,0.16943857421875,0.16937342529296875,0.16930849609375,0.16924375,0.1691791259765625,0.16911468505859376,0.16905048828125,0.16898642578125,0.16892249755859376,0.1688587646484375,0.168795166015625,0.168731787109375,0.16866854248046875,0.16860545654296874,0.168542529296875,0.16847974853515624,0.1684171630859375,0.1683547607421875,0.1682924560546875,0.16823033447265626,0.16816834716796875,0.16810654296875,0.168044873046875,0.1679833984375,0.16792203369140624,0.16786083984375,0.167799755859375,0.1677388916015625,0.167678125,0.1676175048828125,0.167557080078125,0.16749676513671874,0.1674366455078125,0.16737662353515625,0.1673167236328125,0.16725701904296875,0.16719742431640625,0.16713800048828126,0.16707867431640624,0.1670195068359375,0.166960498046875,0.1669016357421875,0.16684290771484375,0.1667842529296875,0.1667258056640625,0.16666749267578124,0.16660926513671875,0.1665511962890625,0.1664932861328125,0.1664354736328125,0.1663778076171875,0.1663202392578125,0.1662628662109375,0.16620560302734375,0.1661484619140625,0.1660914306640625,0.16603455810546874,0.165977783203125,0.1659211669921875,0.1658646484375,0.165808251953125,0.165752001953125,0.16569586181640625,0.16563983154296874,0.165583984375,0.16552818603515626,0.1654725341796875,0.16541705322265626,0.16536158447265625,0.1653063232421875,0.16525118408203124,0.1651961181640625,0.1651412109375,0.165086376953125,0.16503172607421876,0.164977099609375,0.16492264404296875,0.16486832275390625,0.16481405029296875,0.16475994873046876,0.16470595703125,0.1646520263671875,0.1645982666015625,0.164544580078125,0.16449100341796874,0.1644375732421875,0.1643842529296875,0.1643309814453125,0.164277880859375,0.1642248779296875,0.164171923828125,0.16411915283203124,0.164066455078125,0.1640138427734375,0.16396138916015626,0.16390899658203126,0.16385675048828124,0.16380458984375,0.1637525146484375,0.163700537109375,0.1636487060546875,0.1635969482421875,0.163545263671875,0.16349368896484376,0.16344228515625,0.16339088134765625,0.1633396484375,0.1632885009765625,0.16323746337890624,0.163186474609375,0.16313564453125,0.16308487548828124,0.1630342041015625,0.162983642578125,0.16293316650390624,0.1628827880859375,0.16283251953125,0.16278232421875,0.1627322265625,0.1626822265625,0.16263232421875,0.16258253173828124,0.1625327880859375,0.162483203125,0.1624336669921875,0.1623841796875,0.162334814453125,0.16228558349609376,0.16223646240234374,0.162187353515625,0.16213834228515625,0.1620894775390625,0.16204063720703124,0.1619919189453125,0.161943310546875,0.1618947265625,0.1618462646484375,0.16179793701171874,0.16174962158203124,0.161701416015625,0.161653271484375,0.16160526123046876,0.161557373046875,0.16150947265625,0.1614616943359375,0.1614139892578125,0.1613663818359375,0.16131884765625,0.1612714111328125,0.16122406005859374,0.16117677001953126,0.16112958984375,0.161082470703125,0.16103544921875,0.16098848876953126,0.1609416015625,0.16089481201171876,0.16084810791015625,0.16080146484375,0.160754931640625,0.1607084716796875,0.1606620361328125,0.1606157470703125,0.1605695068359375,0.16052330322265626,0.160477294921875,0.1604312255859375,0.160385302734375,0.16033946533203125,0.16029365234375,0.1602479736328125,0.16020234375,0.16015679931640625,0.160111279296875,0.16006588134765626,0.1600205810546875,0.15997532958984376,0.159930126953125,0.1598850341796875,0.1598400146484375,0.1597949951171875,0.159750146484375,0.15970535888671875,0.1596606201171875,0.15961590576171875,0.1595712890625,0.1595267822265625,0.15948232421875,0.159437890625,0.1593936279296875,0.159349365234375,0.15930517578125,0.1592610595703125,0.1592170166015625,0.15917303466796875,0.159129150390625,0.1590853271484375,0.1590415283203125,0.15899786376953126,0.15895423583984375,0.1589106689453125,0.1588671875,0.1588237548828125,0.15878037109375,0.1587370849609375,0.15869384765625,0.15865069580078126,0.1586075927734375,0.1585645751953125,0.15852158203125,0.1584787109375,0.15843582763671876,0.1583930908203125,0.158350341796875,0.1583077392578125,0.15826514892578125,0.15822265625,0.1581801513671875,0.15813780517578124,0.158095458984375,0.15805322265625,0.15801102294921876,0.1579688720703125,0.15792677001953126,0.157884765625,0.157842822265625,0.157800927734375,0.1577590576171875,0.1577173095703125,0.1576756103515625,0.1576339599609375,0.15759237060546874,0.1575508544921875,0.1575093505859375,0.15746795654296875,0.157426611328125,0.15738529052734376,0.157344091796875,0.1573029052734375,0.1572617919921875,0.1572207275390625,0.157179736328125,0.15713880615234374,0.15709788818359374,0.15705712890625,0.157016357421875,0.1569756591796875,0.1569349853515625,0.1568944091796875,0.1568538818359375,0.1568133544921875,0.15677296142578126,0.1567325927734375,0.1566922607421875,0.156652001953125,0.1566117919921875,0.1565716064453125,0.156531591796875,0.1564915283203125,0.1564515625,0.15641162109375,0.15637171630859376,0.15633192138671875,0.156292138671875,0.1562524658203125,0.1562127685546875,0.156173193359375,0.156133642578125,0.15609415283203126,0.1560546875,0.15601533203125,0.1559759765625,0.15593670654296876,0.15589747314453126,0.1558582763671875,0.1558191650390625,0.1557800537109375,0.1557410400390625,0.155702099609375,0.15566314697265626,0.1556242431640625,0.1555854736328125,0.1555466796875,0.15550802001953126,0.1554693359375,0.155430712890625,0.1553921630859375,0.15535362548828124,0.15531514892578124,0.15527674560546875,0.15523836669921875,0.1552000732421875,0.1551617919921875,0.1551235595703125,0.15508541259765626,0.1550472900390625,0.155009228515625,0.15497119140625,0.154933203125,0.15489530029296875,0.15485740966796874,0.15481956787109374,0.1547817626953125,0.1547440673828125,0.15470634765625,0.15466876220703124,0.15463114013671875,0.154593603515625,0.154556103515625,0.15451865234375,0.15448126220703126,0.1544439208984375,0.15440657958984375,0.15436932373046874,0.1543321044921875,0.15429495849609376,0.1542578369140625,0.1542207275390625,0.1541836669921875,0.15414669189453126,0.15410975341796876,0.15407286376953125,0.154035986328125,0.1539991943359375,0.1539624267578125,0.15392569580078125,0.1538890380859375,0.1538523681640625,0.1538157958984375,0.153779296875,0.1537427734375,0.15370635986328124,0.153669921875,0.1536335693359375,0.15359718017578125,0.1535609619140625,0.15352471923828126,0.153488525390625,0.1534523681640625,0.153416259765625,0.15338018798828126,0.15334420166015625,0.1533082275390625,0.15327230224609376,0.15323638916015625,0.153200537109375,0.1531647216796875,0.15312894287109374,0.1530931884765625,0.15305751953125,0.15302191162109374,0.15298629150390625,0.152950732421875,0.152915185546875,0.152879736328125,0.15284432373046875,0.1528089111328125,0.1527735595703125,0.15273819580078124,0.152702978515625,0.15266771240234375,0.15263251953125,0.15259739990234375,0.152562255859375,0.15252724609375,0.1524921875,0.1524572021484375,0.15242225341796875,0.152387353515625,0.15235244140625,0.15231761474609376,0.15228282470703125,0.1522480712890625,0.15221337890625,0.1521787109375,0.15214404296875,0.1521094482421875,0.15207489013671874,0.15204036865234374,0.152005908203125,0.151971435546875,0.151937060546875,0.1519026611328125,0.151868310546875,0.1518340576171875,0.1517998046875,0.151765576171875,0.1517314453125,0.15169727783203124,0.1516631591796875,0.1516291015625,0.1515950927734375,0.1515610595703125,0.15152711181640624,0.1514931884765625,0.15145928955078125,0.15142547607421875,0.1513916748046875,0.15135791015625,0.151324169921875,0.1512905029296875,0.1512568115234375,0.15122318115234376,0.15118958740234376,0.1511560546875,0.151122509765625,0.1510890380859375,0.15105560302734375,0.15102220458984375,0.15098883056640625,0.15095546875,0.15092218017578124,0.1508889404296875,0.1508556396484375,0.1508224853515625,0.15078931884765626,0.15075621337890624,0.1507230712890625,0.15069005126953125,0.15065704345703124,0.15062403564453125,0.1505910888671875,0.15055814208984375,0.1505253173828125,0.15049241943359376,0.150459619140625,0.150426806640625,0.150394091796875,0.1503614013671875,0.1503286865234375,0.150296044921875,0.15026341552734376,0.150230859375,0.15019833984375,0.1501657958984375,0.1501333251953125,0.15010086669921874,0.15006844482421874,0.15003609619140626,0.15000372314453125,0.14997142333984376,0.14993917236328125,0.149906884765625,0.1498746826171875,0.1498425048828125,0.14981036376953125,0.1497782470703125,0.1497461669921875,0.1497141357421875,0.1496821044921875,0.14965013427734375,0.1496181640625,0.14958624267578124,0.1495543212890625,0.1495224853515625,0.149490625,0.14945888671875,0.14942706298828126,0.14939537353515625,0.149363671875,0.1493320068359375,0.1493004150390625,0.14926876220703125,0.14923717041015624,0.1492056396484375,0.1491741455078125,0.1491426513671875,0.1491112060546875,0.1490797607421875,0.149048388671875,0.149017041015625,0.1489857177734375,0.14895440673828125,0.14892313232421875,0.14889190673828126,0.1488607177734375,0.148829541015625,0.1487983642578125,0.1487673095703125,0.1487362060546875,0.148705126953125,0.14867410888671875,0.1486430908203125,0.148612158203125,0.1485812255859375,0.148550341796875,0.14851947021484374,0.14848861083984374,0.148457763671875,0.148427001953125,0.148396240234375,0.14836552734375,0.1483347900390625,0.1483041259765625,0.1482735107421875,0.14824288330078125,0.1482123291015625,0.14818173828125,0.14815125732421874,0.1481207275390625,0.148090283203125,0.1480598388671875,0.14802943115234374,0.14799906005859376,0.14796871337890624,0.14793836669921875,0.14790806884765625,0.1478778076171875,0.1478475830078125,0.14781734619140624,0.14778719482421876,0.14775703125,0.1477268798828125,0.14769677734375,0.147666748046875,0.1476366943359375,0.1476066650390625,0.14757667236328126,0.1475467041015625,0.14751678466796875,0.1474868408203125,0.1474570068359375,0.14742716064453126,0.147397314453125,0.1473675537109375,0.147337744140625,0.14730802001953125,0.14727830810546874,0.1472486328125,0.147218994140625,0.147189306640625,0.1471596923828125,0.1471301513671875,0.1471005615234375,0.147071044921875,0.1470415771484375,0.1470120849609375,0.146982666015625,0.14695323486328124,0.146923828125,0.14689443359375,0.14686510009765624,0.14683583984375,0.14680655517578126,0.14677724609375,0.146748046875,0.146718798828125,0.1466896240234375,0.14666046142578126,0.1466313232421875,0.14660224609375,0.14657315673828125,0.14654410400390624,0.1465150634765625,0.146486083984375,0.1464571044921875,0.146428125,0.1463991943359375,0.1463703125,0.14634141845703125,0.14631259765625,0.14628372802734374,0.146254931640625,0.14622620849609375,0.14619744873046875,0.1461687255859375,0.1461400390625,0.14611136474609376,0.14608267822265625,0.1460540771484375,0.1460254638671875,0.145996875,0.145968359375,0.145939794921875,0.145911279296875,0.1458828369140625,0.1458543701171875,0.14582596435546874,0.14579757080078126,0.145769189453125,0.1457408203125,0.14571248779296875,0.14568419189453125,0.1456558837890625,0.14562763671875,0.14559940185546874,0.14557120361328124,0.14554300537109374,0.1455148681640625,0.14548673095703124,0.1454585693359375,0.1454304931640625,0.1454024169921875,0.1453744140625,0.14534638671875,0.145318359375,0.145290380859375,0.145262451171875,0.14523450927734374,0.1452066162109375,0.1451787109375,0.14515087890625,0.1451230712890625,0.145095263671875,0.14506749267578126,0.1450396728515625,0.14501195068359374,0.1449842041015625,0.14495655517578124,0.144928857421875,0.1449011962890625,0.1448735595703125,0.1448460205078125,0.144818408203125,0.144790869140625,0.14476331787109376,0.14473580322265625,0.1447083251953125,0.144680859375,0.1446533935546875,0.14462598876953126,0.1445986083984375,0.144571240234375,0.14454384765625,0.1445165283203125,0.144489208984375,0.1444619384765625,0.1444346923828125,0.144407421875,0.1443802001953125,0.1443530517578125,0.144325830078125,0.144298681640625,0.1442715576171875,0.144244482421875,0.1442173828125,0.144190283203125,0.1441632080078125,0.14413624267578126,0.144109228515625,0.1440822509765625,0.14405531005859376,0.144028369140625,0.14400142822265624,0.143974560546875,0.14394769287109374,0.14392080078125,0.14389400634765626,0.1438671875,0.1438404296875,0.14381361083984376,0.143786865234375,0.14376015625,0.1437334716796875,0.143706787109375,0.1436801025390625,0.143653515625,0.14362689208984375,0.14360025634765625,0.14357369384765625,0.143547119140625,0.14352059326171876,0.143494091796875,0.1434675537109375,0.14344111328125,0.1434146728515625,0.1433882568359375,0.1433618408203125,0.14333543701171875,0.14330908203125,0.1432827392578125,0.143256396484375,0.143230078125,0.14320379638671876,0.1431775390625,0.1431512939453125,0.14312508544921876,0.14309884033203124,0.143072705078125,0.143046533203125,0.1430203857421875,0.14299423828125,0.1429681396484375,0.14294205322265624,0.14291600341796876,0.1428899658203125,0.142863916015625,0.1428379150390625,0.1428119140625,0.1427859375,0.142760009765625,0.1427341064453125,0.142708154296875,0.14268231201171874,0.1426564453125,0.1426305908203125,0.14260474853515626,0.14257896728515626,0.1425531494140625,0.142527392578125,0.1425016845703125,0.142475927734375,0.1424501953125,0.14242451171875,0.1423988525390625,0.1423732177734375,0.14234754638671876,0.1423219482421875,0.142296337890625,0.1422707763671875,0.14224522705078124,0.142219677734375,0.1421941650390625,0.14216866455078125,0.1421431640625,0.142117724609375,0.14209228515625,0.1420668212890625,0.142041455078125,0.14201605224609376,0.14199072265625,0.1419653564453125,0.1419400146484375,0.1419147216796875,0.1418893798828125,0.14186414794921876,0.14183887939453124,0.14181365966796874,0.1417884521484375,0.14176324462890624,0.141738037109375,0.141712890625,0.14168773193359374,0.14166260986328125,0.14163748779296875,0.14161240234375,0.1415873046875,0.1415622802734375,0.141537255859375,0.1415122314453125,0.14148724365234375,0.1414622802734375,0.1414372802734375,0.141412353515625,0.14138743896484374,0.1413625,0.14133765869140624,0.14131279296875,0.1412879150390625,0.1412630859375,0.14123826904296874,0.14121343994140625,0.14118868408203125,0.1411638916015625,0.14113916015625,0.1411144287109375,0.1410897216796875,0.1410650390625,0.14104033203125,0.141015673828125,0.1409910400390625,0.1409664306640625,0.14094180908203124,0.14091722412109375,0.1408926513671875,0.14086807861328124,0.14084356689453126,0.1408190185546875,0.14079453125,0.1407700439453125,0.14074561767578125,0.140721142578125,0.1406967041015625,0.1406722900390625,0.140647900390625,0.1406235107421875,0.140599169921875,0.1405748291015625,0.14055048828125,0.140526171875,0.14050189208984376,0.14047760009765625,0.14045335693359376,0.1404291015625,0.14040487060546875,0.1403806640625,0.1403564697265625,0.140332275390625,0.14030811767578125,0.1402839599609375,0.14025986328125,0.1402357666015625,0.14021165771484376,0.14018759765625,0.1401635009765625,0.1401394775390625,0.14011544189453126,0.14009141845703124,0.1400674560546875,0.1400434814453125,0.1400195068359375,0.139995556640625,0.139971630859375,0.13994774169921875,0.139923828125,0.139899951171875,0.13987607421875,0.13985224609375,0.13982841796875,0.139804638671875,0.13978079833984375,0.1397570068359375,0.139733251953125,0.13970947265625,0.139685791015625,0.1396620849609375,0.13963836669921875,0.13961468505859376,0.139591015625,0.1395673583984375,0.13954375,0.13952008056640625,0.1394965087890625,0.1394729248046875,0.13944931640625,0.13942576904296874,0.13940225830078126,0.1393787109375,0.13935518798828125,0.13933170166015624,0.1393082275390625,0.139284765625,0.139261328125,0.13923787841796875,0.13921446533203125,0.1391910400390625,0.13916768798828125,0.13914432373046876,0.13912095947265626,0.1390976318359375,0.13907430419921876,0.13905101318359375,0.1390277099609375,0.13900445556640625,0.1389811767578125,0.13895794677734374,0.13893470458984375,0.138911474609375,0.13888829345703124,0.138865087890625,0.13884193115234375,0.1388187744140625,0.138795654296875,0.13877249755859375,0.1387493896484375,0.1387263427734375,0.13870325927734375,0.138680224609375,0.13865712890625,0.13863409423828124,0.138611083984375,0.1385880615234375,0.1385651123046875,0.138542138671875,0.13851915283203126,0.13849620361328124,0.13847332763671874,0.1384503662109375,0.13842747802734376,0.13840457763671876,0.13838175048828125,0.13835888671875,0.13833602294921876,0.13831324462890626,0.138290380859375,0.13826763916015625,0.13824486083984375,0.13822205810546875,0.1381993408203125,0.1381765869140625,0.1381538818359375,0.13813116455078125,0.1381084716796875,0.13808580322265626,0.1380631591796875,0.1380405029296875,0.13801783447265625,0.13799525146484376,0.13797265625,0.1379500244140625,0.13792744140625,0.1379048828125,0.1378823486328125,0.13785980224609376,0.1378372802734375,0.1378147705078125,0.1377923095703125,0.13776983642578125,0.13774736328125,0.13772491455078126,0.137702490234375,0.13768006591796875,0.13765770263671875,0.13763529052734375,0.137612890625,0.13759053955078124,0.13756815185546875,0.13754586181640624,0.13752352294921874,0.1375011962890625,0.1374789306640625,0.137456640625,0.1374343994140625,0.13741214599609375,0.137389892578125,0.13736768798828125,0.1373454833984375,0.13732325439453125,0.1373010498046875,0.13727896728515626,0.1372567626953125,0.137234619140625,0.1372125244140625,0.13719039306640626,0.1371682861328125,0.13714620361328125,0.13712412109375,0.13710208740234375,0.137080029296875,0.1370579345703125,0.13703603515625,0.13701397705078125,0.13699197998046875,0.13697001953125,0.13694803466796876,0.136926123046875,0.1369041259765625,0.1368822509765625,0.136860302734375,0.136838427734375,0.136816552734375,0.1367947021484375,0.1367728271484375,0.1367510009765625,0.136729150390625,0.1367073486328125,0.1366855224609375,0.1366637451171875,0.1366419921875,0.1366202392578125,0.136598486328125,0.13657672119140624,0.13655498046875,0.1365332763671875,0.13651162109375,0.1364899169921875,0.13646824951171874,0.1364465576171875,0.136424951171875,0.1364032958984375,0.13638170166015626,0.1363600830078125,0.1363384765625,0.13631690673828126,0.1362953369140625,0.136273779296875,0.13625224609375,0.13623070068359375,0.1362092041015625,0.1361877197265625,0.13616619873046876,0.1361447509765625,0.13612327880859376,0.1361018310546875,0.13608037109375,0.1360590087890625,0.13603753662109375,0.1360161376953125,0.1359947509765625,0.13597340087890625,0.1359520263671875,0.1359306640625,0.13590933837890626,0.13588802490234375,0.13586671142578124,0.13584541015625,0.1358240966796875,0.1358028564453125,0.135781591796875,0.1357603515625,0.1357390869140625,0.13571785888671875,0.1356966796875,0.1356754638671875,0.13565428466796875,0.13563309326171874,0.1356119140625,0.13559080810546875,0.1355696533203125,0.13554853515625,0.13552744140625,0.1355063232421875,0.13548524169921874,0.13546416015625,0.1354430908203125,0.1354220458984375,0.1354010009765625,0.13537999267578124,0.135358984375,0.13533798828125,0.1353169921875,0.1352960205078125,0.13527503662109375,0.1352540771484375,0.1352331298828125,0.1352122314453125,0.13519129638671876,0.1351704345703125,0.13514951171875,0.1351286376953125,0.135107763671875,0.1350868896484375,0.1350660400390625,0.13504521484375,0.13502440185546874,0.13500355224609376,0.1349827880859375,0.1349619873046875,0.1349412353515625,0.1349204833984375,0.1348997314453125,0.13487901611328126,0.13485826416015625,0.1348375732421875,0.1348168701171875,0.1347961669921875,0.13477548828125,0.134754833984375,0.1347341796875,0.13471351318359376,0.1346928955078125,0.13467227783203126,0.1346516357421875,0.1346310546875,0.1346104736328125,0.13458992919921875,0.13456937255859375,0.134548779296875,0.134528271484375,0.13450771484375,0.1344872314453125,0.13446669921875,0.1344462646484375,0.13442574462890625,0.13440531005859374,0.13438482666015625,0.13436435546875,0.1343439208984375,0.13432353515625,0.134303076171875,0.13428270263671874,0.13426229248046875,0.134241943359375,0.1342215576171875,0.134201220703125,0.1341808837890625,0.134160546875,0.13414022216796875,0.13411988525390625,0.1340996337890625,0.1340793212890625,0.13405908203125,0.134038818359375,0.1340185546875,0.13399830322265624,0.13397806396484374,0.1339578369140625,0.13393763427734376,0.13391741943359375,0.1338972412109375,0.13387705078125,0.1338569091796875,0.13383675537109374,0.13381661376953125,0.13379649658203124,0.1337763427734375,0.1337562744140625,0.13373616943359376,0.133716064453125,0.133696044921875,0.133675927734375,0.133655859375,0.13363583984375,0.1336158203125,0.13359580078125,0.1335757568359375,0.13355574951171875,0.1335358154296875,0.1335158203125,0.133495849609375,0.133475927734375,0.13345596923828126,0.1334360107421875,0.1334160888671875,0.13339617919921876,0.13337626953125,0.13335638427734375,0.1333365234375,0.1333166748046875,0.13329678955078125,0.133276904296875,0.1332571044921875,0.133237255859375,0.1332174560546875,0.1331976806640625,0.1331778564453125,0.13315806884765624,0.13313831787109376,0.13311851806640626,0.133098779296875,0.13307904052734376,0.133059326171875,0.1330395751953125,0.133019873046875,0.1330001953125,0.1329804931640625,0.1329608154296875,0.13294114990234376,0.1329215087890625,0.13290185546875,0.1328822021484375,0.13286258544921875,0.1328429931640625,0.132823388671875,0.132803759765625,0.1327842041015625,0.13276461181640625,0.132745068359375,0.1327255126953125,0.13270596923828126,0.13268641357421876,0.13266689453125,0.13264736328125,0.132627880859375,0.13260843505859374,0.13258890380859376,0.13256944580078125,0.13255001220703125,0.13253055419921875,0.1325111083984375,0.13249169921875,0.132472265625,0.1324528564453125,0.13243345947265625,0.13241405029296874,0.13239466552734375,0.13237532958984374,0.13235595703125,0.1323365966796875,0.1323172607421875,0.13229793701171874,0.13227861328125,0.13225931396484375,0.1322400146484375,0.1322207275390625,0.13220147705078125,0.1321822021484375,0.13216292724609374,0.132143701171875,0.13212445068359374,0.13210521240234374,0.13208602294921876,0.132066796875,0.132047607421875,0.13202840576171876,0.13200921630859375,0.1319900634765625,0.13197088623046874,0.1319517578125,0.13193262939453124,0.1319134765625,0.1318943603515625,0.13187525634765626,0.13185618896484375,0.1318370849609375,0.1318179931640625,0.13179892578125,0.1317798828125,0.13176082763671876,0.131741796875,0.13172275390625,0.13170372314453124,0.1316846923828125,0.1316656982421875,0.1316467529296875,0.1316277099609375,0.13160877685546876,0.1315898193359375,0.13157086181640626,0.1315519287109375,0.1315329833984375,0.1315140625,0.1314951416015625,0.1314762451171875,0.13145733642578125,0.1314384765625,0.131419580078125,0.131400732421875,0.13138182373046875,0.13136298828125,0.1313441650390625,0.131325341796875,0.1313065185546875,0.13128773193359375,0.13126890869140626,0.1312501220703125,0.13123134765625,0.1312125732421875,0.131193798828125,0.13117506103515625,0.1311563232421875,0.1311375732421875,0.1311188720703125,0.131100146484375,0.13108140869140625,0.13106275634765624,0.13104405517578124,0.13102537841796874,0.1310067138671875,0.1309880615234375,0.1309694091796875,0.13095074462890624,0.13093211669921875,0.130913525390625,0.130894873046875,0.13087630615234375,0.130857666015625,0.130839111328125,0.13082054443359376,0.130801953125,0.1307834228515625,0.1307648681640625,0.1307463134765625,0.13072779541015625,0.1307093017578125,0.13069075927734375,0.1306722412109375,0.130653759765625,0.130635302734375,0.130616796875,0.1305983642578125,0.1305798828125,0.1305614501953125,0.13054300537109376,0.130524560546875,0.1305061767578125,0.1304877685546875,0.1304693115234375,0.1304509765625,0.130432568359375,0.1304141845703125,0.13039580078125,0.1303774658203125,0.1303591064453125,0.1303407958984375,0.1303224609375,0.13030411376953124,0.130285791015625,0.1302675048828125,0.1302491943359375,0.1302309326171875,0.13021265869140625,0.130194384765625,0.13017613525390626,0.130157861328125,0.13013961181640624,0.13012139892578126,0.13010316162109375,0.1300849609375,0.13006673583984374,0.13004857177734375,0.1300303466796875,0.1300121826171875,0.1299940185546875,0.12997584228515624,0.12995770263671874,0.12993953857421875,0.1299214111328125,0.12990330810546874,0.1298851806640625,0.1298670654296875,0.1298489501953125,0.12983087158203124,0.12981278076171876,0.1297947021484375,0.12977662353515626,0.12975859375,0.1297405517578125,0.1297224609375,0.1297044677734375,0.12968646240234374,0.1296684326171875,0.1296504150390625,0.129632421875,0.129614404296875,0.12959647216796874,0.129578466796875,0.129560498046875,0.129542529296875,0.1295245849609375,0.1295066650390625,0.12948875732421875,0.12947086181640624,0.1294529296875,0.129435009765625,0.12941708984375,0.12939923095703126,0.12938134765625,0.12936348876953124,0.12934560546875,0.12932774658203125,0.1293099365234375,0.12929208984375,0.12927421875,0.1292564208984375,0.1292385986328125,0.12922081298828125,0.12920301513671875,0.12918524169921874,0.129167431640625,0.1291496826171875,0.1291319580078125,0.1291141845703125,0.129096435546875,0.12907867431640624,0.129060986328125,0.1290432373046875,0.129025537109375,0.12900782470703126,0.12899013671875,0.1289724609375,0.12895477294921875,0.1289371337890625,0.12891947021484376,0.12890179443359376,0.1288841552734375,0.12886650390625,0.128848876953125,0.12883125,0.128813623046875,0.1287960693359375,0.128778466796875,0.1287608642578125,0.1287432861328125,0.1287257080078125,0.128708154296875,0.128690625,0.1286730224609375,0.128655517578125,0.1286379638671875,0.12862042236328125,0.12860294189453125,0.1285854248046875,0.12856796875,0.128550439453125,0.12853294677734375,0.1285154541015625,0.128497998046875,0.1284805419921875,0.12846309814453125,0.1284456298828125,0.12842823486328125,0.128410791015625,0.1283933837890625,0.1283759521484375,0.12835858154296875,0.1283411865234375,0.128323779296875,0.12830640869140625,0.1282890380859375,0.12827169189453125,0.1282543212890625,0.128236962890625,0.12821962890625,0.12820228271484374,0.1281849609375,0.12816763916015625,0.12815037841796875,0.12813306884765624,0.12811578369140625,0.1280984619140625,0.12808118896484375,0.12806392822265625,0.12804666748046875,0.12802939453125,0.12801214599609376,0.12799490966796875,0.12797769775390624,0.1279604736328125,0.12794326171875,0.1279260498046875,0.1279088623046875,0.127891650390625,0.1278744873046875,0.1278572998046875,0.1278401123046875,0.12782294921875,0.1278058349609375,0.127788720703125,0.127771533203125,0.12775439453125,0.1277373046875,0.127720166015625,0.127703076171875,0.1276859619140625,0.1276688720703125,0.12765181884765625,0.12763470458984374,0.1276176025390625,0.12760057373046876,0.12758355712890626,0.1275664794921875,0.127549462890625,0.12753245849609374,0.12751536865234375,0.1274983642578125,0.1274813720703125,0.127464404296875,0.127447412109375,0.12743040771484376,0.1274134033203125,0.127396435546875,0.1273794921875,0.1273625244140625,0.12734559326171874,0.127328662109375,0.12731168212890626,0.127294775390625,0.12727786865234375,0.12726094970703125,0.12724404296875,0.12722713623046875,0.12721024169921874,0.127193359375,0.127176513671875,0.1271596435546875,0.12714278564453124,0.127125927734375,0.1271091064453125,0.1270922607421875,0.1270754150390625,0.1270586181640625,0.12704180908203125,0.1270250244140625,0.12700819091796875,0.12699140625,0.1269746337890625,0.12695782470703126,0.12694110107421874,0.1269242919921875,0.12690753173828126,0.1268907958984375,0.126874072265625,0.12685732421875,0.126840625,0.12682388916015624,0.126807177734375,0.1267904296875,0.126773779296875,0.126757080078125,0.12674039306640625,0.12672373046875,0.12670703125,0.12669039306640625,0.1266737548828125,0.126657080078125,0.1266404052734375,0.126623828125,0.1266072021484375,0.126590576171875,0.12657396240234375,0.1265573486328125,0.12654075927734376,0.12652415771484374,0.1265075927734375,0.126491015625,0.12647445068359375,0.12645787353515625,0.12644134521484374,0.1264248046875,0.12640821533203125,0.12639168701171874,0.1263751708984375,0.12635869140625,0.1263421630859375,0.1263256591796875,0.1263091796875,0.12629267578125,0.1262761962890625,0.126259716796875,0.1262432373046875,0.1262267822265625,0.126210302734375,0.12619384765625,0.12617740478515624,0.126160986328125,0.12614456787109374,0.1261281494140625,0.12611171875,0.1260953369140625,0.126078955078125,0.1260625244140625,0.1260461669921875,0.1260298095703125,0.12601339111328125,0.125997021484375,0.1259806640625,0.1259643310546875,0.125947998046875,0.12593165283203125,0.125915283203125,0.1258989990234375,0.125882666015625,0.1258663818359375,0.12585006103515625,0.125833740234375,0.12581748046875,0.12580123291015624,0.12578492431640625,0.1257686279296875,0.12575240478515626,0.1257361328125,0.125719873046875,0.125703662109375,0.12568740234375,0.12567120361328124,0.12565496826171876,0.1256387451171875,0.1256225830078125,0.12560635986328125,0.12559012451171875,0.12557393798828126,0.12555780029296876,0.1255416259765625,0.125525439453125,0.1255093017578125,0.12549310302734376,0.1254769775390625,0.12546083984375,0.1254447021484375,0.1254285888671875,0.12541248779296876,0.1253963623046875,0.12538023681640625,0.12536416015625,0.1253480712890625,0.1253319580078125,0.12531588134765625,0.1252998291015625,0.12528375244140624,0.12526767578125,0.1252516357421875,0.12523560791015625,0.1252195556640625,0.1252035400390625,0.12518753662109375,0.12517152099609374,0.1251554443359375,0.1251394775390625,0.12512344970703124,0.125107470703125,0.125091455078125,0.12507550048828125,0.1250595458984375,0.1250435791015625,0.125027587890625,0.1250116455078125,0.124995703125,0.12497974853515625,0.124963818359375,0.124947900390625,0.1249319580078125,0.1249160400390625,0.124900146484375,0.1248842529296875,0.1248683349609375,0.1248524658203125,0.1248365966796875,0.12482071533203125,0.124804833984375,0.12478895263671876,0.1247731201171875,0.1247572509765625,0.12474141845703125,0.1247255859375,0.1247097412109375,0.1246939453125,0.1246781005859375,0.1246622802734375,0.12464649658203125,0.1246306884765625,0.12461492919921875,0.1245991455078125,0.1245833251953125,0.12456754150390625,0.1245517822265625,0.12453603515625,0.124520263671875,0.1245045166015625,0.12448878173828125,0.124473046875,0.12445732421875,0.124441650390625,0.12442587890625,0.1244101806640625,0.12439444580078125,0.124378759765625,0.1243630615234375,0.1243473876953125,0.12433170166015625,0.124316064453125,0.1243003662109375,0.12428475341796875,0.1242690673828125,0.12425340576171875,0.124237744140625,0.12422215576171874,0.12420654296875,0.12419091796875,0.1241752685546875,0.1241596923828125,0.1241440673828125,0.124128466796875,0.12411287841796875,0.1240972900390625,0.1240817138671875,0.12406614990234376,0.12405057373046875,0.1240350341796875,0.1240194580078125,0.12400389404296874,0.12398836669921875,0.1239728271484375,0.123957275390625,0.123941796875,0.12392626953125,0.12391075439453125,0.1238952392578125,0.12387974853515625,0.12386424560546876,0.12384876708984376,0.12383328857421876,0.123817822265625,0.12380234375,0.1237868896484375,0.123771435546875,0.12375596923828125,0.12374053955078125,0.12372508544921874,0.12370966796875,0.12369423828125,0.1236788330078125,0.123663427734375,0.1236480224609375,0.12363260498046875,0.12361719970703125,0.1236018310546875,0.12358643798828126,0.12357105712890624,0.12355567626953125,0.12354031982421874,0.1235249755859375,0.123509619140625,0.123494287109375,0.1234789306640625,0.12346357421875,0.1234482421875,0.12343292236328125,0.123417626953125,0.123402294921875,0.12338699951171875,0.123371728515625,0.12335640869140625,0.12334111328125,0.12332584228515625,0.1233105712890625,0.12329527587890625,0.1232800537109375]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss by time\"};\n",
       "\n",
       "  Plotly.plot('plot-761494798', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mlossSeq\u001b[39m: \u001b[32mIndexedSeq\u001b[39m[\u001b[32mcmd5Wrapper\u001b[39m.\u001b[32m<refinement>\u001b[39m.this.type.\u001b[32mOutputData\u001b[39m] = \u001b[33mVector\u001b[39m(\n",
       "  \u001b[32m0.230375927734375\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mplot\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mScatter\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mScatter\u001b[39m(\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mres8_2\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-761494798\"\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lossSeq = for (iteration <- 0 until 2000) yield {\n",
    "  val loss = lossFunction.train(trainData :: vectorizedTrainExpectResult :: HNil)\n",
    "  if(iteration % 100 == 0){\n",
    "    println(s\"at iteration $iteration loss is $loss\")\n",
    "  }\n",
    "  loss\n",
    "}\n",
    "\n",
    "val plot = Seq(\n",
    "  Scatter(lossSeq.indices, lossSeq)\n",
    ")\n",
    "\n",
    "plot.plot(\n",
    "  title = \"loss by time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证神经网络预测准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用已经处理好的测试数据来验证神经网络的预测结果并计算准确率。准确率应该在32%左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result is 32.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mright\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m32.0\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val right = Utils.getAccuracy(myNeuralNetwork.predict(testData), testExpectResult)\n",
    "println(s\"the result is $right %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "在这节中我们学到了：\n",
    "\n",
    "* 准备和处理CIFAR10数据\n",
    "* 编写softmax分类器\n",
    "* 使用softmax分类器编写的神经网络预测图片对应于每个分类的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[完整代码](https://github.com/izhangzhihao/deeplearning-tutorial/blob/master/src/main/scala/com/thoughtworks/deeplearning/tutorial/SoftmaxLinearClassifier.scala)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
